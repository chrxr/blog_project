--
-- PostgreSQL database dump
--

-- Dumped from database version 9.5.23
-- Dumped by pg_dump version 9.5.23

SET statement_timeout = 0;
SET lock_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: plpgsql; Type: EXTENSION; Schema: -; Owner: 
--

CREATE EXTENSION IF NOT EXISTS plpgsql WITH SCHEMA pg_catalog;


--
-- Name: EXTENSION plpgsql; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION plpgsql IS 'PL/pgSQL procedural language';


SET default_tablespace = '';

SET default_with_oids = false;

--
-- Name: auth_group; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_group (
    id integer NOT NULL,
    name character varying(80) NOT NULL
);


ALTER TABLE public.auth_group OWNER TO postgres;

--
-- Name: auth_group_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_group_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_group_id_seq OWNER TO postgres;

--
-- Name: auth_group_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_group_id_seq OWNED BY public.auth_group.id;


--
-- Name: auth_group_permissions; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_group_permissions (
    id integer NOT NULL,
    group_id integer NOT NULL,
    permission_id integer NOT NULL
);


ALTER TABLE public.auth_group_permissions OWNER TO postgres;

--
-- Name: auth_group_permissions_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_group_permissions_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_group_permissions_id_seq OWNER TO postgres;

--
-- Name: auth_group_permissions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_group_permissions_id_seq OWNED BY public.auth_group_permissions.id;


--
-- Name: auth_permission; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_permission (
    id integer NOT NULL,
    name character varying(255) NOT NULL,
    content_type_id integer NOT NULL,
    codename character varying(100) NOT NULL
);


ALTER TABLE public.auth_permission OWNER TO postgres;

--
-- Name: auth_permission_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_permission_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_permission_id_seq OWNER TO postgres;

--
-- Name: auth_permission_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_permission_id_seq OWNED BY public.auth_permission.id;


--
-- Name: auth_user; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_user (
    id integer NOT NULL,
    password character varying(128) NOT NULL,
    last_login timestamp with time zone,
    is_superuser boolean NOT NULL,
    username character varying(150) NOT NULL,
    first_name character varying(30) NOT NULL,
    last_name character varying(150) NOT NULL,
    email character varying(254) NOT NULL,
    is_staff boolean NOT NULL,
    is_active boolean NOT NULL,
    date_joined timestamp with time zone NOT NULL
);


ALTER TABLE public.auth_user OWNER TO postgres;

--
-- Name: auth_user_groups; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_user_groups (
    id integer NOT NULL,
    user_id integer NOT NULL,
    group_id integer NOT NULL
);


ALTER TABLE public.auth_user_groups OWNER TO postgres;

--
-- Name: auth_user_groups_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_user_groups_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_user_groups_id_seq OWNER TO postgres;

--
-- Name: auth_user_groups_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_user_groups_id_seq OWNED BY public.auth_user_groups.id;


--
-- Name: auth_user_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_user_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_user_id_seq OWNER TO postgres;

--
-- Name: auth_user_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_user_id_seq OWNED BY public.auth_user.id;


--
-- Name: auth_user_user_permissions; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_user_user_permissions (
    id integer NOT NULL,
    user_id integer NOT NULL,
    permission_id integer NOT NULL
);


ALTER TABLE public.auth_user_user_permissions OWNER TO postgres;

--
-- Name: auth_user_user_permissions_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_user_user_permissions_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_user_user_permissions_id_seq OWNER TO postgres;

--
-- Name: auth_user_user_permissions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_user_user_permissions_id_seq OWNED BY public.auth_user_user_permissions.id;


--
-- Name: blog_blogindexpage; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.blog_blogindexpage (
    page_ptr_id integer NOT NULL,
    intro text NOT NULL
);


ALTER TABLE public.blog_blogindexpage OWNER TO postgres;

--
-- Name: blog_blogindexrelatedlink; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.blog_blogindexrelatedlink (
    id integer NOT NULL,
    sort_order integer,
    link_external character varying(200) NOT NULL,
    title character varying(255) NOT NULL,
    link_page_id integer,
    page_id integer NOT NULL
);


ALTER TABLE public.blog_blogindexrelatedlink OWNER TO postgres;

--
-- Name: blog_blogindexrelatedlink_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.blog_blogindexrelatedlink_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.blog_blogindexrelatedlink_id_seq OWNER TO postgres;

--
-- Name: blog_blogindexrelatedlink_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.blog_blogindexrelatedlink_id_seq OWNED BY public.blog_blogindexrelatedlink.id;


--
-- Name: blog_blogpage; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.blog_blogpage (
    page_ptr_id integer NOT NULL,
    date date,
    intro character varying(250),
    body text,
    main_image_id integer,
    subtitle character varying(255),
    listing_image_id integer,
    listing_intro text
);


ALTER TABLE public.blog_blogpage OWNER TO postgres;

--
-- Name: blog_blogpagetag; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.blog_blogpagetag (
    id integer NOT NULL,
    content_object_id integer NOT NULL,
    tag_id integer NOT NULL
);


ALTER TABLE public.blog_blogpagetag OWNER TO postgres;

--
-- Name: blog_blogpagetag_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.blog_blogpagetag_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.blog_blogpagetag_id_seq OWNER TO postgres;

--
-- Name: blog_blogpagetag_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.blog_blogpagetag_id_seq OWNED BY public.blog_blogpagetag.id;


--
-- Name: blog_bookmark; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.blog_bookmark (
    id integer NOT NULL,
    url character varying(200) NOT NULL,
    title character varying(255) NOT NULL,
    notes text,
    date_read date NOT NULL
);


ALTER TABLE public.blog_bookmark OWNER TO postgres;

--
-- Name: blog_bookmark_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.blog_bookmark_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.blog_bookmark_id_seq OWNER TO postgres;

--
-- Name: blog_bookmark_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.blog_bookmark_id_seq OWNED BY public.blog_bookmark.id;


--
-- Name: blog_bookmarkpage; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.blog_bookmarkpage (
    page_ptr_id integer NOT NULL,
    intro character varying(250)
);


ALTER TABLE public.blog_bookmarkpage OWNER TO postgres;

--
-- Name: blog_bookmarkplacement; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.blog_bookmarkplacement (
    id integer NOT NULL,
    page_id integer NOT NULL,
    quote_id integer NOT NULL
);


ALTER TABLE public.blog_bookmarkplacement OWNER TO postgres;

--
-- Name: blog_bookmarkplacement_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.blog_bookmarkplacement_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.blog_bookmarkplacement_id_seq OWNER TO postgres;

--
-- Name: blog_bookmarkplacement_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.blog_bookmarkplacement_id_seq OWNED BY public.blog_bookmarkplacement.id;


--
-- Name: blog_bookmarktag; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.blog_bookmarktag (
    id integer NOT NULL,
    content_object_id integer NOT NULL,
    tag_id integer NOT NULL
);


ALTER TABLE public.blog_bookmarktag OWNER TO postgres;

--
-- Name: blog_bookmarktag_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.blog_bookmarktag_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.blog_bookmarktag_id_seq OWNER TO postgres;

--
-- Name: blog_bookmarktag_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.blog_bookmarktag_id_seq OWNED BY public.blog_bookmarktag.id;


--
-- Name: django_admin_log; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.django_admin_log (
    id integer NOT NULL,
    action_time timestamp with time zone NOT NULL,
    object_id text,
    object_repr character varying(200) NOT NULL,
    action_flag smallint NOT NULL,
    change_message text NOT NULL,
    content_type_id integer,
    user_id integer NOT NULL,
    CONSTRAINT django_admin_log_action_flag_check CHECK ((action_flag >= 0))
);


ALTER TABLE public.django_admin_log OWNER TO postgres;

--
-- Name: django_admin_log_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.django_admin_log_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.django_admin_log_id_seq OWNER TO postgres;

--
-- Name: django_admin_log_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.django_admin_log_id_seq OWNED BY public.django_admin_log.id;


--
-- Name: django_content_type; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.django_content_type (
    id integer NOT NULL,
    app_label character varying(100) NOT NULL,
    model character varying(100) NOT NULL
);


ALTER TABLE public.django_content_type OWNER TO postgres;

--
-- Name: django_content_type_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.django_content_type_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.django_content_type_id_seq OWNER TO postgres;

--
-- Name: django_content_type_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.django_content_type_id_seq OWNED BY public.django_content_type.id;


--
-- Name: django_migrations; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.django_migrations (
    id integer NOT NULL,
    app character varying(255) NOT NULL,
    name character varying(255) NOT NULL,
    applied timestamp with time zone NOT NULL
);


ALTER TABLE public.django_migrations OWNER TO postgres;

--
-- Name: django_migrations_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.django_migrations_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.django_migrations_id_seq OWNER TO postgres;

--
-- Name: django_migrations_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.django_migrations_id_seq OWNED BY public.django_migrations.id;


--
-- Name: django_session; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.django_session (
    session_key character varying(40) NOT NULL,
    session_data text NOT NULL,
    expire_date timestamp with time zone NOT NULL
);


ALTER TABLE public.django_session OWNER TO postgres;

--
-- Name: home_homepage; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.home_homepage (
    page_ptr_id integer NOT NULL,
    subtitle character varying(255)
);


ALTER TABLE public.home_homepage OWNER TO postgres;

--
-- Name: taggit_tag; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.taggit_tag (
    id integer NOT NULL,
    name character varying(100) NOT NULL,
    slug character varying(100) NOT NULL
);


ALTER TABLE public.taggit_tag OWNER TO postgres;

--
-- Name: taggit_tag_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.taggit_tag_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.taggit_tag_id_seq OWNER TO postgres;

--
-- Name: taggit_tag_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.taggit_tag_id_seq OWNED BY public.taggit_tag.id;


--
-- Name: taggit_taggeditem; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.taggit_taggeditem (
    id integer NOT NULL,
    object_id integer NOT NULL,
    content_type_id integer NOT NULL,
    tag_id integer NOT NULL
);


ALTER TABLE public.taggit_taggeditem OWNER TO postgres;

--
-- Name: taggit_taggeditem_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.taggit_taggeditem_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.taggit_taggeditem_id_seq OWNER TO postgres;

--
-- Name: taggit_taggeditem_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.taggit_taggeditem_id_seq OWNED BY public.taggit_taggeditem.id;


--
-- Name: wagtailcore_collection; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_collection (
    id integer NOT NULL,
    path character varying(255) COLLATE pg_catalog."C" NOT NULL,
    depth integer NOT NULL,
    numchild integer NOT NULL,
    name character varying(255) NOT NULL,
    CONSTRAINT wagtailcore_collection_depth_check CHECK ((depth >= 0)),
    CONSTRAINT wagtailcore_collection_numchild_check CHECK ((numchild >= 0))
);


ALTER TABLE public.wagtailcore_collection OWNER TO postgres;

--
-- Name: wagtailcore_collection_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_collection_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_collection_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_collection_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_collection_id_seq OWNED BY public.wagtailcore_collection.id;


--
-- Name: wagtailcore_collectionviewrestriction; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_collectionviewrestriction (
    id integer NOT NULL,
    restriction_type character varying(20) NOT NULL,
    password character varying(255) NOT NULL,
    collection_id integer NOT NULL
);


ALTER TABLE public.wagtailcore_collectionviewrestriction OWNER TO postgres;

--
-- Name: wagtailcore_collectionviewrestriction_groups; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_collectionviewrestriction_groups (
    id integer NOT NULL,
    collectionviewrestriction_id integer NOT NULL,
    group_id integer NOT NULL
);


ALTER TABLE public.wagtailcore_collectionviewrestriction_groups OWNER TO postgres;

--
-- Name: wagtailcore_collectionviewrestriction_groups_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_collectionviewrestriction_groups_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_collectionviewrestriction_groups_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_collectionviewrestriction_groups_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_collectionviewrestriction_groups_id_seq OWNED BY public.wagtailcore_collectionviewrestriction_groups.id;


--
-- Name: wagtailcore_collectionviewrestriction_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_collectionviewrestriction_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_collectionviewrestriction_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_collectionviewrestriction_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_collectionviewrestriction_id_seq OWNED BY public.wagtailcore_collectionviewrestriction.id;


--
-- Name: wagtailcore_groupcollectionpermission; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_groupcollectionpermission (
    id integer NOT NULL,
    collection_id integer NOT NULL,
    group_id integer NOT NULL,
    permission_id integer NOT NULL
);


ALTER TABLE public.wagtailcore_groupcollectionpermission OWNER TO postgres;

--
-- Name: wagtailcore_groupcollectionpermission_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_groupcollectionpermission_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_groupcollectionpermission_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_groupcollectionpermission_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_groupcollectionpermission_id_seq OWNED BY public.wagtailcore_groupcollectionpermission.id;


--
-- Name: wagtailcore_grouppagepermission; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_grouppagepermission (
    id integer NOT NULL,
    permission_type character varying(20) NOT NULL,
    group_id integer NOT NULL,
    page_id integer NOT NULL
);


ALTER TABLE public.wagtailcore_grouppagepermission OWNER TO postgres;

--
-- Name: wagtailcore_grouppagepermission_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_grouppagepermission_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_grouppagepermission_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_grouppagepermission_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_grouppagepermission_id_seq OWNED BY public.wagtailcore_grouppagepermission.id;


--
-- Name: wagtailcore_page; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_page (
    id integer NOT NULL,
    path character varying(255) NOT NULL,
    depth integer NOT NULL,
    numchild integer NOT NULL,
    title character varying(255) NOT NULL,
    slug character varying(255) NOT NULL,
    live boolean NOT NULL,
    has_unpublished_changes boolean NOT NULL,
    url_path text NOT NULL,
    seo_title character varying(255) NOT NULL,
    show_in_menus boolean NOT NULL,
    search_description text NOT NULL,
    go_live_at timestamp with time zone,
    expire_at timestamp with time zone,
    expired boolean NOT NULL,
    content_type_id integer NOT NULL,
    owner_id integer,
    locked boolean NOT NULL,
    latest_revision_created_at timestamp with time zone,
    first_published_at timestamp with time zone,
    live_revision_id integer,
    last_published_at timestamp with time zone,
    draft_title character varying(255) NOT NULL,
    CONSTRAINT wagtailcore_page_depth_check CHECK ((depth >= 0)),
    CONSTRAINT wagtailcore_page_numchild_check CHECK ((numchild >= 0))
);


ALTER TABLE public.wagtailcore_page OWNER TO postgres;

--
-- Name: wagtailcore_page_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_page_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_page_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_page_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_page_id_seq OWNED BY public.wagtailcore_page.id;


--
-- Name: wagtailcore_pagerevision; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_pagerevision (
    id integer NOT NULL,
    submitted_for_moderation boolean NOT NULL,
    created_at timestamp with time zone NOT NULL,
    content_json text NOT NULL,
    approved_go_live_at timestamp with time zone,
    page_id integer NOT NULL,
    user_id integer
);


ALTER TABLE public.wagtailcore_pagerevision OWNER TO postgres;

--
-- Name: wagtailcore_pagerevision_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_pagerevision_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_pagerevision_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_pagerevision_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_pagerevision_id_seq OWNED BY public.wagtailcore_pagerevision.id;


--
-- Name: wagtailcore_pageviewrestriction; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_pageviewrestriction (
    id integer NOT NULL,
    password character varying(255) NOT NULL,
    page_id integer NOT NULL,
    restriction_type character varying(20) NOT NULL
);


ALTER TABLE public.wagtailcore_pageviewrestriction OWNER TO postgres;

--
-- Name: wagtailcore_pageviewrestriction_groups; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_pageviewrestriction_groups (
    id integer NOT NULL,
    pageviewrestriction_id integer NOT NULL,
    group_id integer NOT NULL
);


ALTER TABLE public.wagtailcore_pageviewrestriction_groups OWNER TO postgres;

--
-- Name: wagtailcore_pageviewrestriction_groups_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_pageviewrestriction_groups_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_pageviewrestriction_groups_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_pageviewrestriction_groups_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_pageviewrestriction_groups_id_seq OWNED BY public.wagtailcore_pageviewrestriction_groups.id;


--
-- Name: wagtailcore_pageviewrestriction_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_pageviewrestriction_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_pageviewrestriction_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_pageviewrestriction_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_pageviewrestriction_id_seq OWNED BY public.wagtailcore_pageviewrestriction.id;


--
-- Name: wagtailcore_site; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailcore_site (
    id integer NOT NULL,
    hostname character varying(255) NOT NULL,
    port integer NOT NULL,
    is_default_site boolean NOT NULL,
    root_page_id integer NOT NULL,
    site_name character varying(255)
);


ALTER TABLE public.wagtailcore_site OWNER TO postgres;

--
-- Name: wagtailcore_site_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailcore_site_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailcore_site_id_seq OWNER TO postgres;

--
-- Name: wagtailcore_site_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailcore_site_id_seq OWNED BY public.wagtailcore_site.id;


--
-- Name: wagtaildocs_document; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtaildocs_document (
    id integer NOT NULL,
    title character varying(255) NOT NULL,
    file character varying(100) NOT NULL,
    created_at timestamp with time zone NOT NULL,
    uploaded_by_user_id integer,
    collection_id integer NOT NULL,
    file_size integer,
    file_hash character varying(40) NOT NULL,
    CONSTRAINT wagtaildocs_document_file_size_check CHECK ((file_size >= 0))
);


ALTER TABLE public.wagtaildocs_document OWNER TO postgres;

--
-- Name: wagtaildocs_document_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtaildocs_document_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtaildocs_document_id_seq OWNER TO postgres;

--
-- Name: wagtaildocs_document_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtaildocs_document_id_seq OWNED BY public.wagtaildocs_document.id;


--
-- Name: wagtailembeds_embed; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailembeds_embed (
    id integer NOT NULL,
    url character varying(200) NOT NULL,
    max_width smallint,
    type character varying(10) NOT NULL,
    html text NOT NULL,
    title text NOT NULL,
    author_name text NOT NULL,
    provider_name text NOT NULL,
    thumbnail_url character varying(200),
    width integer,
    height integer,
    last_updated timestamp with time zone NOT NULL
);


ALTER TABLE public.wagtailembeds_embed OWNER TO postgres;

--
-- Name: wagtailembeds_embed_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailembeds_embed_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailembeds_embed_id_seq OWNER TO postgres;

--
-- Name: wagtailembeds_embed_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailembeds_embed_id_seq OWNED BY public.wagtailembeds_embed.id;


--
-- Name: wagtailforms_formsubmission; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailforms_formsubmission (
    id integer NOT NULL,
    form_data text NOT NULL,
    submit_time timestamp with time zone NOT NULL,
    page_id integer NOT NULL
);


ALTER TABLE public.wagtailforms_formsubmission OWNER TO postgres;

--
-- Name: wagtailforms_formsubmission_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailforms_formsubmission_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailforms_formsubmission_id_seq OWNER TO postgres;

--
-- Name: wagtailforms_formsubmission_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailforms_formsubmission_id_seq OWNED BY public.wagtailforms_formsubmission.id;


--
-- Name: wagtailimages_image; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailimages_image (
    id integer NOT NULL,
    title character varying(255) NOT NULL,
    file character varying(100) NOT NULL,
    width integer NOT NULL,
    height integer NOT NULL,
    created_at timestamp with time zone NOT NULL,
    focal_point_x integer,
    focal_point_y integer,
    focal_point_width integer,
    focal_point_height integer,
    uploaded_by_user_id integer,
    file_size integer,
    collection_id integer NOT NULL,
    file_hash character varying(40) NOT NULL,
    CONSTRAINT wagtailimages_image_file_size_check CHECK ((file_size >= 0)),
    CONSTRAINT wagtailimages_image_focal_point_height_check CHECK ((focal_point_height >= 0)),
    CONSTRAINT wagtailimages_image_focal_point_width_check CHECK ((focal_point_width >= 0)),
    CONSTRAINT wagtailimages_image_focal_point_x_check CHECK ((focal_point_x >= 0)),
    CONSTRAINT wagtailimages_image_focal_point_y_check CHECK ((focal_point_y >= 0))
);


ALTER TABLE public.wagtailimages_image OWNER TO postgres;

--
-- Name: wagtailimages_image_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailimages_image_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailimages_image_id_seq OWNER TO postgres;

--
-- Name: wagtailimages_image_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailimages_image_id_seq OWNED BY public.wagtailimages_image.id;


--
-- Name: wagtailimages_rendition; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailimages_rendition (
    id integer NOT NULL,
    file character varying(100) NOT NULL,
    width integer NOT NULL,
    height integer NOT NULL,
    focal_point_key character varying(16) NOT NULL,
    image_id integer NOT NULL,
    filter_spec character varying(255) NOT NULL
);


ALTER TABLE public.wagtailimages_rendition OWNER TO postgres;

--
-- Name: wagtailimages_rendition_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailimages_rendition_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailimages_rendition_id_seq OWNER TO postgres;

--
-- Name: wagtailimages_rendition_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailimages_rendition_id_seq OWNED BY public.wagtailimages_rendition.id;


--
-- Name: wagtailredirects_redirect; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailredirects_redirect (
    id integer NOT NULL,
    old_path character varying(255) NOT NULL,
    is_permanent boolean NOT NULL,
    redirect_link character varying(255) NOT NULL,
    redirect_page_id integer,
    site_id integer
);


ALTER TABLE public.wagtailredirects_redirect OWNER TO postgres;

--
-- Name: wagtailredirects_redirect_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailredirects_redirect_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailredirects_redirect_id_seq OWNER TO postgres;

--
-- Name: wagtailredirects_redirect_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailredirects_redirect_id_seq OWNED BY public.wagtailredirects_redirect.id;


--
-- Name: wagtailsearchpromotions_searchpromotion; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailsearchpromotions_searchpromotion (
    id integer NOT NULL,
    sort_order integer,
    description text NOT NULL,
    page_id integer NOT NULL,
    query_id integer NOT NULL
);


ALTER TABLE public.wagtailsearchpromotions_searchpromotion OWNER TO postgres;

--
-- Name: wagtailsearch_editorspick_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailsearch_editorspick_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailsearch_editorspick_id_seq OWNER TO postgres;

--
-- Name: wagtailsearch_editorspick_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailsearch_editorspick_id_seq OWNED BY public.wagtailsearchpromotions_searchpromotion.id;


--
-- Name: wagtailsearch_query; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailsearch_query (
    id integer NOT NULL,
    query_string character varying(255) NOT NULL
);


ALTER TABLE public.wagtailsearch_query OWNER TO postgres;

--
-- Name: wagtailsearch_query_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailsearch_query_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailsearch_query_id_seq OWNER TO postgres;

--
-- Name: wagtailsearch_query_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailsearch_query_id_seq OWNED BY public.wagtailsearch_query.id;


--
-- Name: wagtailsearch_querydailyhits; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailsearch_querydailyhits (
    id integer NOT NULL,
    date date NOT NULL,
    hits integer NOT NULL,
    query_id integer NOT NULL
);


ALTER TABLE public.wagtailsearch_querydailyhits OWNER TO postgres;

--
-- Name: wagtailsearch_querydailyhits_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailsearch_querydailyhits_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailsearch_querydailyhits_id_seq OWNER TO postgres;

--
-- Name: wagtailsearch_querydailyhits_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailsearch_querydailyhits_id_seq OWNED BY public.wagtailsearch_querydailyhits.id;


--
-- Name: wagtailusers_userprofile; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.wagtailusers_userprofile (
    id integer NOT NULL,
    submitted_notifications boolean NOT NULL,
    approved_notifications boolean NOT NULL,
    rejected_notifications boolean NOT NULL,
    user_id integer NOT NULL,
    preferred_language character varying(10) NOT NULL,
    current_time_zone character varying(40) NOT NULL,
    avatar character varying(100) NOT NULL
);


ALTER TABLE public.wagtailusers_userprofile OWNER TO postgres;

--
-- Name: wagtailusers_userprofile_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.wagtailusers_userprofile_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.wagtailusers_userprofile_id_seq OWNER TO postgres;

--
-- Name: wagtailusers_userprofile_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.wagtailusers_userprofile_id_seq OWNED BY public.wagtailusers_userprofile.id;


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group ALTER COLUMN id SET DEFAULT nextval('public.auth_group_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions ALTER COLUMN id SET DEFAULT nextval('public.auth_group_permissions_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_permission ALTER COLUMN id SET DEFAULT nextval('public.auth_permission_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user ALTER COLUMN id SET DEFAULT nextval('public.auth_user_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups ALTER COLUMN id SET DEFAULT nextval('public.auth_user_groups_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions ALTER COLUMN id SET DEFAULT nextval('public.auth_user_user_permissions_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogindexrelatedlink ALTER COLUMN id SET DEFAULT nextval('public.blog_blogindexrelatedlink_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogpagetag ALTER COLUMN id SET DEFAULT nextval('public.blog_blogpagetag_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmark ALTER COLUMN id SET DEFAULT nextval('public.blog_bookmark_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarkplacement ALTER COLUMN id SET DEFAULT nextval('public.blog_bookmarkplacement_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarktag ALTER COLUMN id SET DEFAULT nextval('public.blog_bookmarktag_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_admin_log ALTER COLUMN id SET DEFAULT nextval('public.django_admin_log_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_content_type ALTER COLUMN id SET DEFAULT nextval('public.django_content_type_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_migrations ALTER COLUMN id SET DEFAULT nextval('public.django_migrations_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.taggit_tag ALTER COLUMN id SET DEFAULT nextval('public.taggit_tag_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.taggit_taggeditem ALTER COLUMN id SET DEFAULT nextval('public.taggit_taggeditem_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collection ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_collection_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collectionviewrestriction ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_collectionviewrestriction_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collectionviewrestriction_groups ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_collectionviewrestriction_groups_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_groupcollectionpermission ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_groupcollectionpermission_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_grouppagepermission ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_grouppagepermission_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_page ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_page_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pagerevision ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_pagerevision_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pageviewrestriction ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_pageviewrestriction_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pageviewrestriction_groups ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_pageviewrestriction_groups_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_site ALTER COLUMN id SET DEFAULT nextval('public.wagtailcore_site_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtaildocs_document ALTER COLUMN id SET DEFAULT nextval('public.wagtaildocs_document_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailembeds_embed ALTER COLUMN id SET DEFAULT nextval('public.wagtailembeds_embed_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailforms_formsubmission ALTER COLUMN id SET DEFAULT nextval('public.wagtailforms_formsubmission_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailimages_image ALTER COLUMN id SET DEFAULT nextval('public.wagtailimages_image_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailimages_rendition ALTER COLUMN id SET DEFAULT nextval('public.wagtailimages_rendition_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailredirects_redirect ALTER COLUMN id SET DEFAULT nextval('public.wagtailredirects_redirect_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearch_query ALTER COLUMN id SET DEFAULT nextval('public.wagtailsearch_query_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearch_querydailyhits ALTER COLUMN id SET DEFAULT nextval('public.wagtailsearch_querydailyhits_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearchpromotions_searchpromotion ALTER COLUMN id SET DEFAULT nextval('public.wagtailsearch_editorspick_id_seq'::regclass);


--
-- Name: id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailusers_userprofile ALTER COLUMN id SET DEFAULT nextval('public.wagtailusers_userprofile_id_seq'::regclass);


--
-- Data for Name: auth_group; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_group (id, name) FROM stdin;
1	Moderators
2	Editors
\.


--
-- Name: auth_group_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_group_id_seq', 2, true);


--
-- Data for Name: auth_group_permissions; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_group_permissions (id, group_id, permission_id) FROM stdin;
1	1	1
2	1	2
3	1	3
4	2	1
5	2	2
6	2	3
7	1	4
8	2	4
9	1	5
10	1	6
11	1	7
12	2	5
13	2	6
14	2	7
\.


--
-- Name: auth_group_permissions_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_group_permissions_id_seq', 14, true);


--
-- Data for Name: auth_permission; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_permission (id, name, content_type_id, codename) FROM stdin;
1	Can add image	2	add_image
2	Can change image	2	change_image
3	Can delete image	2	delete_image
4	Can access Wagtail admin	4	access_admin
5	Can add document	5	add_document
6	Can change document	5	change_document
7	Can delete document	5	delete_document
8	Can add log entry	6	add_logentry
9	Can change log entry	6	change_logentry
10	Can delete log entry	6	delete_logentry
11	Can add permission	7	add_permission
12	Can change permission	7	change_permission
13	Can delete permission	7	delete_permission
14	Can add group	8	add_group
15	Can change group	8	change_group
16	Can delete group	8	delete_group
17	Can add user	9	add_user
18	Can change user	9	change_user
19	Can delete user	9	delete_user
20	Can add content type	10	add_contenttype
21	Can change content type	10	change_contenttype
22	Can delete content type	10	delete_contenttype
23	Can add session	11	add_session
24	Can change session	11	change_session
25	Can delete session	11	delete_session
26	Can add Tag	12	add_tag
27	Can change Tag	12	change_tag
28	Can delete Tag	12	delete_tag
29	Can add Tagged Item	13	add_taggeditem
30	Can change Tagged Item	13	change_taggeditem
31	Can delete Tagged Item	13	delete_taggeditem
32	Can add Site	14	add_site
33	Can change Site	14	change_site
34	Can delete Site	14	delete_site
35	Can add page	1	add_page
36	Can change page	1	change_page
37	Can delete page	1	delete_page
38	Can add Page Revision	15	add_pagerevision
39	Can change Page Revision	15	change_pagerevision
40	Can delete Page Revision	15	delete_pagerevision
41	Can add Group Page Permission	16	add_grouppagepermission
42	Can change Group Page Permission	16	change_grouppagepermission
43	Can delete Group Page Permission	16	delete_grouppagepermission
44	Can add Page View Restriction	17	add_pageviewrestriction
45	Can change Page View Restriction	17	change_pageviewrestriction
46	Can delete Page View Restriction	17	delete_pageviewrestriction
47	Can add query	18	add_query
48	Can change query	18	change_query
49	Can delete query	18	delete_query
50	Can add Query Daily Hits	19	add_querydailyhits
51	Can change Query Daily Hits	19	change_querydailyhits
52	Can delete Query Daily Hits	19	delete_querydailyhits
59	Can add rendition	22	add_rendition
60	Can change rendition	22	change_rendition
61	Can delete rendition	22	delete_rendition
62	Can add User Profile	23	add_userprofile
63	Can change User Profile	23	change_userprofile
64	Can delete User Profile	23	delete_userprofile
65	Can add Embed	24	add_embed
66	Can change Embed	24	change_embed
67	Can delete Embed	24	delete_embed
68	Can add Redirect	25	add_redirect
69	Can change Redirect	25	change_redirect
70	Can delete Redirect	25	delete_redirect
71	Can add Form Submission	26	add_formsubmission
72	Can change Form Submission	26	change_formsubmission
73	Can delete Form Submission	26	delete_formsubmission
74	Can add home page	3	add_homepage
75	Can change home page	3	change_homepage
76	Can delete home page	3	delete_homepage
77	Can add blog page	27	add_blogpage
78	Can change blog page	27	change_blogpage
79	Can delete blog page	27	delete_blogpage
80	Can add blog index related link	28	add_blogindexrelatedlink
81	Can change blog index related link	28	change_blogindexrelatedlink
82	Can delete blog index related link	28	delete_blogindexrelatedlink
83	Can add blog index page	29	add_blogindexpage
84	Can change blog index page	29	change_blogindexpage
85	Can delete blog index page	29	delete_blogindexpage
86	Can add blog page tag	30	add_blogpagetag
87	Can change blog page tag	30	change_blogpagetag
88	Can delete blog page tag	30	delete_blogpagetag
89	Can add Search promotion	31	add_searchpromotion
90	Can change Search promotion	31	change_searchpromotion
91	Can delete Search promotion	31	delete_searchpromotion
92	Can add bookmark tag	32	add_bookmarktag
93	Can change bookmark tag	32	change_bookmarktag
94	Can delete bookmark tag	32	delete_bookmarktag
95	Can add bookmark	33	add_bookmark
96	Can change bookmark	33	change_bookmark
97	Can delete bookmark	33	delete_bookmark
98	Can add bookmark placement	34	add_bookmarkplacement
99	Can change bookmark placement	34	change_bookmarkplacement
100	Can delete bookmark placement	34	delete_bookmarkplacement
101	Can add bookmark page	35	add_bookmarkpage
102	Can change bookmark page	35	change_bookmarkpage
103	Can delete bookmark page	35	delete_bookmarkpage
104	Can add collection	36	add_collection
105	Can change collection	36	change_collection
106	Can delete collection	36	delete_collection
107	Can add group collection permission	37	add_groupcollectionpermission
108	Can change group collection permission	37	change_groupcollectionpermission
109	Can delete group collection permission	37	delete_groupcollectionpermission
110	Can add collection view restriction	38	add_collectionviewrestriction
111	Can change collection view restriction	38	change_collectionviewrestriction
112	Can delete collection view restriction	38	delete_collectionviewrestriction
113	Can view log entry	6	view_logentry
114	Can view group	8	view_group
115	Can view user	9	view_user
116	Can view permission	7	view_permission
117	Can view content type	10	view_contenttype
118	Can view session	11	view_session
119	Can view Tag	12	view_tag
120	Can view Tagged Item	13	view_taggeditem
121	Can view group page permission	16	view_grouppagepermission
122	Can view page	1	view_page
123	Can view page revision	15	view_pagerevision
124	Can view collection view restriction	38	view_collectionviewrestriction
125	Can view page view restriction	17	view_pageviewrestriction
126	Can view collection	36	view_collection
127	Can view site	14	view_site
128	Can view group collection permission	37	view_groupcollectionpermission
129	Can view Query Daily Hits	19	view_querydailyhits
130	Can view query	18	view_query
131	Can view image	2	view_image
132	Can view rendition	22	view_rendition
133	Can view document	5	view_document
134	Can view user profile	23	view_userprofile
135	Can view embed	24	view_embed
136	Can view redirect	25	view_redirect
137	Can view form submission	26	view_formsubmission
138	Can view search promotion	31	view_searchpromotion
139	Can view home page	3	view_homepage
140	Can view blog page tag	30	view_blogpagetag
141	Can view bookmark tag	32	view_bookmarktag
142	Can view blog index page	29	view_blogindexpage
143	Can view bookmark	33	view_bookmark
144	Can view bookmark placement	34	view_bookmarkplacement
145	Can view blog page	27	view_blogpage
146	Can view blog index related link	28	view_blogindexrelatedlink
147	Can view bookmark page	35	view_bookmarkpage
\.


--
-- Name: auth_permission_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_permission_id_seq', 147, true);


--
-- Data for Name: auth_user; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_user (id, password, last_login, is_superuser, username, first_name, last_name, email, is_staff, is_active, date_joined) FROM stdin;
2	pbkdf2_sha256$20000$ZZxIXgJGM80F$Pc9c/EMFYJoVejylVJSy16aLbMaZl7T3a60FVk0IB00=	2015-06-26 11:44:07.710834+00	f	editor	test	editor	fsdf@afsf.com	f	t	2015-06-26 11:43:59.876257+00
3	pbkdf2_sha256$20000$Mw4ozEWJU94V$ca0EErN9oUJr7m+RRZ4nui5iF964t/S5uXns0VaGnts=	2015-06-26 11:44:46.119709+00	f	moderator	test	moderator	fsd@fsdf.com	f	t	2015-06-26 11:44:38.283923+00
1	pbkdf2_sha256$120000$xlezHKkOBv97$gb2tpEIbce0Eenpr9+ZO5jVDcWwPOwp7e77ztO5hD10=	2019-12-13 15:29:23.152493+00	t	chrxr			chrxr@wefwe.com	t	t	2015-06-22 12:22:38.61379+00
\.


--
-- Data for Name: auth_user_groups; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_user_groups (id, user_id, group_id) FROM stdin;
1	2	2
2	3	1
\.


--
-- Name: auth_user_groups_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_user_groups_id_seq', 2, true);


--
-- Name: auth_user_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_user_id_seq', 3, true);


--
-- Data for Name: auth_user_user_permissions; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_user_user_permissions (id, user_id, permission_id) FROM stdin;
\.


--
-- Name: auth_user_user_permissions_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_user_user_permissions_id_seq', 1, false);


--
-- Data for Name: blog_blogindexpage; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.blog_blogindexpage (page_ptr_id, intro) FROM stdin;
\.


--
-- Data for Name: blog_blogindexrelatedlink; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.blog_blogindexrelatedlink (id, sort_order, link_external, title, link_page_id, page_id) FROM stdin;
\.


--
-- Name: blog_blogindexrelatedlink_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.blog_blogindexrelatedlink_id_seq', 1, false);


--
-- Data for Name: blog_blogpage; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.blog_blogpage (page_ptr_id, date, intro, body, main_image_id, subtitle, listing_image_id, listing_intro) FROM stdin;
6	2015-06-24	My name is Chris Rogers and I'm currently working as a project manager at Torchbox, a digital agency based in Oxford, England.	[{"type": "paragraph", "value": "<ul><li>Previously worked in digital publishing for 5 years, at Oxford University Press and Penguin Books.</li><li>Now a hands on project manager at Torchbox, developing websites and digital strategies for large multi-national charities.</li><li>Like to get my hands dirty helping with development where necessary.</li><li>Main experience is with Python, Django, JavaScript, CSS and putting these all together into Wagtail</li></ul>"}]	\N	A brief history of me	\N	
7	2015-06-29		[{"type": "paragraph", "value": "<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>"}, {"type": "real_codeblock", "value": {"code": "LOGGING = {\\r\\n    'version': 1,\\r\\n    'disable_existing_loggers': False,\\r\\n    'handlers': {\\r\\n        'console': {\\r\\n            'class': 'logging.StreamHandler',\\r\\n        },\\r\\n    },\\r\\n    'loggers': {\\r\\n        'django': {\\r\\n            'handlers': ['console'],\\r\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\r\\n        },\\r\\n    },\\r\\n}", "language": "python"}}, {"type": "paragraph", "value": "<p>Then run the following Heroku command to display application messages in real time:</p>"}, {"type": "real_codeblock", "value": {"code": "heroku logs --source app --tail", "language": "bash"}}, {"type": "paragraph", "value": "<h3><hr/></h3><h3>What's going on here?</h3><p>We'll break this down into each individual section:</p>"}, {"type": "real_codeblock", "value": {"code": "'version': 1", "language": "python"}}, {"type": "paragraph", "value": "<p>Identifies the format of the logging dictionary. Currently there is only 1 version available, but there could be more in the future.</p>"}, {"type": "real_codeblock", "value": {"code": "'disable_existing_loggers': False,", "language": "python"}}, {"type": "paragraph", "value": "<p>This indicates that we shouldn't disable the default logging configuration. The default is 'True', but it's not recommended to do this, as the default logs can be useful. Instead we will keep the default logs and redefine certain elements of them to output logs to the console.</p>"}, {"type": "real_codeblock", "value": {"code": "    'handlers': {\\r\\n        'console': {\\r\\n            'class': 'logging.StreamHandler',\\r\\n        },\\r\\n    },", "language": "python"}}, {"type": "paragraph", "value": "<p>There are four elements of a logging configuration:</p><p></p><ul><li>Loggers</li><li>Handlers</li><li>Filters</li><li>Formatters</li></ul><p>For our configuration we are only interested in the first two. The code above sets up our logging handler. We are declaring the console as a logging stream handler.</p><p></p>"}, {"type": "real_codeblock", "value": {"code": "    'loggers': {\\r\\n        'django': {\\r\\n            'handlers': ['console'],\\r\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\r\\n        },\\r\\n    },", "language": "python"}}, {"type": "paragraph", "value": "<p>Then finally we set up the logger itself, instruct it to utilise the 'console' handler, and set the minimum level of logging to 'ERROR'. This way we only see when things are actually wrong with our application, rather than logging all messages coming out of Django (the 'DEBUG' logging level).</p>"}, {"type": "paragraph", "value": "<p>For more information on Django logging\\u00a0<a href=\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\">see the documentation</a>.</p><p>For more information on Heroku logging\\u00a0<a href=\\"https://devcenter.heroku.com/articles/logging\\">see their support centre</a>.</p>"}]	\N		\N	<p>A simple code block to output Django error messages to the console</p>
9	2015-07-29		[{"type": "paragraph", "value": "<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>"}, {"type": "real_codeblock", "value": {"code": "import dj_database_url\\r\\nDATABASES['default'] =  dj_database_url.config()\\r\\n", "language": "python"}}, {"type": "paragraph", "value": "<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>"}, {"type": "real_codeblock", "value": {"code": "raise ImproperlyConfigured(\\"settings.DATABASES is improperly configured. \\"\\r\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\r\\nis improperly configured. \\r\\nPlease supply the ENGINE value. Check settings documentation for more details.\\r\\n", "language": "bash"}}, {"type": "paragraph", "value": "<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\"DATABASE_URL\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to settings.py to detect whether or not the environment variable was present:</p>"}, {"type": "real_codeblock", "value": {"code": "env = os.environ.copy()\\r\\ndb_url = env.get('DATABASE_URL', False)\\r\\n\\r\\nif db_url != False:\\r\\n    import dj_database_url\\r\\n    DATABASES['default'] =  dj_database_url.config()", "language": "python"}}, {"type": "paragraph", "value": "<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\"https://wiki.python.org/moin/KeyError\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\u00e0! My app now runs on both my local machine and Heroku.</p>"}]	\N	For when Heroku settings break your local build	\N	<p>For when Heroku settings break your local build</p>
17	2016-02-25		[]	\N	Make Google happy with you by serving news content quickly	\N	
18	2016-03-01		[{"type": "paragraph", "value": "<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across <a href=\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\">this article</a> by designer, Joshua Taylor, on Medium.</p>"}]	\N	It's not the designer's job to know what business objectives are, it's yours	\N	
14	2015-12-14	I'm the type of person who really needs a to-do list but is rubbish at maintaining one. I've tried many different solutions: specific apps, pen and paper, spreadsheets. None of them have lasted for longer than a couple of weeks.	[{"type": "paragraph", "value": "<p>Now though I think I've settled on a system that really works for me. I use Evernote ALL THE TIME. It's an essential tool for me. So I thought, how can I use the tool that I already use to help me organise my time.<br/></p><p>The key was to format it in an appropriate way. At the top of my list I have the 'week beginning' date. Then I have my current list of to-dos, ordered roughly by priority. I add EVERYTHING work-related that I have to do that is a discrete task, even small tasks. If I do something work-related that's not on the to-do list, I add it to the list. I use the checkbox list type, as it soothes my OCD side to see each box ticked off. Once a task has been ticked off, I copy and paste it into another list below the main list, which has today's date as the heading. So as the days go by you end up with an archive of your completed tasks for each individual day.</p><p>Visibility of the list is key. One of the biggest pains in trying to use a specific tool was that it was one more thing to have open on the screen. With Evernote, I just add it to my 'Shortcuts' list. Then it's always there in the top-left of Evernote, a tool that I would have open all the time anyway!</p>"}, {"type": "image", "value": 2}, {"type": "paragraph", "value": "<p>At the end of the month, I'll archive that particular note in a separate notebook, take it out of my shortcuts, and create a new to-do note for the month.</p><p>I've now started to expand the format to include other types of to-dos. I often review relevant newsletters and blogs for content as soon as I get into work, but then I like to actually read them over lunch. So I've created a 'Lunchtime reading' list, again with tick-boxes, and a 'Reading archive' at the bottom of the to-do note.</p>"}, {"type": "image", "value": 3}, {"type": "paragraph", "value": "<p>Anyway, it's a very simple solution, that appeals to me for it's 'Collect the completed tasks!' nature.</p>"}]	\N		\N	<p>Organise your day and manage your reading list better using Evernote</p>
19	2016-03-04		[{"type": "paragraph", "value": "<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented an RSS feed on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>"}, {"type": "real_codeblock", "value": {"code": "from django.db import models\\r\\nfrom django.contrib.syndication.views import Feed\\r\\nfrom blog.models import BlogPage\\r\\n\\r\\nclass BlogsFeed(Feed):\\r\\n    title = \\"My blog articles\\"\\r\\n    link = \\"/blogs-feed/\\"\\r\\n    description = \\"All of my blogs as they are published\\"\\r\\n\\r\\n    def items(self):\\r\\n        return BlogPage.objects.live().order_by('-date')\\r\\n\\r\\n    def item_title(self, item):\\r\\n        return item.title\\r\\n\\r\\n    def item_description(self, item):\\r\\n        return item.intro", "language": "python"}}, {"type": "paragraph", "value": "<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>"}, {"type": "real_codeblock", "value": {"code": "class BlogsFeed(Feed):", "language": "python"}}, {"type": "paragraph", "value": "<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\"http://chrxr.com/blog-feed\\">chrxr.com/blog-feed</a>. These are required!</p>"}, {"type": "real_codeblock", "value": {"code": "<title>My blog articles</title>\\r\\n<link>https://chrxr.com/blogs-feed/</link>\\r\\n<description>All of my blogs as they are published</description>", "language": "html"}}, {"type": "paragraph", "value": "<p>Following that we define three standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>"}, {"type": "real_codeblock", "value": {"code": "    def items(self):\\r\\n        return BlogPage.objects.live().order_by('-date')", "language": "python"}}, {"type": "paragraph", "value": "<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>"}, {"type": "real_codeblock", "value": {"code": "    def item_title(self, item):\\r\\n        return item.title\\r\\n\\r\\n    def item_description(self, item):\\r\\n        return item.intro", "language": "python"}}, {"type": "paragraph", "value": "<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>"}, {"type": "real_codeblock", "value": {"code": "    def get_absolute_url(self):\\r\\n        return self.full_url", "language": "python"}}, {"type": "paragraph", "value": "<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply import your Feed class into the\\u00a0<b>urls.py </b>file,\\u00a0\\u00a0then\\u00a0add the line below to the urlpatterns section.</p>"}, {"type": "real_codeblock", "value": {"code": "from blog_feed.feeds import BlogsFeed\\r\\n\\r\\nurlpatterns = [\\r\\n    #.... lots of URLs\\r\\n\\r\\n    url(r'blog-feed/$', BlogsFeed()),\\r\\n\\r\\n   # .... more URLs\\r\\n]", "language": "python"}}, {"type": "paragraph", "value": "<p>So, when someone visits the URL http://chrxr.com/blog-feed, Wagtail initialises a new instance of the class BlogFeed, which responds to the browser with the feed of links defined in the class methods.</p><p>If you have an RSS browser extension installed (like me), this might display the links in the feed as a nice list. If not you'll likely just see a load of XML. Generally though, as long as you don't see an error, then it's working!</p>"}, {"type": "image", "value": 6}, {"type": "paragraph", "value": "<h3>See it on GitHub</h3><p>This blog uses GitHub for version control, so the source code for anything I mention can generally <a href=\\"http://github.com/chrxr/blog_project\\">be found there</a>.</p><p></p><ul><li><a href=\\"https://github.com/chrxr/blog_project/blob/master/blog_feed/feeds.py#L25\\">feed.py file</a><br/></li><li><a href=\\"https://github.com/chrxr/blog_project/blob/master/blog_project/urls.py#L22\\">URL configuration</a></li></ul><p></p>"}]	\N		\N	<p>Add an RSS feed to Wagtail using Django's out-of-the-box functionality</p>
16	2016-02-22		[{"type": "paragraph", "value": "<p>I've been collating links as part of my <a id=\\"14\\" linktype=\\"page\\">reading list</a>\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that finding individual ones has become difficult.</p><p>So! I created a <a href=\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\">quick python script</a> to import the bookmarks from a <a href=\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p><p>By the way, you can now\\u00a0<a id=\\"15\\" linktype=\\"page\\">follow my reading list here</a>!</p>"}]	\N	I wanted to import some bookmarks, so I wrote a little script	\N	<p>A short script that imports content from a CSV file into Django / Wagtail CMS</p>
20	2016-03-09		[{"type": "paragraph", "value": "<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\u00a0Whilst discipline is something that can be enforced with training and checklists \\u2014 open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in a future post.<br/></p><p></p>"}]	\N	To Prince2 or not to Prince2	\N	<p>To Prince2 or not to Prince2</p>
22	2016-08-15		[{"type": "paragraph", "value": "<p>I wrote a small python script to allow the validation of single IIIF manifests, or a folder of manifests. It can be run via the command line, or imported as a module into another Python application.</p><p>The code, and full usage instructions can be <a href=\\"https://github.com/chrxr/IIIF-local-validator\\">found in the GitHub repository</a>.</p><p>The script utilises the IIIF manifest loader and factory scripts from the <a href=\\"https://github.com/IIIF/presentation-api\\">IIIF Presentation Implementations repository</a>, so mucho credit to <a href=\\"https://github.com/azaroth42\\">Rob Sanderson at Stanford for those</a>.</p>"}]	\N	I made a thingumabob 	\N	
24	2016-11-14	When employing a digital agency on a project, you can save time and money with a little pre-project preparation	[{"type": "paragraph", "value": "<p>Having been a project manager on both the agency and client side of big website builds, I've found that the smoothest projects and best agency client relationships are a result of good preparation on the client side. Here are five simple ways to get your relationship with your agency off to the best possible start.</p><h3>1 - Establish clear lines of communications</h3><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\u00a0meetings\\u00a0with the top-stakeholders\\u00a0at the beginning of the process\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>"}]	\N		\N	<p>When employing a digital agency on a project, you can save time and money with a little pre-project preparation</p>
4	2015-06-22	\N	[{"value": "<p></p><ul><li>This site is built using the <a href=\\"http://www.github.com/torchbox/wagtail\\">Wagtail CMS (v2.4)</a></li><li>It is hosted on a AWS t2.micro ec2 instance, running Ubuntu 16.04.</li><li>The server is <a href=\\"http://wiki.nginx.org/Main\\">Nginx</a> with <a href=\\"https://uwsgi-docs.readthedocs.org/en/latest/\\">uWSGI</a>.</li><li>The site is cached using <a href=\\"http://memcached.org/\\">Memcached</a>.</li><li>For the styling I&#x27;ve tried to stick to the<a href=\\"https://smacss.com/\\"> SMACSS</a> methodology.</li><li>It was originally deployed over a lunchtime using a combination of the <a href=\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\">Wagtail docs</a> and <a href=\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\"http://www.github.com/chrxr/blog_project\\">Github here</a>.</li></ul><p></p>", "type": "paragraph", "id": "450074c1-7864-431f-aaa3-b8272c3ee2ee"}]	\N	How this site was made, and what it was made with	\N	<p>How this site was made, and what it was made with</p>
25	2019-04-30	Recently I've been thinking about where my added value as a technical project manager lies.	[{"value": "<p>As a project manager, getting things done quicker is often the thing that&#x27;s most on my mind. This is great and as it should be!</p><p>However, as someone who likes to get his hands dirty, I&#x27;m often tempted to get into the nitty gritty of the work, to try to move things along faster. This almost always actually leads to projects going slower!</p><p>Whilst having the technical expertise that I have is a great advantage as a PM, it does not make me a professional developer. By switching my focus away from facilitating the work of others, I&#x27;m actually reducing their potential productivity. And one developer at full productivity is going to produce a bunch more quality work than me trying my best!</p><p>So, nowadays I like to say that the less I need to get involved in development, the more successful the project is. I know that my added value is in my ability to see the big picture, tying people and ideas together, motivating a team, and keeping a focus on customer needs.</p>", "type": "paragraph", "id": "31333200-43d1-4217-aa54-9237cc62c961"}]	\N	\N	\N	<p>Recently I&#x27;ve been thinking about where my added value as a technical project manager lies.</p>
23	2016-08-27	\N	[{"value": "<p>I&#x27;d been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\"https://www.vagrantup.com/docs/why-vagrant/\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn&#x27;t much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I&#x27;m going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distributions. I&#x27;m not a Windows user so you&#x27;ll have to do your own research if that&#x27;s what you need.</p><p>You will need <a href=\\"https://www.vagrantup.com/\\">Vagrant</a> and <a href=\\"https://www.virtualbox.org\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><ol><li>One or more <a href=\\"https://zookeeper.apache.org/\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The &#x27;Cloud&#x27; part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. Queries arrive at one of the nodes and the query is then forwarded to the node where a replica of the appropriate shard is located.</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p>You can find a basic introduction to how SolrCloud <a href=\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\">works on the Solr wiki</a>. For the rest of this article I&#x27;m going to assume you are aware of the basics.</p><p>Our aim for this test is to have each element of the SolrCloud setup running on its own virtual machine. Our setup will have three Solr nodes with which we can store and query the data, and a single Zookeeper instance to manage the nodes. We could have multiple Zookeeper instances to provide further redundancy. This would be called a <a href=\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\">Zookeeper ensemble</a>. However, for this initial test we&#x27;re going to stick with just the one.</p><p>Having three Solr nodes means that we can split our data into two shards with two replicas of each, and if one of the Solr nodes goes down we&#x27;ll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Our test network will be built using Ubuntu virtual machines. The first thing we&#x27;re going to do is create a new directory for our test VMs on our host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>", "type": "paragraph", "id": "1ebd142a-84f7-4091-b466-a2a5f35d7e5b"}, {"value": {"language": "bash", "code": "mkdir -p ~/solrcloud-test\\r\\ncd ~/solrcloud-test\\r\\nvagrant init ubuntu/trusty64"}, "type": "real_codeblock", "id": "56c7cf60-a22a-4f40-8bcc-a80ed71895fb"}, {"value": "This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. I'm going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). So, we need  to open the Vagrantfile and replace the line `config.vm.box = \\"ubuntu/trusty64\\"` with the instructions below:", "type": "markdown", "id": "98d24dbb-a9f4-420a-938d-54ce844c019b"}, {"value": {"language": "bash", "code": "config.vm.provider \\"virtualbox\\" do |v|\\r\\n  v.memory = 1024\\r\\n  v.cpus = 2\\r\\nend\\r\\n\\r\\nconfig.vm.define \\"zoo1\\" do |zoo1|\\r\\n  zoo1.vm.box = \\"ubuntu/trusty64\\"\\r\\n  zoo1.vm.network \\"private_network\\", type: \\"dhcp\\"\\r\\nend\\r\\n\\r\\nconfig.vm.define \\"solr1\\" do |solr1|\\r\\n  solr1.vm.box = \\"ubuntu/trusty64\\"\\r\\n  solr1.vm.network \\"private_network\\", type: \\"dhcp\\"\\r\\nend\\r\\n\\r\\nconfig.vm.define \\"solr2\\" do |solr2|\\r\\n  solr2.vm.box = \\"ubuntu/trusty64\\"\\r\\n  solr2.vm.network \\"private_network\\", type: \\"dhcp\\"\\r\\nend\\r\\n\\r\\nconfig.vm.define \\"solr3\\" do |solr3|\\r\\n  solr3.vm.box = \\"ubuntu/trusty64\\"\\r\\n  solr3.vm.network \\"private_network\\", type: \\"dhcp\\"\\r\\nend"}, "type": "real_codeblock", "id": "a02ee92d-450b-4aa7-93b6-8d25a6eba49e"}, {"value": "The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\r\\n\\r\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\"What is DCHP?\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\r\\n\\r\\nNow let's get these Vagrant boxes running.", "type": "markdown", "id": "5ed02442-b4c9-4f90-b906-b7bd8971a9be"}, {"value": {"language": "bash", "code": "cd ~/solrcloud-test\\r\\nvagrant up"}, "type": "real_codeblock", "id": "9ff9e702-22b3-4b2f-a3dc-f695acad6f3b"}, {"value": "The process of building the four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\r\\n\\r\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\r\\n\\r\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:", "type": "markdown", "id": "49257c11-ebf1-4e9c-abdc-580053599135"}, {"value": {"language": "bash", "code": "vagrant ssh zoo1\\r\\nvagrant ssh solr1\\r\\netc..."}, "type": "real_codeblock", "id": "bf12a1b4-011d-4b16-b593-c044d0bfc94f"}, {"value": "Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:", "type": "markdown", "id": "ebb36154-589f-4d1d-bb3f-5d3c53c903ff"}, {"value": {"language": "bash", "code": "eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\r\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\r\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\r\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\r\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\r\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\r\\n          collisions:0 txqueuelen:1000 \\r\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\r\\n\\r\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\r\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\r\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\r\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\r\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\r\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\r\\n          collisions:0 txqueuelen:1000 \\r\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\r\\n\\r\\nlo        Link encap:Local Loopback  \\r\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\r\\n          inet6 addr: ::1/128 Scope:Host\\r\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\r\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\r\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\r\\n          collisions:0 txqueuelen:0 \\r\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)"}, "type": "real_codeblock", "id": "a7c14ea9-9146-437a-9543-d82995ec9b16"}, {"value": "The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If we run this command on each box, we should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\"RFC 1918 -  Address Allocation for Private Internets\\"). For example, the addresses generated for my example are:\\r\\n\\r\\n* 172.28.128.3\\r\\n* 172.28.128.4\\r\\n* 172.28.128.5\\r\\n* 172.28.128.6\\r\\n\\r\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:", "type": "markdown", "id": "0ed9675b-32ae-4e0f-b03b-6c20603c915e"}, {"value": {"language": "bash", "code": "ssh vagrant@172.28.128.4"}, "type": "real_codeblock", "id": "a5763017-deef-4016-a54a-2c5fd6aa7ca7"}, {"value": "And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\"Iptables How To\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\r\\n\\r\\nSo, next we need to install the relevant software on each machine.", "type": "markdown", "id": "16f82195-41cf-4938-9439-fba31d853438"}, {"value": "<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we&#x27;re going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>", "type": "paragraph", "id": "29e8e843-d444-4821-8df4-5c043f5556a4"}, {"value": {"language": "bash", "code": "sudo add-apt-repository ppa:webupd8team/java\\r\\nsudo apt-get update\\r\\nsudo apt-get install oracle-java8-installer"}, "type": "real_codeblock", "id": "8cddfd50-aba7-4de7-83fc-1bd7f3b85d76"}, {"value": "<p>This installs both the JRE and JDK versions of Oracle&#x27;s official Java package. If you would prefer to use OpenJDK, <a href=\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\">you can follow the instructions here</a>.</p>", "type": "paragraph", "id": "1a1f1530-7ce6-477d-9abe-b76aae8edaae"}, {"value": "<h2>Installing Zookeeper</h2>", "type": "paragraph", "id": "907250f0-0090-49e8-b9b6-ab569008eb1b"}, {"value": "[As the website states](https://zookeeper.apache.org/ \\"Zookeeper home\\"), \\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\". For the purposes of SolrCloud, Zookeeper does the following:\\r\\n\\r\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\r\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\r\\n* Ensures the synchronisation of data between replicas of collection shards.\\r\\n\\r\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:", "type": "markdown", "id": "4d07ca3d-4882-4a29-9569-b63e249a2f3c"}, {"value": {"language": "bash", "code": "curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\r\\ntar -zxf zookeeper-3.4.8.tar.gz"}, "type": "real_codeblock", "id": "81ae7536-2df1-4c67-be41-592ac029af5b"}, {"value": "We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.", "type": "markdown", "id": "eccafba7-ccf9-4256-bcf9-793c6fc28d44"}, {"value": {"language": "bash", "code": "nano ~/zookeeper-3.4.8/conf/zoo.cfg"}, "type": "real_codeblock", "id": "11d0dd08-e15e-443f-9163-91b194fff0f7"}, {"value": "Now copy the following three lines into that file and save it.", "type": "markdown", "id": "f946537a-f152-463e-a7ee-9b94503082b7"}, {"value": {"language": "bash", "code": "tickTime=2000\\r\\ndataDir=/var/lib/zookeeper\\r\\nclientPort=2181"}, "type": "real_codeblock", "id": "e95324f9-720a-4d4c-91b1-58d44ca63945"}, {"value": "* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\r\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\r\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\r\\n\\r\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:", "type": "markdown", "id": "5b3b1d8c-c536-4257-925b-6e98dd9fcd08"}, {"value": {"language": "bash", "code": "sudo ~/zookeeper-3.4.8/bin/zkServer.sh start"}, "type": "real_codeblock", "id": "c9584d5b-68bf-4f31-a678-8eccf2160862"}, {"value": "<p>If all has gone well, you should see the following output in your terminal:</p>", "type": "paragraph", "id": "3271c0b7-d9fa-4a8e-ae69-4d82004970c6"}, {"value": {"language": "bash", "code": "ZooKeeper JMX enabled by default\\r\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\r\\nStarting zookeeper ... STARTED"}, "type": "real_codeblock", "id": "4effec9c-fb7e-419a-985c-0646c4e5eafd"}, {"value": "<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>", "type": "paragraph", "id": "4c97f7fc-eed4-4852-856f-fe2a66b72747"}, {"value": {"language": "bash", "code": "curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\r\\ntar -xzf solr-6.2.0.tgz"}, "type": "real_codeblock", "id": "6436cc2c-2e1d-4fc5-aeb7-60fa82be5fb1"}, {"value": "<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>", "type": "paragraph", "id": "31cd0aa9-b7a9-4b4b-a3da-7db5d15e7dfd"}, {"value": {"language": "bash", "code": "cd ~/solr-6.2.0\\r\\nbin/solr start"}, "type": "real_codeblock", "id": "f7f6949a-c5f9-4aab-9a57-ab60c4e94ba4"}, {"value": "Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\r\\n\\r\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\r\\n\\r\\nThe first thing to do is stop the node we currently have running.", "type": "markdown", "id": "b3006f8d-ead9-49bb-88f2-5d8d6dcd065f"}, {"value": {"language": "bash", "code": "bin/solr stop"}, "type": "real_codeblock", "id": "3523c902-135a-4429-8ef1-e59f5a7cf8e9"}, {"value": "Then we restart in cloud mode with the following command:", "type": "markdown", "id": "57e87a15-9b69-458e-81ff-900cfbfc967f"}, {"value": {"language": "bash", "code": "bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983"}, "type": "real_codeblock", "id": "ab95c913-da3b-432e-b84f-b1a71f5a62d1"}, {"value": "Let's break down the elements of this command:\\r\\n\\r\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\r\\n\\r\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\r\\n\\r\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.", "type": "markdown", "id": "fa1e9d45-627d-4290-b681-98acf7b8d380"}, {"value": "After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.", "type": "markdown", "id": "511a0930-15e8-4e0a-82a4-32cd0264743a"}, {"value": "<h2>Creating a test collection</h2><p>A &#x27;Collection&#x27; in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>", "type": "paragraph", "id": "a693e420-4a1e-4d4d-8535-e986c1df0031"}, {"value": {"language": "bash", "code": "bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 -replicationFactor 2"}, "type": "real_codeblock", "id": "66f233de-d788-43e9-a77e-aa2ecaea7f8a"}, {"value": "I'm not going to go into great detail on how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\r\\n\\r\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\r\\n\\r\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\r\\n\\r\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\r\\n\\r\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\r\\n\\r\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\").", "type": "markdown", "id": "6bc42cef-db3f-40b0-97b0-2c00e944a631"}, {"value": "<h2>Conclusion</h2><p>So now if you go to the &#x27;Cloud&#x27; section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p>There you have it, a working SolrCloud setup using Vagrant. We&#x27;ve got no data in our test collection, but adding in data isn&#x27;t SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>", "type": "paragraph", "id": "999c03d5-8c5d-4ffa-89b0-246414e750d2"}]	\N	Easy steps to emulate a multi-machine setup locally	\N	<p>Easy steps to emulate a multi-machine setup locally</p>
26	2019-06-27	Over the last two years, I’ve been working with Harvard SEAS faculty and cloud providers to determine how we can best facilitate students use of cloud technologies in the classroom.	[{"value": "<p></p><p>Over this time I have found that there are a few principles that cloud providers (e.g. AWS and GCP) who are hoping to penetrate the higher education market would do well to adhere to.</p><p>If cloud providers follow these principles when developing their tools and services for educational use then I believe they have a much greater chance of being adopted in the classroom.</p><h2><b>Most CS courses teach concepts, not tools</b></h2><p>The aim of most courses that utilize the cloud is not to teach the use of a particular tool or platform, but to teach a computing concept and to demonstrate its application.</p><p>When teaching these concepts, teaching time is invaluable. If you spend 2 weeks trying to make sure everybody in a class has an account and basic knowledge of how to use your platform, then that time is lost and cannot be made up.</p><p>For this reason, onboarding for the tool should be as seamless as possible. There should be a very clear pathway for students to learn how to navigate your platform. A dedicated set of short videos aimed at students that could be assigned as a homework 0 task would be ideal.</p><h2><b>It must be easy for TFs and other teaching staff to support their students</b></h2><p>Consider integrations with common LMS platforms to simplify adding students to resources and easily enable SSO.</p><p>Provide an administrative interface from which teaching staff can monitor students\\u2019 spend.</p><h2><b>Be generous with the amount of credit provided to each student.</b></h2><p>Every semester we see students using advanced, CPU and memory intensive operations earlier and earlier in their academic journey.</p><p>Many entry level tasks these days involve GPUs which cost upwards of 1$ per hour. It is not unreasonable to expect a student to use 200 - 300 hours of GPU time over a 4 month course. So, credit limits should be flexible.</p><p>It should be very easy for teaching staff, or even students themselves, to extend their credit limit if necessary.</p><h2><b>Students should never be charged</b></h2><p>I personally believe that asking students to sign-up with a credit card is potentially discriminatory.</p><p>In the past, we have seen students overspend on their accounts (for example, by leaving a GPU running, unused). This has led to many hours of teaching staff negotiating on their behalf to have bills cancelled. Making it as easy as possible for students and teaching staff to monitor student spend would help here.</p><p>Tools should also enable limiting the resources that a student can use, but these limits should be flexible or customizable.</p>", "type": "paragraph", "id": "9ff852dd-1f32-41f8-9664-7e560501ac8b"}]	\N	\N	\N	<p></p>
21	2017-05-20	\N	[{"value": "<p>Since March 2017 I have been working as the Director of Engineering in the Computing department of the <a href=\\"https://seas.harvard.edu\\">Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS)</a>. I manage a multi-disciplinary team of applications developers, dev-ops engineers and computational scientists. We create applications and manage infrastructure to support SEAS staff, faculty and students in both administrative and educational settings.</p><p>Previously I&#x27;ve worked at the Bodleian Library, part of Oxford University, as the Digital Projects Manager. Before that I worked as a project manager for the digital agency <a href=\\"http://torchbox.com\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><ul><li><a href=\\"http://rca.ac.uk\\">The Royal College of Art</a></li><li><a href=\\"http://royaldrawingschool.org\\">The Royal Drawing School</a></li><li><a href=\\"http://globalwitness.org.uk\\">Global Witness</a></li><li><a href=\\"http://plan-international.org.uk\\">Plan International</a></li><li><a href=\\"http://events.burton.com/\\">Burton Snowboards</a></li><li><a href=\\"http://election.kingsfund.org.uk/\\">The King&#x27;s Fund</a></li></ul><p>I was the original project manager for the development of the <a href=\\"https://wagtail.io\\">Wagtail CMS</a> open source project, and I still occasionally contribute to the project. I&#x27;m also a skateboarder of 18+ years.</p><p></p>", "type": "paragraph", "id": "df9dc178-a982-44ef-9de0-59e5c69f5201"}]	\N	Chris Rogers	\N	<p></p>
\.


--
-- Data for Name: blog_blogpagetag; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.blog_blogpagetag (id, content_object_id, tag_id) FROM stdin;
64	16	2
65	16	47
66	16	7
77	20	46
46	9	9
47	9	5
48	9	7
49	7	5
50	7	6
51	7	7
80	22	74
81	22	75
120	25	46
129	23	76
130	23	77
131	23	78
132	23	63
\.


--
-- Name: blog_blogpagetag_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.blog_blogpagetag_id_seq', 132, true);


--
-- Data for Name: blog_bookmark; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.blog_bookmark (id, url, title, notes, date_read) FROM stdin;
17	https://studio.uxpin.com/blog/what-all-ceos-should-know-about-design/?mkt_tok=3RkMMJWWfF9wsRolvarJZKXonjHpfsXw6uwvWqWylMI%2F0ER3fOvrPUfGjI4DRcRkI%2BSLDwEYGJlv6SgFS7PGMbJiz7gFXxI%3D	What CEOs Should Know About Design	<p>Originally from Medium, posted on UX-Pin. Article written by Irene Au, former head of design at Google. </p><p></p><ol><li>Design with empathy.</li><li>Keep your values, strategy and principles in mind.</li><li>"There is only careless design or thoughtful design."</li></ol><p></p>	2016-02-29
12	https://moz.com/blog/content-audit-tutorial	How to do a content audit	<p>From Moz. Super useful instructional blog on how to do a content audit from Everett Sizemore at <a href="http://www.goinflow.com">Inflow</a>. Very in-depth, well worth reading.</p>	2015-11-27
9	https://medium.com/the-job-to-be-done/replacing-the-user-story-with-the-job-story-af7cdee10c27#.cz8h59k79	Replacing the user story with the job story	<p>Via Medium. Changes the semantics around a user story to try to get a more realistic and helpful description of a user task. A useful approach.</p>	2015-11-26
23	http://awealthofcommonsense.com/2016/02/bogle-vs-golitath/	Bogle vs Goliath	<p>Major US college endowment investment funds did no better than the Vanguard Three Fund Portfolio.</p>	2016-03-01
13	http://www.gallup.com/opinion/chairman/171302/employee-satisfaction-doesn-matter.aspx	Employee satisfaction doesn't matter	<p>via <a href="https://news.ycombinator.com/">HackerNews</a>. Don't just give your employees free lunch and barbecues on a Friday. Engage them with interesting work and invest in their development.</p>	2016-02-24
8	http://www.uxbooth.com/articles/overcoming-the-agile-ux-divide/	Overcoming the Agile / UCD divide	<p>From UXBooth. A really good introduction to implementing agile methodologies whilst still maintaining a user centred design approach. Light on detail with regards costing such a project, but full of good tips.</p>	2015-11-26
6	https://24ways.org/2015/get-expressive-with-your-typography/	Get expressive with your typography	<p>From 24 Ways. A bit of an inspiration piece, rather than an informative piece. Explains, briefly, the difference between workhorse typefaces and display typefaces.</p>	2015-11-27
7	https://studio.uxpin.com/blog/communicating-with-designers-means-speaking-their-language	How to Quickly Explain Your Website Vision to Designers	<p>From UX Pin. TL;DR: Design brief is a conversation, not instruction. Create prioritised list of content on page, then build basic wireframes to stimulate the conversation.</p>	2015-11-26
18	https://medium.com/@userfocus/the-7-deadly-sins-of-user-research-22857c5a971b#.ntcjy467x	The 7 deadly sins of user research	<p>Credulity, Dogmatism, Bias, Obscurantism, Laziness, Vagueness, Hubris<br/></p><p>"People don’t have reliable insight into their mental processes, so there is no point asking them what they want."<br/></p>	2016-02-29
14	http://www.themacro.com/articles/2016/01/minimum-viable-product-process/	A Minimum Viable Product Is Not a Product, It’s a Process	<p>Via <a href="http://www.themacro.com">The Macro</a>. An excellent summary of the MVP process.</p><p></p><ol><li>What's my riskiest assumption?</li><li>What's the smallest experiment I can do to test this assumption? </li></ol><p></p>	2016-02-24
15	http://www.infoworld.com/article/2825890/application-development/why-redis-beats-memcached-for-caching.html	Why Redis beats Memcached for caching	<p>Use Memcached if you're caching small stuff, like static HTML snippets. Use Redis for everything else.</p>	2016-02-26
16	https://www.nginx.com/blog/nginx-caching-guide/	A guide to caching with NGINX	<p>Easy to follow guide to the implementation of caching using NGINX's built-in features.</p>	2016-02-26
5	http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/	Digital marketing and measurement tools	<p>Avinash Kaushik is an analytics expert who has had a profound effect on the world of digital marketing and tracking success. This post from his blog "<a href="http://www.kaushik.net/">Occam's Razor</a>" describes the "Digital Marketing &amp; Measurement Model". This model provides a framework for defining the goals for a website, based on business objectives. It encourages you to think specifically about KPIs for these goals, and allows you to plan how these will be measured.</p>	2015-11-27
19	http://tech.trivago.com/2016/02/02/large-scale-css-refactoring-at-trivago/	Large scale CSS refactoring at Trivago	<p>A case study documenting what can be gained by refactoring a large CSS codebase. It takes a lot of effort, but you save huge amounts of time, energy and money in the long run, and enable possibilities and options that would have been impossible in the past.</p>	2016-02-29
20	http://www.secnetix.de/olli/Python/lambda_functions.hawk	Lambda functions in Python (with lots of examples)	<p>A really good, but quite old, tutorial.</p>	2016-02-29
21	http://developerblog.redhat.com/2016/02/24/10-things-to-avoid-in-docker-containers/	10 things to avoid in docker containers	<p>Clue's in the name. I'm just starting on my explorations of Docker, but this looks like a useful list to keep in mind.</p>	2016-03-01
22	http://introjs.com/	Intro.js	<p> A library to easily add instructional tooltips to your website. Thinking about trying this on Wagtail.</p>	2016-03-01
2	http://www.newyorker.com/magazine/2016/02/29/mr-money-mustache-the-frugal-guru	The Scold: Mr. Money Mustache’s retirement (sort of) plan	<p>I'd never heard of Mr MM before, and although his level of frugality doesn't appeal to me (sometimes I just like to buy stuff), his ideas on saving and limited consumption are very interesting.</p>	2016-02-22
10	https://getflywheel.com/layout/your-homepage/	Your homepage is not your site's lobby	<p>From Flywheel. Use data to work out where people actually want to be on your website. Instead of using your homepage to showcase content, move people quickly to where they want to go.</p><p>Also, recognise that your homepage will often not be the key landing page. Make sure these other landing pages are frequently updated, and sprinkle some design magic on them as well.</p>	2015-11-27
11	https://www.linkedin.com/pulse/pursuit-perfect-ux-portfolio-andy-fitzgerald-phd	In Pursuit of the Perfect UX Portfolio	<p>Via LinkedIn. This is something I've struggled with for ages, and blogs like this come up on my reading list quite often. UX designer Andy Fitzgerald provides an excellent three-point list for how to make a good portfolio site:</p><p></p><ol><li>Make it user centred.</li><li>Make it navigable</li><li>Fill it with stories!</li></ol><p></p>	2015-11-27
24	https://zachholman.com/posts/deploying-software#goals	Deploying software	<p>A long post outlining major things to consider when developing a deployment workflow. Very useful.</p>	2016-03-02
25	http://www.creativebloq.com/web-design/manage-large-css-projects-itcss-101517528	What is ITCSS?	<p>ITCSS = Inverted Triangle CSS</p><p>This is a model developed by the consultant Harry Roberts. It's based on the idea that your CSS should be structured by defined metrics (specificity, reach, explicitness) rather than thematic groups.</p><p>I can see how this would work for a single site, or family of sites that share the same theme, but I can imagine it getting much more complicated when you have multiple sites or sections of a site with quite different themes sharing the same settings, tools, generic and element layers. Would be interesting to know if anyone has tried to combine this with Atomic design.</p>	2016-03-03
26	http://codingvc.com/startups-are-risk-bundles/	Startups are risk bundles	<p>Description of how investors may assess the risks associated with a start-up when doing a valuation.</p>	2016-03-03
27	https://articles.uie.com/three_hund_million_button/	The $300 Million Button	<p>A case study describing how changing a button on a page resulted in a $300m increase in annual profit. The lesson is, don't assume you know how your customers think. Test, test, test.</p>	2016-03-07
28	http://www.darrenmothersele.com/blog/2015/11/16/surviving-open-source-gentrification/	How to survive gentrification of the Drupal community	<p>Something to think about in all OSS communities. How do we keep participation up.</p>	2016-03-08
4	http://conversionxl.com/how-to-use-priming-to-improve-ux	How to use priming to improve UX	<p>More of a review of the effectiveness of priming, with some interesting case studies, rather than a how-to guide. Useful background on some criticisms of priming. </p><p>TL;DR: Priming is a 'nudge' towards a desired behaviour, rather than a concrete cause/effect stimulus.<br/></p>	2016-02-22
29	https://www.upguard.com/articles/ansible-puppet	Ansible vs Puppet	<p>A brief introduction to two of the most popular configuration management and remote execution tools. Provides basic pros and cons. Not in-depth. <br/></p>	2016-03-10
30	https://tech.scrunch.com/blog/lessons-learned-from-a-year-of-running-elasticsearch-in-production/	Lessons learned from a year of running elasticsearch in production	<p>Highly technical, but good advise for running elasticsearch at scale.</p>	2016-03-14
31	http://www.howtogeek.com/168147/add-public-ssh-key-to-remote-server-in-a-single-command/	Add public key to remote server with a single command	<p>Very useful one-liner to remember.</p>	2016-03-14
32	http://blogs.adobe.com/creativecloud/experts-weigh-in-what-are-your-non-negotiables-when-it-comes-to-designing-great-user-experiences/?utm_content=bufferd92cf&utm_medium=social&utm_source=twitter.com&	Great user experiences: non-negotiables	<p>A good list of things that should always be taken into consideration when designing.</p>	2016-03-14
33	http://jvns.ca/blog/2016/03/16/tcpdump-is-amazing/	tcpdump is amazing	<p>Article from Julia Evans' blog. I'v never used tcpdump before, and tbh this blog doesn't get enough traffic to warrant it :) but this article gives a good, simple introduction to its use.</p>	2016-03-17
34	https://medium.com/@letsworkshop/david-ogilvy-s-20-unconventional-rules-for-getting-clients-319f9abed7d5#.rp6m0zf82	David Ogilvy’s 20 unconventional rules for getting clients	<p>I'm not sure how 'unconventional' these are, but there's some excellent advice here. I think the personality point (18) is particularly important. You don't always win clients on your results. You need to sell your services effectively.</p>	2016-03-21
35	http://www.yourcoach.be/blog/wp-content/uploads/2014/02/Dale-Carnegie-How-to-win-friends-and-influence-people.pdf	How to win friends and influence people	<p>A decent PDF version of the classic 1930s book by Dale Carnegie.</p>	2016-03-21
36	https://code.facebook.com/posts/1737605303120405/dragon-a-distributed-graph-query-engine/	Dragon: A distributed graph query engine	<p>Information regarding Dragon, Facebook's new engine for for querying the social graph. The new engine significantly reduces latency on complex queries.</p>	2016-03-22
37	https://www.smashingmagazine.com/2016/04/web-developer-guide-color/	Web developer color guide	<p>An easy to follow guide to picking great colours for websites, without having to delve into the science of colour theory.</p>	2016-04-05
38	http://www.joelonsoftware.com/items/2007/10/26.html	Evidence based scheduling	<p>Joel Spolsky's original, ground-breaking article on how to get a realistic schedule based on estimates, and how to help developers get better at estimating in general. </p>	2016-04-05
39	https://samphippen.com/agile-etc/	Agile, etc	<p>A long, but well written write-up of a talk on developing applications for clients.</p>	2016-04-05
40	http://stevehanov.ca/blog/index.php?id=132	The multi-armed bandit	<p>A description of the algorithm behind effective multi-variant testing, and why it works.</p>	2016-04-06
\.


--
-- Name: blog_bookmark_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.blog_bookmark_id_seq', 40, true);


--
-- Data for Name: blog_bookmarkpage; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.blog_bookmarkpage (page_ptr_id, intro) FROM stdin;
15	
\.


--
-- Data for Name: blog_bookmarkplacement; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.blog_bookmarkplacement (id, page_id, quote_id) FROM stdin;
\.


--
-- Name: blog_bookmarkplacement_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.blog_bookmarkplacement_id_seq', 1, false);


--
-- Data for Name: blog_bookmarktag; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.blog_bookmarktag (id, content_object_id, tag_id) FROM stdin;
122	10	40
123	10	44
124	10	66
125	10	19
126	10	28
127	12	40
128	12	66
129	12	44
130	12	46
131	11	66
132	11	19
133	23	15
134	24	54
135	24	63
136	25	67
137	25	60
138	25	61
139	26	68
140	27	19
141	28	71
142	4	19
143	4	20
144	29	63
145	30	72
146	31	54
147	32	19
148	33	54
149	34	68
150	35	68
151	36	9
152	37	19
35	6	25
36	6	26
153	37	28
154	38	46
155	39	49
156	39	46
41	7	27
42	7	19
43	7	28
44	7	31
157	40	19
81	9	35
82	9	19
83	9	36
84	9	46
85	9	38
90	13	50
91	14	51
92	14	52
93	14	53
94	15	56
95	15	54
96	15	55
97	16	1
98	16	54
99	16	57
100	5	24
101	5	40
102	5	21
103	5	22
104	17	58
105	17	28
106	18	59
107	18	19
108	19	19
109	19	60
110	19	61
111	20	47
112	21	62
113	21	63
114	22	64
115	22	65
116	2	16
117	8	33
118	8	19
119	8	49
\.


--
-- Name: blog_bookmarktag_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.blog_bookmarktag_id_seq', 157, true);


--
-- Data for Name: django_admin_log; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.django_admin_log (id, action_time, object_id, object_repr, action_flag, change_message, content_type_id, user_id) FROM stdin;
\.


--
-- Name: django_admin_log_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.django_admin_log_id_seq', 1, false);


--
-- Data for Name: django_content_type; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.django_content_type (id, app_label, model) FROM stdin;
1	wagtailcore	page
2	wagtailimages	image
3	home	homepage
4	wagtailadmin	admin
5	wagtaildocs	document
6	admin	logentry
7	auth	permission
8	auth	group
9	auth	user
10	contenttypes	contenttype
11	sessions	session
12	taggit	tag
13	taggit	taggeditem
14	wagtailcore	site
15	wagtailcore	pagerevision
16	wagtailcore	grouppagepermission
17	wagtailcore	pageviewrestriction
18	wagtailsearch	query
19	wagtailsearch	querydailyhits
22	wagtailimages	rendition
23	wagtailusers	userprofile
24	wagtailembeds	embed
25	wagtailredirects	redirect
26	wagtailforms	formsubmission
27	blog	blogpage
28	blog	blogindexrelatedlink
29	blog	blogindexpage
30	blog	blogpagetag
31	wagtailsearchpromotions	searchpromotion
32	blog	bookmarktag
33	blog	bookmark
34	blog	bookmarkplacement
35	blog	bookmarkpage
36	wagtailcore	collection
37	wagtailcore	groupcollectionpermission
38	wagtailcore	collectionviewrestriction
\.


--
-- Name: django_content_type_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.django_content_type_id_seq', 38, true);


--
-- Data for Name: django_migrations; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.django_migrations (id, app, name, applied) FROM stdin;
1	contenttypes	0001_initial	2015-06-22 12:09:58.602686+00
2	auth	0001_initial	2015-06-22 12:09:58.82727+00
3	admin	0001_initial	2015-06-22 12:09:58.907559+00
4	contenttypes	0002_remove_content_type_name	2015-06-22 12:09:59.043245+00
5	auth	0002_alter_permission_name_max_length	2015-06-22 12:09:59.100998+00
6	auth	0003_alter_user_email_max_length	2015-06-22 12:09:59.147698+00
7	auth	0004_alter_user_username_opts	2015-06-22 12:09:59.190058+00
8	auth	0005_alter_user_last_login_null	2015-06-22 12:09:59.236258+00
9	auth	0006_require_contenttypes_0002	2015-06-22 12:09:59.240648+00
10	taggit	0001_initial	2015-06-22 12:09:59.365278+00
11	wagtailimages	0001_initial	2015-06-22 12:09:59.636347+00
12	wagtailcore	0001_initial	2015-06-22 12:10:00.752719+00
13	wagtailcore	0002_initial_data	2015-06-22 12:10:00.756567+00
14	wagtailcore	0003_add_uniqueness_constraint_on_group_page_permission	2015-06-22 12:10:00.76006+00
15	wagtailcore	0004_page_locked	2015-06-22 12:10:00.763789+00
16	wagtailcore	0005_add_page_lock_permission_to_moderators	2015-06-22 12:10:00.767682+00
17	wagtailcore	0006_add_lock_page_permission	2015-06-22 12:10:00.770359+00
18	wagtailcore	0007_page_latest_revision_created_at	2015-06-22 12:10:00.772416+00
19	wagtailcore	0008_populate_latest_revision_created_at	2015-06-22 12:10:00.774412+00
20	wagtailcore	0009_remove_auto_now_add_from_pagerevision_created_at	2015-06-22 12:10:00.776298+00
21	wagtailcore	0010_change_page_owner_to_null_on_delete	2015-06-22 12:10:00.77811+00
22	wagtailcore	0011_page_first_published_at	2015-06-22 12:10:00.780006+00
23	wagtailcore	0012_extend_page_slug_field	2015-06-22 12:10:00.782012+00
24	wagtailcore	0013_update_golive_expire_help_text	2015-06-22 12:10:00.783949+00
25	wagtailcore	0014_add_verbose_name	2015-06-22 12:10:00.785942+00
26	wagtailcore	0015_add_more_verbose_names	2015-06-22 12:10:00.788525+00
27	wagtailcore	0016_change_page_url_path_to_text_field	2015-06-22 12:10:00.791725+00
28	wagtailimages	0002_initial_data	2015-06-22 12:10:00.825165+00
29	wagtailimages	0003_fix_focal_point_fields	2015-06-22 12:10:01.226604+00
30	wagtailimages	0004_make_focal_point_key_not_nullable	2015-06-22 12:10:01.372388+00
31	wagtailimages	0005_make_filter_spec_unique	2015-06-22 12:10:01.478854+00
32	wagtailimages	0006_add_verbose_names	2015-06-22 12:10:01.958722+00
33	blog	0001_initial	2015-06-22 12:10:02.008631+00
34	blog	0002_blogpage_main_image	2015-06-22 12:10:02.059909+00
35	blog	0003_blogindexpage_blogindexrelatedlink	2015-06-22 12:10:02.181692+00
36	blog	0004_auto_20150613_1908	2015-06-22 12:10:02.237153+00
37	blog	0005_auto_20150613_1920	2015-06-22 12:10:02.395017+00
38	blog	0006_auto_20150621_1957	2015-06-22 12:10:02.447501+00
39	blog	0007_auto_20150621_2048	2015-06-22 12:10:02.700176+00
40	home	0001_initial	2015-06-22 12:10:02.751632+00
41	home	0002_create_homepage	2015-06-22 12:10:02.778974+00
42	home	0003_homepage_body	2015-06-22 12:10:02.841139+00
43	sessions	0001_initial	2015-06-22 12:10:02.860737+00
44	wagtailadmin	0001_create_admin_access_permissions	2015-06-22 12:10:02.876649+00
45	wagtaildocs	0001_initial	2015-06-22 12:10:02.935824+00
46	wagtaildocs	0002_initial_data	2015-06-22 12:10:02.959365+00
47	wagtaildocs	0003_add_verbose_names	2015-06-22 12:10:03.14392+00
48	wagtailembeds	0001_initial	2015-06-22 12:10:03.183785+00
49	wagtailembeds	0002_add_verbose_names	2015-06-22 12:10:03.19551+00
50	wagtailforms	0001_initial	2015-06-22 12:10:03.26866+00
51	wagtailforms	0002_add_verbose_names	2015-06-22 12:10:03.374826+00
52	wagtailredirects	0001_initial	2015-06-22 12:10:03.453342+00
53	wagtailredirects	0002_add_verbose_names	2015-06-22 12:10:03.742741+00
54	wagtailsearch	0001_initial	2015-06-22 12:10:04.171133+00
55	wagtailsearch	0002_add_verbose_names	2015-06-22 12:10:04.934461+00
56	wagtailusers	0001_initial	2015-06-22 12:10:05.103543+00
57	wagtailusers	0002_add_verbose_name_on_userprofile	2015-06-22 12:10:05.57202+00
58	wagtailusers	0003_add_verbose_names	2015-06-22 12:10:05.729976+00
59	blog	0008_auto_20150624_1226	2015-06-25 12:09:38.20533+00
60	blog	0009_auto_20150630_0644	2015-06-30 06:54:31.195862+00
61	blog	0010_auto_20150707_2114	2015-07-07 21:33:39.682266+00
62	wagtailcore	0017_change_edit_page_permission_description	2015-09-27 16:02:21.137205+00
63	wagtailcore	0018_pagerevision_submitted_for_moderation_index	2015-09-27 16:02:21.21893+00
64	wagtailcore	0019_verbose_names_cleanup	2015-09-27 16:02:21.435581+00
65	wagtailimages	0007_image_file_size	2015-09-27 16:02:21.498842+00
66	wagtailimages	0008_image_created_at_index	2015-09-27 16:02:21.55866+00
67	wagtailsearch	0003_remove_editors_pick	2015-09-27 16:02:21.626505+00
68	wagtailsearchpromotions	0001_initial	2015-09-27 16:02:22.179642+00
69	wagtailcore	0020_add_index_on_page_first_published_at	2016-02-18 21:08:34.385842+00
70	wagtailcore	0021_capitalizeverbose	2016-02-18 21:08:36.319065+00
71	wagtailcore	0022_add_site_name	2016-02-18 21:08:36.357671+00
72	wagtailcore	0023_alter_page_revision_on_delete_behaviour	2016-02-18 21:08:36.409003+00
73	blog	0011_auto_20160218_2034	2016-02-18 21:08:36.721781+00
74	blog	0012_auto_20160218_2036	2016-02-18 21:08:36.793653+00
75	blog	0013_bookmarkpage	2016-02-18 21:08:36.904438+00
76	wagtaildocs	0004_capitalizeverbose	2016-02-18 21:08:37.57246+00
77	wagtailembeds	0003_capitalizeverbose	2016-02-18 21:08:37.597194+00
78	wagtailforms	0003_capitalizeverbose	2016-02-18 21:08:37.949558+00
79	wagtailimages	0009_capitalizeverbose	2016-02-18 21:08:38.456938+00
80	wagtailimages	0010_change_on_delete_behaviour	2016-02-18 21:08:38.545931+00
81	wagtailredirects	0003_make_site_field_editable	2016-02-18 21:08:38.70582+00
82	wagtailredirects	0004_set_unique_on_path_and_site	2016-02-18 21:08:39.001436+00
83	wagtailredirects	0005_capitalizeverbose	2016-02-18 21:08:39.795168+00
84	wagtailsearchpromotions	0002_capitalizeverbose	2016-02-18 21:08:40.218237+00
85	wagtailusers	0004_capitalizeverbose	2016-02-18 21:08:40.765641+00
86	blog	0014_auto_20160220_1133	2016-02-20 11:39:39.243869+00
87	taggit	0002_auto_20150616_2121	2016-02-25 13:04:53.404898+00
88	home	0004_auto_20160228_1927	2016-02-28 19:29:27.35929+00
89	wagtailcore	0024_collection	2016-03-09 17:05:49.063853+00
90	wagtailcore	0025_collection_initial_data	2016-03-09 17:05:49.116465+00
91	wagtailcore	0026_group_collection_permission	2016-03-09 17:05:49.386741+00
92	wagtailcore	0027_fix_collection_path_collation	2016-03-09 17:05:49.414174+00
93	wagtailcore	0024_alter_page_content_type_on_delete_behaviour	2016-03-09 17:05:49.576145+00
94	wagtailcore	0028_merge	2016-03-09 17:05:49.579203+00
95	wagtaildocs	0005_document_collection	2016-03-09 17:05:49.679706+00
96	wagtaildocs	0006_copy_document_permissions_to_collections	2016-03-09 17:05:49.721457+00
97	wagtaildocs	0005_alter_uploaded_by_user_on_delete_action	2016-03-09 17:05:49.939196+00
98	wagtaildocs	0007_merge	2016-03-09 17:05:49.943921+00
99	wagtailimages	0011_image_collection	2016-03-09 17:05:50.116687+00
100	wagtailimages	0012_copy_image_permissions_to_collections	2016-03-09 17:05:50.158232+00
101	wagtailimages	0013_make_rendition_upload_callable	2016-05-24 06:47:48.182349+00
102	wagtailcore	0029_unicode_slugfield_dj19	2016-09-01 08:29:20.548031+00
103	wagtailcore	0030_index_on_pagerevision_created_at	2016-10-22 13:48:10.793655+00
104	wagtailimages	0014_add_filter_spec_field	2016-10-22 13:48:11.008517+00
105	wagtailimages	0015_fill_filter_spec_field	2016-10-22 13:48:11.027331+00
106	wagtailusers	0005_make_related_name_wagtail_specific	2016-10-22 13:48:11.156346+00
107	wagtailcore	0031_add_page_view_restriction_types	2017-12-19 16:25:40.449296+00
108	wagtailcore	0032_add_bulk_delete_page_permission	2017-12-19 16:25:40.525196+00
109	wagtailimages	0016_deprecate_rendition_filter_relation	2017-12-19 16:25:40.862746+00
110	wagtailimages	0017_reduce_focal_point_key_max_length	2017-12-19 20:06:43.733224+00
111	wagtailimages	0018_remove_rendition_filter	2017-12-19 20:06:43.842362+00
112	wagtailcore	0033_remove_golive_expiry_help_text	2017-12-19 20:10:36.482421+00
113	wagtailimages	0019_delete_filter	2017-12-19 20:10:36.500484+00
114	wagtailusers	0006_userprofile_prefered_language	2017-12-19 20:10:36.599971+00
115	wagtailcore	0034_page_live_revision	2017-12-19 21:43:55.87413+00
116	wagtailcore	0035_page_last_published_at	2017-12-19 21:43:55.94466+00
117	wagtailcore	0036_populate_page_last_published_at	2017-12-19 21:43:55.956606+00
118	wagtailcore	0037_set_page_owner_editable	2017-12-19 21:43:56.063035+00
119	wagtailcore	0038_make_first_published_at_editable	2017-12-19 21:43:56.13271+00
120	wagtailcore	0039_collectionviewrestriction	2017-12-19 21:43:56.348368+00
121	wagtailcore	0040_page_draft_title	2017-12-19 21:59:41.276474+00
122	admin	0002_logentry_remove_auto_add	2018-03-10 22:24:40.347449+00
123	auth	0007_alter_validators_add_error_messages	2018-03-10 22:24:40.384553+00
124	auth	0008_alter_user_username_max_length	2018-03-10 22:24:40.427965+00
125	wagtailcore	0001_squashed_0016_change_page_url_path_to_text_field	2018-03-10 22:24:40.432382+00
126	admin	0003_logentry_add_action_flag_choices	2019-03-21 17:36:28.321588+00
127	auth	0009_alter_user_last_name_max_length	2019-03-21 17:36:28.341576+00
128	wagtailcore	0041_group_collection_permissions_verbose_name_plural	2019-03-21 17:36:28.356958+00
129	wagtaildocs	0008_document_file_size	2019-03-21 17:36:28.37853+00
130	wagtaildocs	0009_document_verbose_name_plural	2019-03-21 17:36:28.399191+00
131	wagtaildocs	0010_document_file_hash	2019-03-21 17:36:28.431043+00
132	wagtailembeds	0004_embed_verbose_name_plural	2019-03-21 17:36:28.436643+00
133	wagtailimages	0020_add-verbose-name	2019-03-21 17:36:28.455751+00
134	wagtailimages	0021_image_file_hash	2019-03-21 17:36:28.490218+00
135	wagtailredirects	0006_redirect_increase_max_length	2019-03-21 17:36:28.508497+00
136	wagtailsearch	0004_querydailyhits_verbose_name_plural	2019-03-21 17:36:28.515333+00
137	wagtailusers	0007_userprofile_current_time_zone	2019-03-21 17:36:28.536975+00
138	wagtailusers	0008_userprofile_avatar	2019-03-21 17:36:28.559807+00
139	wagtailusers	0009_userprofile_verbose_name_plural	2019-03-21 17:36:28.573217+00
140	wagtailimages	0001_squashed_0021	2019-03-21 17:36:28.576909+00
\.


--
-- Name: django_migrations_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.django_migrations_id_seq', 140, true);


--
-- Data for Name: django_session; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.django_session (session_key, session_data, expire_date) FROM stdin;
4qq5ri52u343863b38qf1cqpc2y4pyzy	NjM4MjA0NmY3NjIxMWE5OGQyYjYyZDFjYTIwY2FjMmJiZDU5NDhjZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjZiMTIxY2U4MDY5YWI3NjJiNmQ5MWIwOWMzMjIwMTZiODc0NDBjNDQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-07-09 22:01:38.38156+00
brtynyi84lwg9eoyzwco6pqt0p3htmmo	NjM4MjA0NmY3NjIxMWE5OGQyYjYyZDFjYTIwY2FjMmJiZDU5NDhjZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjZiMTIxY2U4MDY5YWI3NjJiNmQ5MWIwOWMzMjIwMTZiODc0NDBjNDQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-07-13 12:29:48.973515+00
6glv7nqd6ec1vksulrfy4z7lezpoyt3m	NjM4MjA0NmY3NjIxMWE5OGQyYjYyZDFjYTIwY2FjMmJiZDU5NDhjZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjZiMTIxY2U4MDY5YWI3NjJiNmQ5MWIwOWMzMjIwMTZiODc0NDBjNDQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-07-13 19:57:41.762869+00
1ypcg9agh2kg74kr3ci85stowbc4t277	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-07-15 16:06:59.885673+00
xtthhauyc5pyn8n2zgj9zzoejoy0wsfr	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-07-21 21:27:39.052411+00
e9t8zhrtmixeoyewqi6tk8fw0ynwtfjb	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-08-12 21:34:20.559136+00
fhsd5l17vftuauk4kcex5ygion73ve8j	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-08-29 15:20:19.980853+00
cwyc3tzdple4o262edz9xdjfol0cewz0	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-10-11 16:12:43.10736+00
oobup1iey7qhggxeaiftqqtxq3fd33lg	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-12-21 22:07:00.828749+00
5une7zvmzoawmz3wd28isqzbgyiowxyw	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2015-12-21 22:08:36.202329+00
lkwn0e3nhlbxwiunhr4uca09s60dw00e	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-01-11 20:03:36.811113+00
eo67s01qkstov7eh7o6okrihoe2scoaz	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-02-06 15:54:49.891103+00
qvhm7brcl1afei5u7p4ozhzsgjl7y8mm	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-02-08 12:06:26.04362+00
hnvu06w185gnndl2oztmbles1rgpo0wn	M2QwZjY3NTUxNDc0MzA5NjM4ODc5NTY0NTBmM2M3NmNjZjk2ZDM3ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjdiMTRjMmE0MzdmOGJlOTU5NzYzYzM2NGIyMmZlZjZlYTdkZjIwMDIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-03-03 21:08:58.358537+00
en843i6ba4188o5zqtugzi29ym5vw8u3	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-03-07 10:30:28.057923+00
tg9k3v1e8py0vc7gje4g5mw4pv3mhcgu	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-03-10 14:02:05.325702+00
xxnsu7k3ll0nq7r7punz5d9jwiebdcst	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-03-13 19:31:03.518364+00
qsjv7jl5odbv1cyxuyfhxxroxub9lhgo	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-03-16 10:03:44.286734+00
1hye459opxmu39847voh48ogst87tp0l	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-03-20 10:51:12.765212+00
1bgipml5of7kli5c3yhdatiov6y9lsjb	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-03-29 21:49:50.884468+00
zgspeq546pfxz7ftrvrxt8hh11e1rnh6	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-03-31 09:33:17.899774+00
3cx5s4w3yn6szwziumgypi5kz7okj78f	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-04-20 08:31:08.266812+00
y5r5vig0hakvgobj9r4rbj84pb8jo4o2	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-04-20 08:36:10.936366+00
09q3y3igqda59hrzfs9dvctsdgrylxi6	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-05-24 14:31:25.512488+00
t6jip0xxjo4sv6knsx1xxznfiqu1tupw	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-06-13 11:09:53.64149+00
q72ama6u6aynzbowct2vj7vau8tlsuqb	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-07-01 21:05:54.30055+00
6ivtdtqcxtmajcbiq19xecx53c5gssvl	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-08-05 10:51:49.04687+00
eijhk3gv57i26aubhhqx3p4zyb5ag33d	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-08-29 19:06:16.06068+00
xjocq8xfuzjyj4hsqcjs6xlul3yhifsp	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-09-15 07:07:29.641087+00
svy4e07blrzpo0ldfun8loo3cjhki1ux	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-10-03 08:44:11.992473+00
6knpibboghrly65xg9h6hauckw5efw1z	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-10-27 08:35:18.644913+00
ocnh8ny1vl1b9zhgdbqbi7cb06f4512m	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-11-05 13:38:57.869737+00
vmy6cundmsefq5bfbushu3fvhl91bnpm	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-11-14 14:33:33.498528+00
2ezrecs2q7arlag2uic93fh8yl9sxcgm	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2016-11-28 17:32:10.021299+00
mxknns3ftedoxesrpvytc7rupmfkef83	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2017-06-09 23:17:17.745306+00
3oikidleun5mbxji93umjb6xlpqp3uyq	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2017-08-24 15:40:22.496703+00
npgqh62rsrw9iwm1kc01b4mx0m5l4rlz	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2017-10-17 16:59:10.901196+00
znz5ubfomans8d1wy1ss25u0phz04n6a	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2017-11-15 18:34:58.899236+00
8fzhg7o48si821s4e72xp0whydbu233l	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2017-11-29 19:17:40.407742+00
c6px24p4sta1bo0qpad531eu4pctdqmp	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-01-02 16:25:23.177104+00
yytxzytowrl359p1eb7h5i8q2r4mtwcw	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-01-02 19:58:00.856997+00
zs9ufcbxhrxy52500lcy2l2ymsh3vlrz	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-01-02 21:52:14.951913+00
gb8p3jsergk9u4hsdpf52hcmjkqtera3	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-01-02 22:21:56.80181+00
hxunrk3fie8xajumsq57qm29nmu2p77s	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-01-24 18:43:32.630593+00
iu5u3vwudppxtplvfvlvps9pbz8bvj64	MjFhN2RhYjBmYzI3MmY0OTRmNjRhZTE5N2ZhZGFhMTU0YzhlYWE3Mzp7fQ==	2018-03-16 22:10:47.349406+00
4e11thgar3yu2xy88uozw9q13mbke8ck	MDRhY2E3OTUxOGZlMzY4MDNhNTNkYjFiYWNkZjJjOTU3YjdmMjA1Yjp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0NWZkMmVhMWQyZDQyZGE3NDJkMjhjYTkzMDFiMDg2MTBkZDExMmQiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-03-20 20:58:48.098042+00
247fukej9484lc7wksu80j6pjibjynr2	ZTliNDE1NzUzYTU4NjdjMTVmZDM2NTUyYTY0ZmE3ZTA5OTkyNjg5Mzp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0Nzg3YTM1NTgyZThiMTIzMzY0ZjkwOTE0MDNiZWM1MDQwZTYxMzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-04-11 17:41:07.171643+00
55x93f3ivryhnxzfh674lqweuh5a9p0k	ZTliNDE1NzUzYTU4NjdjMTVmZDM2NTUyYTY0ZmE3ZTA5OTkyNjg5Mzp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0Nzg3YTM1NTgyZThiMTIzMzY0ZjkwOTE0MDNiZWM1MDQwZTYxMzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-07-11 18:23:37.358464+00
52rmrcbalqusbwicyfek07syejiqr84l	ZTliNDE1NzUzYTU4NjdjMTVmZDM2NTUyYTY0ZmE3ZTA5OTkyNjg5Mzp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0Nzg3YTM1NTgyZThiMTIzMzY0ZjkwOTE0MDNiZWM1MDQwZTYxMzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-07-11 18:26:46.215013+00
1fhh6lmqqegmiikaf6eack5bq9t90t8e	ZTliNDE1NzUzYTU4NjdjMTVmZDM2NTUyYTY0ZmE3ZTA5OTkyNjg5Mzp7Il9hdXRoX3VzZXJfaGFzaCI6IjM0Nzg3YTM1NTgyZThiMTIzMzY0ZjkwOTE0MDNiZWM1MDQwZTYxMzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2018-08-03 21:14:51.348702+00
h06cqv18y9lleyspozbii3f2703sxdrs	NDA2MjY5YzU1MTkwOWY3NDE3YzY0ZjU3NTBkNGY2YmFjMjVhY2I4Yzp7Il9hdXRoX3VzZXJfaGFzaCI6IjZkZWQyZmU0MWI5ZTQyY2UxMDJiMWE4NjI2NDc1YzRiMWEyNzdhOTIiLCJfYXV0aF91c2VyX2lkIjoiMSIsIl9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIn0=	2019-04-04 17:34:10.314192+00
aapdcxwss9j5is2zjzsande4nzh335i2	MjkyZTE5ZDc0ZDYzNDQ0YzFmYTFhNjNhZTY5NDExYzE4MGJkM2JlNzp7Il9hdXRoX3VzZXJfaWQiOiIxIiwiX2F1dGhfdXNlcl9oYXNoIjoiNmRlZDJmZTQxYjllNDJjZTEwMmIxYTg2MjY0NzVjNGIxYTI3N2E5MiIsIl9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIn0=	2019-04-06 01:19:12.086541+00
8zzkwk4314azexwmalg66nc8itaptazj	MDMwODVhMWQ0ZDNhNmVlZWI0MDg3YzVlYTkxMTcwZTAzYTBlN2I5Yjp7Il9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIiwiX2F1dGhfdXNlcl9oYXNoIjoiNmRlZDJmZTQxYjllNDJjZTEwMmIxYTg2MjY0NzVjNGIxYTI3N2E5MiIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=	2019-04-06 01:31:23.987509+00
ctyqu8hrku76j1clc01mwfile8b6iq22	ZjE1YzZhYjcyZWY4N2UwZDFjZTg4MTMwNDBmYzU0NWNiYjc5MWUwMTp7Il9hdXRoX3VzZXJfaWQiOiIxIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2ZGVkMmZlNDFiOWU0MmNlMTAyYjFhODYyNjQ3NWM0YjFhMjc3YTkyIn0=	2019-04-06 01:48:14.519679+00
3f13e4aje67whodfm4mem6vtwvy1wwpu	c300d0bcf59946b47fcc32a6b3400976ce4f086e:{"_auth_user_backend":"django.contrib.auth.backends.ModelBackend","_auth_user_id":"1","wagtail-preview-23":["body-36-type=markdown&body-7-id=bf12a1b4-011d-4b16-b593-c044d0bfc94f&body-12-order=12&expire_at=&body-38-order=38&body-17-order=17&body-11-type=real_codeblock&body-37-type=paragraph&body-39-type=markdown&intro=&body-17-type=markdown&body-19-id=eccafba7-ccf9-4256-bcf9-793c6fc28d44&body-7-order=7&body-1-order=1&body-21-id=f946537a-f152-463e-a7ee-9b94503082b7&body-35-type=markdown&body-29-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22329jt%22%2C%0D%0A++++++++++++%22text%22%3A+%22To+test+everything+is+working%2C+try+starting+Solr+in+basic+standalone+mode+using+the+Solr+start+script+provided+in+the+distribution.%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-12-value=And+that%27s+it%21+We+have+our+machines+up+and+running.+You+could+use+this+setup+to+test+any+distributed+network+setup.+You+could+test+security+settings+on+top+of+an+application+stack+using+%5Biptables%5D%28https%3A%2F%2Fhelp.ubuntu.com%2Fcommunity%2FIptablesHowTo+%22Iptables+How+To%22%29.+These+things+are+beyond+the+scope+of+this+tutorial%2C+but+I%27d+encourage+you+to+play+around+with+this.%0D%0A%0D%0ASo%2C+next+we+need+to+install+the+relevant+software+on+each+machine.&body-40-order=40&body-23-type=markdown&body-3-value-code=config.vm.provider+%22virtualbox%22+do+%7Cv%7C%0D%0A++v.memory+%3D+1024%0D%0A++v.cpus+%3D+2%0D%0Aend%0D%0A%0D%0Aconfig.vm.define+%22zoo1%22+do+%7Czoo1%7C%0D%0A++zoo1.vm.box+%3D+%22ubuntu%2Ftrusty64%22%0D%0A++zoo1.vm.network+%22private_network%22%2C+type%3A+%22dhcp%22%0D%0Aend%0D%0A%0D%0Aconfig.vm.define+%22solr1%22+do+%7Csolr1%7C%0D%0A++solr1.vm.box+%3D+%22ubuntu%2Ftrusty64%22%0D%0A++solr1.vm.network+%22private_network%22%2C+type%3A+%22dhcp%22%0D%0Aend%0D%0A%0D%0Aconfig.vm.define+%22solr2%22+do+%7Csolr2%7C%0D%0A++solr2.vm.box+%3D+%22ubuntu%2Ftrusty64%22%0D%0A++solr2.vm.network+%22private_network%22%2C+type%3A+%22dhcp%22%0D%0Aend%0D%0A%0D%0Aconfig.vm.define+%22solr3%22+do+%7Csolr3%7C%0D%0A++solr3.vm.box+%3D+%22ubuntu%2Ftrusty64%22%0D%0A++solr3.vm.network+%22private_network%22%2C+type%3A+%22dhcp%22%0D%0Aend&date=2016-08-27&body-20-value-language=bash&title=Testing+SolrCloud+with+Vagrant&body-38-value-code=bin%2Fsolr+create+-c+testCollection+-d++data_driven_schema_configs+-n+testCollection_cfg+-shards+2+-replicationFactor+2&body-7-value-code=vagrant+ssh+zoo1%0D%0Avagrant+ssh+solr1%0D%0Aetc...&body-10-type=markdown&body-19-order=19&body-2-deleted=&body-9-id=a7c14ea9-9146-437a-9543-d82995ec9b16&body-23-deleted=&body-24-id=c9584d5b-68bf-4f31-a678-8eccf2160862&body-0-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%0D%0A++++++++%220%22%3A+%7B%0D%0A++++++++++++%22data%22%3A+%7B%0D%0A++++++++++++++++%22url%22%3A+%22https%3A%2F%2Fwww.vagrantup.com%2Fdocs%2Fwhy-vagrant%2F%22%0D%0A++++++++++++%7D%2C%0D%0A++++++++++++%22mutability%22%3A+%22MUTABLE%22%2C%0D%0A++++++++++++%22type%22%3A+%22LINK%22%0D%0A++++++++%7D%2C%0D%0A++++++++%221%22%3A+%7B%0D%0A++++++++++++%22data%22%3A+%7B%0D%0A++++++++++++++++%22url%22%3A+%22https%3A%2F%2Fwww.vagrantup.com%2F%22%0D%0A++++++++++++%7D%2C%0D%0A++++++++++++%22mutability%22%3A+%22MUTABLE%22%2C%0D%0A++++++++++++%22type%22%3A+%22LINK%22%0D%0A++++++++%7D%2C%0D%0A++++++++%222%22%3A+%7B%0D%0A++++++++++++%22data%22%3A+%7B%0D%0A++++++++++++++++%22url%22%3A+%22https%3A%2F%2Fwww.virtualbox.org%22%0D%0A++++++++++++%7D%2C%0D%0A++++++++++++%22mutability%22%3A+%22MUTABLE%22%2C%0D%0A++++++++++++%22type%22%3A+%22LINK%22%0D%0A++++++++%7D%2C%0D%0A++++++++%223%22%3A+%7B%0D%0A++++++++++++%22data%22%3A+%7B%0D%0A++++++++++++++++%22url%22%3A+%22https%3A%2F%2Fzookeeper.apache.org%2F%22%0D%0A++++++++++++%7D%2C%0D%0A++++++++++++%22mutability%22%3A+%22MUTABLE%22%2C%0D%0A++++++++++++%22type%22%3A+%22LINK%22%0D%0A++++++++%7D%2C%0D%0A++++++++%224%22%3A+%7B%0D%0A++++++++++++%22data%22%3A+%7B%0D%0A++++++++++++++++%22url%22%3A+%22https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2Fsolr%2FHow%2BSolrCloud%2BWorks%22%0D%0A++++++++++++%7D%2C%0D%0A++++++++++++%22mutability%22%3A+%22MUTABLE%22%2C%0D%0A++++++++++++%22type%22%3A+%22LINK%22%0D%0A++++++++%7D%2C%0D%0A++++++++%225%22%3A+%7B%0D%0A++++++++++++%22data%22%3A+%7B%0D%0A++++++++++++++++%22url%22%3A+%22https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2Fsolr%2FSetting%2BUp%2Ban%2BExternal%2BZooKeeper%2BEnsemble%22%0D%0A++++++++++++%7D%2C%0D%0A++++++++++++%22mutability%22%3A+%22MUTABLE%22%2C%0D%0A++++++++++++%22type%22%3A+%22LINK%22%0D%0A++++++++%7D%0D%0A++++%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%0D%0A++++++++++++++++%7B%0D%0A++++++++++++++++++++%22offset%22%3A+127%2C%0D%0A++++++++++++++++++++%22key%22%3A+0%2C%0D%0A++++++++++++++++++++%22length%22%3A+7%0D%0A++++++++++++++++%7D%0D%0A++++++++++++%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22pofff%22%2C%0D%0A++++++++++++%22text%22%3A+%22I%27d+been+thinking+about+using+SolrCloud+for+a+project+at+work+recently%2C+and+I+wanted+to+test+it+out+locally.+To+do+this+I+used+Vagrant+to+setup+a+multi-machine+private+network+with+static+IPs.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22xft4f%22%2C%0D%0A++++++++++++%22text%22%3A+%22Getting+the+machines+setup+was+much+easier+than+I+expected%2C+and+then+getting+SolrCloud+working+wasn%27t+much+more+complicated.+However%2C+there+does+appear+to+be+a+lack+of+good+examples+of+exactly+how+to+get+something+like+this+going%2C+so+in+this+blog+post+I%27m+going+to+take+you+through+the+steps+to+do+just+that.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22header-two%22%2C%0D%0A++++++++++++%22key%22%3A+%225v6w2%22%2C%0D%0A++++++++++++%22text%22%3A+%22Requirements%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22krstz%22%2C%0D%0A++++++++++++%22text%22%3A+%22I+used+OSX+to+create+this+setup%2C+but+it+should+also+work+on+major+Linux+distributions.+I%27m+not+a+Windows+user+so+you%27ll+have+to+do+your+own+research+if+that%27s+what+you+need.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%0D%0A++++++++++++++++%7B%0D%0A++++++++++++++++++++%22offset%22%3A+14%2C%0D%0A++++++++++++++++++++%22key%22%3A+1%2C%0D%0A++++++++++++++++++++%22length%22%3A+7%0D%0A++++++++++++++++%7D%2C%0D%0A++++++++++++++++%7B%0D%0A++++++++++++++++++++%22offset%22%3A+26%2C%0D%0A++++++++++++++++++++%22key%22%3A+2%2C%0D%0A++++++++++++++++++++%22length%22%3A+10%0D%0A++++++++++++++++%7D%0D%0A++++++++++++%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22hqmwl%22%2C%0D%0A++++++++++++%22text%22%3A+%22You+will+need+Vagrant+and+VirtualBox+installed+on+your+host+machine.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22header-two%22%2C%0D%0A++++++++++++%22key%22%3A+%22u4cvr%22%2C%0D%0A++++++++++++%22text%22%3A+%22Designing+the+setup%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22rwq7l%22%2C%0D%0A++++++++++++%22text%22%3A+%22A+SolrCloud+setup+has+two+types+of+component%3A%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22zpx6b%22%2C%0D%0A++++++++++++%22text%22%3A+%22%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%0D%0A++++++++++++++++%7B%0D%0A++++++++++++++++++++%22offset%22%3A+12%2C%0D%0A++++++++++++++++++++%22key%22%3A+3%2C%0D%0A++++++++++++++++++++%22length%22%3A+16%0D%0A++++++++++++++++%7D%0D%0A++++++++++++%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22ordered-list-item%22%2C%0D%0A++++++++++++%22key%22%3A+%22n0495%22%2C%0D%0A++++++++++++%22text%22%3A+%22One+or+more+Apache+Zookeeper+instances+to+manage+the+distribution+of+data+across+the+Solr+cloud%2C+and+the+configuration+and+administration+of+the+Solr+nodes.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22ordered-list-item%22%2C%0D%0A++++++++++++%22key%22%3A+%22g21pf%22%2C%0D%0A++++++++++++%22text%22%3A+%22One+or+more+Solr+nodes+on+which+to+store+your+data+and+perform+your+queries.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%228qh11%22%2C%0D%0A++++++++++++%22text%22%3A+%22The+%27Cloud%27+part+of+SolrCloud+comes+from+the+fact+that+any+data+you+push+into+your+set+of+Solr+nodes+can+be+split+into+shards+and+distributed+across+the+nodes.+Each+shard+is+then+replicated+one+or+more+times+on+different+nodes+to+provide+redundancy.+Queries+arrive+at+one+of+the+nodes+and+the+query+is+then+forwarded+to+the+node+where+a+replica+of+the+appropriate+shard+is+located.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22zz858%22%2C%0D%0A++++++++++++%22text%22%3A+%22The+amount+of+shards+into+which+the+data+is+split%2C+and+the+amount+of+replicas+for+each+shard+is+set+at+the+point+of+creating+a+collection+%28more+about+this+later%29.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%221wlug%22%2C%0D%0A++++++++++++%22text%22%3A+%22%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%0D%0A++++++++++++++++%7B%0D%0A++++++++++++++++++++%22offset%22%3A+51%2C%0D%0A++++++++++++++++++++%22key%22%3A+4%2C%0D%0A++++++++++++++++++++%22length%22%3A+22%0D%0A++++++++++++++++%7D%0D%0A++++++++++++%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%2237rqj%22%2C%0D%0A++++++++++++%22text%22%3A+%22You+can+find+a+basic+introduction+to+how+SolrCloud+works+on+the+Solr+wiki.+For+the+rest+of+this+article+I%27m+going+to+assume+you+are+aware+of+the+basics.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%0D%0A++++++++++++++++%7B%0D%0A++++++++++++++++++++%22offset%22%3A+336%2C%0D%0A++++++++++++++++++++%22key%22%3A+5%2C%0D%0A++++++++++++++++++++%22length%22%3A+18%0D%0A++++++++++++++++%7D%0D%0A++++++++++++%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22u4h0q%22%2C%0D%0A++++++++++++%22text%22%3A+%22Our+aim+for+this+test+is+to+have+each+element+of+the+SolrCloud+setup+running+on+its+own+virtual+machine.+Our+setup+will+have+three+Solr+nodes+with+which+we+can+store+and+query+the+data%2C+and+a+single+Zookeeper+instance+to+manage+the+nodes.+We+could+have+multiple+Zookeeper+instances+to+provide+further+redundancy.+This+would+be+called+a+Zookeeper+ensemble.+However%2C+for+this+initial+test+we%27re+going+to+stick+with+just+the+one.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22j2hl1%22%2C%0D%0A++++++++++++%22text%22%3A+%22Having+three+Solr+nodes+means+that+we+can+split+our+data+into+two+shards+with+two+replicas+of+each%2C+and+if+one+of+the+Solr+nodes+goes+down+we%27ll+still+be+able+to+access+all+of+the+data.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22header-two%22%2C%0D%0A++++++++++++%22key%22%3A+%22smxe4%22%2C%0D%0A++++++++++++%22text%22%3A+%22Setting+up+the+machines%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%224oqod%22%2C%0D%0A++++++++++++%22text%22%3A+%22Our+test+network+will+be+built+using+Ubuntu+virtual+machines.+The+first+thing+we%27re+going+to+do+is+create+a+new+directory+for+our+test+VMs+on+our+host+machine%2C+and+then+generate+a+Vagrant+file+including+the+Ubuntu+Trusty64+Vagrant+box.%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-32-value-code=bin%2Fsolr+stop&body-12-type=markdown&body-17-id=4d07ca3d-4882-4a29-9569-b63e249a2f3c&body-36-value=After+running+this+command%2C+you+should+be+able+to+go+to+your+Solr+admin+for+that+node+%28e.g.+http%3A%2F%2F172.28.128.4%3A8983%2Fsolr%2F%29%2C+and+you+should+see+the+%27Cloud%27+option+in+the+left-hand+menu.+If+you+click+this%2C+currently+you+should+only+see+a+blank+white+area%2C+with+a+key+in+the+bottom+right.+For+anything+to+display+in+this+section+we+need+to+upload+a+%27Collection%27.&body-38-type=real_codeblock&body-13-order=13&body-13-id=29e8e843-d444-4821-8df4-5c043f5556a4&body-31-order=31&subtitle=Easy+steps+to+emulate+a+multi-machine+setup+locally&body-27-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22header-two%22%2C%0D%0A++++++++++++%22key%22%3A+%223pabj%22%2C%0D%0A++++++++++++%22text%22%3A+%22Installing+Solr+and+starting+in+cloud+mode%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%222mex3%22%2C%0D%0A++++++++++++%22text%22%3A+%22We+now+need+to+install+our+three+instances+of+Solr.+Like+Zookeeper%2C+we+need+to+download+a+distribution+from+the+Apache+Solr+website%2C+and+unpack+it.%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-20-order=20&body-0-type=paragraph&body-1-deleted=&body-38-id=66f233de-d788-43e9-a77e-aa2ecaea7f8a&body-7-deleted=&body-count=41&body-28-id=6436cc2c-2e1d-4fc5-aeb7-60fa82be5fb1&body-17-value=%5BAs+the+website+states%5D%28https%3A%2F%2Fzookeeper.apache.org%2F+%22Zookeeper+home%22%29%2C+%22ZooKeeper+is+a+centralized+service+for+maintaining+configuration+information%2C+naming%2C+providing+distributed+synchronization%2C+and+providing+group+services%22.+For+the+purposes+of+SolrCloud%2C+Zookeeper+does+the+following%3A%0D%0A%0D%0A%2A+Stores+and+distributes+configuration+files+for+SolrCloud+collections+to+each+node.%0D%0A%2A+Manages+the+election+of+%5B%27leaders%27%5D%28https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2Fsolr%2FShards%2Band%2BIndexing%2BData%2Bin%2BSolrCloud%29.%0D%0A%2A+Ensures+the+synchronisation+of+data+between+replicas+of+collection+shards.%0D%0A%0D%0AInstalling+and+configuring+Zookeeper+for+our+SolrCloud+test+is+pretty+easy.+First%2C+pull+down+the+latest+version+with+the+%60curl%60+command+and+unpack+it%3A&body-8-order=8&body-11-id=a5763017-deef-4016-a54a-2c5fd6aa7ca7&body-20-type=real_codeblock&body-14-id=8cddfd50-aba7-4de7-83fc-1bd7f3b85d76&body-31-deleted=&body-14-type=real_codeblock&body-16-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22header-two%22%2C%0D%0A++++++++++++%22key%22%3A+%22ry37e%22%2C%0D%0A++++++++++++%22text%22%3A+%22Installing+Zookeeper%5Cn%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-34-deleted=&body-32-id=3523c902-135a-4429-8ef1-e59f5a7cf8e9&body-15-deleted=&body-16-deleted=&body-37-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22header-two%22%2C%0D%0A++++++++++++%22key%22%3A+%22fh3e7%22%2C%0D%0A++++++++++++%22text%22%3A+%22Creating+a+test+collection%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22a31eo%22%2C%0D%0A++++++++++++%22text%22%3A+%22A+%27Collection%27+in+SolrCloud+is+the+equivalent+of+a+Solr+core+in+standalone+mode.+We+can+easily+create+a+simple+collection+with+the+following+command%2C+run+from+the+root+folder+of+one+of+your+Solr+nodes%3A%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-13-type=paragraph&body-4-deleted=&body-31-value=Then+visit+your+VM%27s+IP+in+your+host+machines+browser%2C+appending+%60%3A8983%2Fsolr%60+to+the+end.+So+for+example%2C+%60http%3A%2F%2F172.28.128.4%3A8983%2Fsolr%60.+If+all+is+successful+you+should+see+the+Solr+admin.%0D%0A%0D%0AHowever%2C+we+don%27t+want+these+Solr+instances+to+run+in+standalone+mode%2C+we+want+them+to+run+in+cloud+mode.+This+is+just+as+easy%2C+you+just+need+to+know+the+IP+for+your+Zookeeper+machine%2C+and+the+IP+of+each+connecting+Solr+VM.%0D%0A%0D%0AThe+first+thing+to+do+is+stop+the+node+we+currently+have+running.&body-30-value-code=cd+%7E%2Fsolr-6.2.0%0D%0Abin%2Fsolr+start&body-27-id=4c97f7fc-eed4-4852-856f-fe2a66b72747&body-29-order=29&body-14-value-language=bash&body-8-type=markdown&body-36-id=511a0930-15e8-4e0a-82a4-32cd0264743a&body-15-order=15&body-20-value-code=nano+%7E%2Fzookeeper-3.4.8%2Fconf%2Fzoo.cfg&body-0-deleted=&body-14-deleted=&body-30-deleted=&body-21-deleted=&body-6-id=49257c11-ebf1-4e9c-abdc-580053599135&body-19-deleted=&body-27-type=paragraph&search_description=Easy+steps+to+emulate+a+multi-machine+setup+locally&body-36-order=36&body-25-id=3271c0b7-d9fa-4a8e-ae69-4d82004970c6&body-5-value-language=bash&body-27-order=27&body-14-order=14&body-5-id=9ff9e702-22b3-4b2f-a3dc-f695acad6f3b&body-39-deleted=&body-14-value-code=sudo+add-apt-repository+ppa%3Awebupd8team%2Fjava%0D%0Asudo+apt-get+update%0D%0Asudo+apt-get+install+oracle-java8-installer&body-31-id=b3006f8d-ead9-49bb-88f2-5d8d6dcd065f&body-5-order=5&body-4-order=4&body-31-type=markdown&body-2-type=markdown&body-30-value-language=bash&body-21-value=Now+copy+the+following+three+lines+into+that+file+and+save+it.&body-33-type=markdown&body-18-type=real_codeblock&body-27-deleted=&body-24-type=real_codeblock&body-0-order=0&body-38-value-language=bash&body-22-value-language=bash&body-2-order=2&body-22-value-code=tickTime%3D2000%0D%0AdataDir%3D%2Fvar%2Flib%2Fzookeeper%0D%0AclientPort%3D2181&body-2-id=98d24dbb-a9f4-420a-938d-54ce844c019b&body-24-value-code=sudo+%7E%2Fzookeeper-3.4.8%2Fbin%2FzkServer.sh+start&body-34-value-language=bash&body-8-deleted=&body-23-id=5b3b1d8c-c536-4257-925b-6e98dd9fcd08&body-29-type=paragraph&body-26-order=26&body-36-deleted=&body-28-type=real_codeblock&body-40-deleted=&body-18-value-code=curl+-O+http%3A%2F%2Fmirrors.ukfast.co.uk%2Fsites%2Fftp.apache.org%2Fzookeeper%2Fzookeeper-3.4.8%2Fzookeeper-3.4.8.tar.gz%0D%0Atar+-zxf+zookeeper-3.4.8.tar.gz&body-13-deleted=&body-10-order=10&listing_image=&body-3-value-language=bash&body-12-id=16f82195-41cf-4938-9439-fba31d853438&body-5-value-code=cd+%7E%2Fsolrcloud-test%0D%0Avagrant+up&body-35-deleted=&body-5-type=real_codeblock&body-30-order=30&body-1-id=56c7cf60-a22a-4f40-8bcc-a80ed71895fb&body-6-order=6&body-15-id=1a1f1530-7ce6-477d-9abe-b76aae8edaae&body-7-value-language=bash&body-11-value-code=ssh+vagrant%40172.28.128.4&body-3-deleted=&body-6-type=markdown&body-40-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22header-two%22%2C%0D%0A++++++++++++%22key%22%3A+%22srec4%22%2C%0D%0A++++++++++++%22text%22%3A+%22Conclusion%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%227fttx%22%2C%0D%0A++++++++++++%22text%22%3A+%22So+now+if+you+go+to+the+%27Cloud%27+section+of+your+Solr+admin+on+any+of+your+connected+nodes%2C+you+should+now+see+a+graph+with+your+collection+name+on+the+left%2C+the+split+of+your+shards+in+the+middle%2C+and+the+locations+of+the+replicas+of+these+shards+on+the+right.%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22u4xsv%22%2C%0D%0A++++++++++++%22text%22%3A+%22There+you+have+it%2C+a+working+SolrCloud+setup+using+Vagrant.+We%27ve+got+no+data+in+our+test+collection%2C+but+adding+in+data+isn%27t+SolrCloud+specific.+You+can+use+any+method+for+pushing+in+data+that+you+would+use+when+using+Solr+in+standalone+mode.%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-28-value-language=bash&tags=Solr%2CSolrCloud%2CVagrant%2Cdevops&slug=testing-solrcloud-vagrant&body-34-id=ab95c913-da3b-432e-b84f-b1a71f5a62d1&body-38-deleted=&body-34-value-code=bin%2Fsolr+start+-c+-z+172.28.128.3%3A2181+-h+172.28.128.4%3A8983&body-25-type=paragraph&body-22-order=22&body-32-deleted=&body-24-deleted=&body-1-value-language=bash&body-2-value=This+will+generate+a+file+called+%60Vagrantfile%60+which+includes+the+instructions+for+Vagrant+to+build+a+basic+Ubuntu+VM.+I%27m+going+to+use+this+file+to+create+all+four+of+the+necessary+VMs+for+our+test.+Vagrant+includes+the+ability+to+%5Bcreate+multi-machine+setups+out+of+the+box%5D%28https%3A%2F%2Fwww.vagrantup.com%2Fdocs%2Fmulti-machine%2F%29.+So%2C+we+need++to+open+the+Vagrantfile+and+replace+the+line+%60config.vm.box+%3D+%22ubuntu%2Ftrusty64%22%60+with+the+instructions+below%3A&body-32-type=real_codeblock&body-39-value=I%27m+not+going+to+go+into+great+detail+on+how+to+create+Collections+in+this+blog+post%2C+but+here%27s+a+quick+breakdown+of+the+command+we%27ve+just+run%3A%0D%0A%0D%0A-+%60bin%2Fsolr+create+-c+testCollection%60+%3A+The+create+command+followed+by+the+%60-c%60+modifier+which+defines+the+name+of+the+new+collection.%0D%0A%0D%0A-+%60-d+data_driven_schema_configs%60+%3A+The+%60-d%60+modifier+is+required+to+set+the+config+directory+for+the+Collection.+This+config+is+uploaded+to+Zookeeper%2C+which+then+shares+it+with+the+other+Solr+nodes.+In+this+example+I%27ve+used+%60data_driven_schema_configs%60%2C+which+is+one+of+the+example+config+sets.+The+default+directory+in+which+the+Solr+create+command+will+look+for+the+config+is+%60%2Fsolr-6.2.0%2Fserver%2Fsolr%2Fconfigsets%2F%60.+If+you+want+to+create+your+own+config%2C+you+can+copy+one+of+the+example+config+sets+into+a+new+folder%2C+then+provide+a+relative+path+to+that+folder+instead.+For+example%2C+if+running+from+the+root+directory+of+your+Solr+install+%60server%2Fsolr%2FtestCollectionConf%2Fconf%60.%0D%0A%0D%0A-+%60-shards+3%60+%3A+This+defines+how+many+shards+the+Collection+should+be+split+into.%0D%0A%0D%0A-+%60-replicationFactor+3%60+%3A+This+defines+how+many+replicas+of+each+Shard+are+created.%0D%0A%0D%0AFor+more+info+on+the+usage+of+the+%27create%27+command%2C+%5Bsee+the+Solr+docs%5D%28%22https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2Fsolr%2FSolr%2BStart%2BScript%2BReference%23SolrStartScriptReference-CollectionsandCores%22%29.&body-10-id=0ed9675b-32ae-4e0f-b03b-6c20603c915e&body-13-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22header-two%22%2C%0D%0A++++++++++++%22key%22%3A+%228ef9h%22%2C%0D%0A++++++++++++%22text%22%3A+%22Installing+Java+8%22%0D%0A++++++++%7D%2C%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%225gchq%22%2C%0D%0A++++++++++++%22text%22%3A+%22Both+Solr+and+Zookeeper+rely+on+Java+8+in+one+way+or+another.+So+the+first+thing+we%27re+going+to+do+is+install+this+on+each+of+the+boxes.+Run+the+following+commands+in+each+of+the+tabs+you+have+open.%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-18-order=18&body-40-type=paragraph&body-34-order=34&next=&body-16-type=paragraph&body-28-deleted=&body-0-id=1ebd142a-84f7-4091-b466-a2a5f35d7e5b&body-28-value-code=curl+-O+http%3A%2F%2Fmirrors.muzzy.org.uk%2Fapache%2Flucene%2Fsolr%2F6.2.0%2Fsolr-6.2.0.tgz%0D%0Atar+-xzf+solr-6.2.0.tgz&body-3-id=a02ee92d-450b-4aa7-93b6-8d25a6eba49e&body-10-value=The+address+you%27re+looking+for+is+the+%60inet+addr%3A+172.28.128.3%60+bit+in+the+%60eth1%60+block.+If+we+run+this+command+on+each+box%2C+we+should+find+the+IP+address+is+the+same+for+each+apart+from+the+final+number.+This+is+because+the+names+are+taken+from+the+%5Breserved+IP+addresses+space%5D%28https%3A%2F%2Ftools.ietf.org%2Fhtml%2Frfc1918%23section-3+%22RFC+1918+-++Address+Allocation+for+Private+Internets%22%29.+For+example%2C+the+addresses+generated+for+my+example+are%3A%0D%0A%0D%0A%2A+172.28.128.3%0D%0A%2A+172.28.128.4%0D%0A%2A+172.28.128.5%0D%0A%2A+172.28.128.6%0D%0A%0D%0AYou+can+test+your+private+network+by+ssh%27ing+from+one+Vagrant+box+into+another%2C+with+the+username+%60vagrant%60+and+the+password+%60vagrant%60%3A&body-8-value=Now+that+we+are+working+within+the+virtual+machines%2C+the+first+thing+we+need+to+do+is+make+a+note+of+the+IP+addresses+on+each.+There+are+a+few+ways+to+do+this%2C+but+I+use+the+%60ifconfig+-a%60+command.+You+should+see+something+similar+to+the+output+below%3A&body-22-deleted=&body-35-value=Let%27s+break+down+the+elements+of+this+command%3A%0D%0A%0D%0A-+%60bin%2Fsolr+start+-c%60%3A+This+is+the+familiar+start+command%2C+with+the+%27-c%27+modifier+which+is+a+shortened+version+of+%60-cloud%60.%0D%0A%0D%0A-+%60-z+172.28.128.3%3A2181%60%3A+the+%60-z%60+modifier+instructs+Solr+to+connect+to+a+Zookeeper+instance+with+the+following+IP+and+port+number.%0D%0A%0D%0A-+%60-h+172.28.128.4%3A8983%60%3A+this+defines+the+hostname+and+port+to+start+Solr+with.+This+should+be+set+to+the+specific+Solr+machine%27s+IP.+The+port+can+be+anything+that+doesn%27t+clash+with+something+else%2C+but+I%27d+suggest+sticking+with+the+default+Solr+port+of+8983.&body-39-id=6bc42cef-db3f-40b0-97b0-2c00e944a631&csrfmiddlewaretoken=4pjRPlhzgHbnZ1Hs4y07vWyPJe39c1S57zRtK77nVP8sJSs727yaJYRamZaEKAHV&body-40-id=999c03d5-8c5d-4ffa-89b0-246414e750d2&body-3-order=3&body-24-order=24&body-25-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22zcdn6%22%2C%0D%0A++++++++++++%22text%22%3A+%22If+all+has+gone+well%2C+you+should+see+the+following+output+in+your+terminal%3A%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-9-value-code=eth0++++++Link+encap%3AEthernet++HWaddr+08%3A00%3A27%3A55%3A57%3A5e++%0D%0A++++++++++inet+addr%3A10.0.2.15++Bcast%3A10.0.2.255++Mask%3A255.255.255.0%0D%0A++++++++++inet6+addr%3A+fe80%3A%3Aa00%3A27ff%3Afe55%3A575e%2F64+Scope%3ALink%0D%0A++++++++++UP+BROADCAST+RUNNING+MULTICAST++MTU%3A1500++Metric%3A1%0D%0A++++++++++RX+packets%3A754+errors%3A0+dropped%3A0+overruns%3A0+frame%3A0%0D%0A++++++++++TX+packets%3A584+errors%3A0+dropped%3A0+overruns%3A0+carrier%3A0%0D%0A++++++++++collisions%3A0+txqueuelen%3A1000+%0D%0A++++++++++RX+bytes%3A79919+%2879.9+KB%29++TX+bytes%3A69421+%2869.4+KB%29%0D%0A%0D%0Aeth1++++++Link+encap%3AEthernet++HWaddr+08%3A00%3A27%3Ac4%3A24%3Aec++%0D%0A++++++++++inet+addr%3A172.28.128.3++Bcast%3A172.28.128.255++Mask%3A255.255.255.0%0D%0A++++++++++inet6+addr%3A+fe80%3A%3Aa00%3A27ff%3Afec4%3A24ec%2F64+Scope%3ALink%0D%0A++++++++++UP+BROADCAST+RUNNING+MULTICAST++MTU%3A1500++Metric%3A1%0D%0A++++++++++RX+packets%3A175+errors%3A0+dropped%3A0+overruns%3A0+frame%3A0%0D%0A++++++++++TX+packets%3A16+errors%3A0+dropped%3A0+overruns%3A0+carrier%3A0%0D%0A++++++++++collisions%3A0+txqueuelen%3A1000+%0D%0A++++++++++RX+bytes%3A37752+%2837.7+KB%29++TX+bytes%3A2538+%282.5+KB%29%0D%0A%0D%0Alo++++++++Link+encap%3ALocal+Loopback++%0D%0A++++++++++inet+addr%3A127.0.0.1++Mask%3A255.0.0.0%0D%0A++++++++++inet6+addr%3A+%3A%3A1%2F128+Scope%3AHost%0D%0A++++++++++UP+LOOPBACK+RUNNING++MTU%3A65536++Metric%3A1%0D%0A++++++++++RX+packets%3A0+errors%3A0+dropped%3A0+overruns%3A0+frame%3A0%0D%0A++++++++++TX+packets%3A0+errors%3A0+dropped%3A0+overruns%3A0+carrier%3A0%0D%0A++++++++++collisions%3A0+txqueuelen%3A0+%0D%0A++++++++++RX+bytes%3A0+%280.0+B%29++TX+bytes%3A0+%280.0+B%29&body-19-type=markdown&body-26-type=real_codeblock&body-37-order=37&body-32-value-language=bash&body-29-id=31cd0aa9-b7a9-4b4b-a3da-7db5d15e7dfd&body-33-deleted=&body-9-type=real_codeblock&body-21-order=21&body-20-deleted=&listing_intro=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%220jypo%22%2C%0D%0A++++++++++++%22text%22%3A+%22Easy+steps+to+emulate+a+multi-machine+setup+locally%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-22-id=e95324f9-720a-4d4c-91b1-58d44ca63945&body-20-id=11d0dd08-e15e-443f-9163-91b194fff0f7&body-19-value=We+then+need+to+update+the+Zookeeper+configuration+with+some+basics.+Zookeeper+comes+with+a+sample+config+file+%28%60conf%2Fzoo_sample.cfg%60%29%2C+but+we+don%27t+need+all+the+comments+and+examples+that+that+file+provides%2C+so+we%27ll+just+create+a+new+one+using+your+editor+of+choice.+I%27m+going+to+use+nano.&body-33-order=33&body-8-id=ebb36154-589f-4d1d-bb3f-5d3c53c903ff&body-23-value=%2A+%60tickTime%60+is+the+amount+of+time+in+milliseconds+that+Zookeeper+will+wait+before+determining+that+one+of+your+Solr+servers+is+down.%0D%0A%2A+%60dataDir%60+is+where+Zookeeper+will+store+the+data+about+your+SolrCloud+cluster.+If+this+directory+doesn%27t+exist+then+Zookeeper+will+creat+it+when+it+first+starts+up.%0D%0A%2A+%60clientPort%60+is+the+port+on+which+your+SolrCloud+nodes+will+connect+to+Zookeeper.%0D%0A%0D%0AFinally%2C+you+need+to+start+Zookeeper+with+the+start-up+script+provided+with+the+installation%3A&body-18-deleted=&body-22-type=real_codeblock&body-9-value-language=bash&body-9-deleted=&main_image=&body-26-value-language=bash&body-10-deleted=&body-30-type=real_codeblock&body-21-type=markdown&body-6-value=The+process+of+building+the+four+Vagrant+boxes+will+begin.+This+could+take+a+few+minutes%2C+particularly+if+you+haven%27t+used+the+Ubuntu+Trusty64+box+before%2C+as+Vagrant+will+download+it.%0D%0A%0D%0A%2A%2ANote%3A%2A%2A+Some+older+versions+of+Vagrant+have+an+issue+when+using+the+DCHP+network+type.+They+fail+on+%60vagrant+up%60+with+an+error+saying+a+network+of+that+type+already+exists.+Upgrade+to+the+latest+version+of+Vagrant+and+that+error+will+disappear.%0D%0A%0D%0ANow+that+the+Vagrant+boxes+are+built+and+running%2C+we+can+SSH+into+them.+Open+three+additional+tabs+or+windows+for+your+terminal%2C+go+to+the+solrcloud-test+directory+in+each+and+use+the+command+%60vagrant+ssh+%5Bbox+name%5D%60%2C+e.g.%3A&body-16-id=907250f0-0090-49e8-b9b6-ab569008eb1b&body-37-deleted=&body-9-order=9&body-4-value=The+first+block+in+the+configuration+above+defines+the+amount+of+memory+and+CPUs+that+should+be+assigned+for+each+of+the+VMs+below.+The+default+is+512mb%2C+but+this+isn%27t+enough+to+run+Solr%2C+so+we+need+to+bump+up+to+1024mb.%0D%0A%0D%0AEach+of+the+next+four+blocks+defines+a+separate+virtual+machine+and+gives+each+box+a+name.+The+%60%5Bname%5D.vm.box%60+command+is+telling+Vagrant+which+template+to+use+for+creating+each+box%2C+and+the+%60%5Bname%5D.vm.network%60+command+instructs+Vagrant+to+create+a+private+network+using+the+%5BDHCP%5D%28https%3A%2F%2Fkb.iu.edu%2Fd%2Fadov+%22What+is+DCHP%3F%22%29+protocol.+This+means+that+each+of+our+boxes+will+be+assigned+an+IP+address+that+can+only+be+accessed+within+our+private+network+%28the+four+vagrant+boxes+and+our+host+machine%29.%0D%0A%0D%0ANow+let%27s+get+these+Vagrant+boxes+running.&body-12-deleted=&body-15-type=paragraph&body-18-id=81ae7536-2df1-4c67-be41-592ac029af5b&body-34-type=real_codeblock&body-11-value-language=bash&body-7-type=real_codeblock&body-29-deleted=&body-3-type=real_codeblock&body-24-value-language=bash&body-11-order=11&body-35-id=fa1e9d45-627d-4290-b681-98acf7b8d380&body-33-id=57e87a15-9b69-458e-81ff-900cfbfc967f&body-4-id=5ed02442-b4c9-4f90-b906-b7bd8971a9be&body-1-type=real_codeblock&go_live_at=&body-25-order=25&body-23-order=23&body-32-order=32&body-5-deleted=&body-16-order=16&body-30-id=f7f6949a-c5f9-4aab-9a57-ab60c4e94ba4&body-26-id=4effec9c-fb7e-419a-985c-0646c4e5eafd&body-37-id=a693e420-4a1e-4d4d-8535-e986c1df0031&body-6-deleted=&body-18-value-language=bash&body-1-value-code=mkdir+-p+%7E%2Fsolrcloud-test%0D%0Acd+%7E%2Fsolrcloud-test%0D%0Avagrant+init+ubuntu%2Ftrusty64&body-39-order=39&body-26-value-code=ZooKeeper+JMX+enabled+by+default%0D%0AUsing+config%3A+%2Fhome%2Fvagrant%2Fzookeeper-3.4.8%2Fbin%2F..%2Fconf%2Fzoo.cfg%0D%0AStarting+zookeeper+...+STARTED&body-35-order=35&body-33-value=Then+we+restart+in+cloud+mode+with+the+following+command%3A&body-28-order=28&body-26-deleted=&seo_title=Testing+SolrCloud+with+Vagrant&body-4-type=markdown&body-17-deleted=&body-25-deleted=&body-15-value=%7B%0D%0A++++%22entityMap%22%3A+%7B%0D%0A++++++++%220%22%3A+%7B%0D%0A++++++++++++%22data%22%3A+%7B%0D%0A++++++++++++++++%22url%22%3A+%22http%3A%2F%2Fubuntuhandbook.org%2Findex.php%2F2015%2F01%2Finstall-openjdk-8-ubuntu-14-04-12-04-lts%2F%22%0D%0A++++++++++++%7D%2C%0D%0A++++++++++++%22mutability%22%3A+%22MUTABLE%22%2C%0D%0A++++++++++++%22type%22%3A+%22LINK%22%0D%0A++++++++%7D%0D%0A++++%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%0D%0A++++++++++++++++%7B%0D%0A++++++++++++++++++++%22offset%22%3A+115%2C%0D%0A++++++++++++++++++++%22key%22%3A+0%2C%0D%0A++++++++++++++++++++%22length%22%3A+36%0D%0A++++++++++++++++%7D%0D%0A++++++++++++%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22tfbbz%22%2C%0D%0A++++++++++++%22text%22%3A+%22This+installs+both+the+JRE+and+JDK+versions+of+Oracle%27s+official+Java+package.+If+you+would+prefer+to+use+OpenJDK%2C+you+can+follow+the+instructions+here.%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&body-11-deleted=",1556629906.2291355],"wagtail-preview-blog,blogpage,3":["main_image=&search_description=&body-0-value=%7B%22blocks%22%3A%5B%7B%22key%22%3A%22ekqnr%22%2C%22text%22%3A%22As+a+project+manager%2C+getting+things+done+quicker+is+often+the+thing+that%27s+most+on+my+mind.+This+is+great+and+as+it+should+be%21++%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22nngn%22%2C%22text%22%3A%22However%2C+as+someone+who+likes+to+get+his+hands+dirty%2C+I%27m+often+tempted+to+get+into+the+nitty+gritty+of+the+work%2C+to+try+to+move+things+along+faster.+This+almost+always+actually+leads+to+projects+going+slower%21++%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%2289s7g%22%2C%22text%22%3A%22Whilst+having+the+technical+expertise+that+I+have+is+a+great+advantage+as+a+PM%2C+it+does+not+make+me+a+professional+developer.+By+switching+my+focus+away+from+facilitating+the+work+of+others%2C+I%27m+actually+reducing+their+potential+productivity.+And+one+developer+at+full+productivity+is+going+to+produce+a+bunch+more+quality+work+than+me+trying+my+best%21++%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%228ukqa%22%2C%22text%22%3A%22So%2C+nowadays+I+like+to+say+that+the+less+I+need+to+get+involved+in+development%2C+the+more+successful+the+project+is.+I+know+that+my+added+value+is+in+my+ability+to+see+the+big+picture%2C+tying+people+and+ideas+together%2C+motivating+a+team%2C+and+keeping+a+focus+on+customer+needs.%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%5D%2C%22entityMap%22%3A%7B%7D%7D&listing_image=&body-count=1&body-0-deleted=&body-0-order=0&slug=where-my-added-value&csrfmiddlewaretoken=mYKgMIn9odTxCaTFwW33lAM6b41fhmSop8iSHudX3lQCm1EkuvB6zC5rOP8KPVHe&expire_at=&date=2019-04-30&intro=Recently+I%27ve+been+thinking+about+where+my+added+value+as+a+technical+project+manager+lies.&go_live_at=&listing_intro=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%225k3ob%22%2C%0D%0A++++++++++++%22text%22%3A+%22%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&subtitle=&body-0-id=&next=&seo_title=&tags=&body-0-type=paragraph&title=Where+is+my+added+value%3F",1556629612.2629163],"_auth_user_hash":"6ded2fe41b9e42ce102b1a8626475c4b1a277a92"}	2019-05-14 13:11:46.259593+00
mn46lbr3mcke31t5kgldyj32c151166y	ODc0MzI3NDQ4OGRjZGU5OWNiZWJjMTAyYTdkMjlhZTQ0OTMzNDUyMTp7Il9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIiwiX2F1dGhfdXNlcl9pZCI6IjEiLCJfYXV0aF91c2VyX2hhc2giOiI2ZGVkMmZlNDFiOWU0MmNlMTAyYjFhODYyNjQ3NWM0YjFhMjc3YTkyIn0=	2019-05-30 16:03:44.021951+00
tvkjg9w1xjeg6ao0ticoz5wlptf0xis3	b1630e4fce7c19afa27dab3ef931c027b8d67dc7:{"_auth_user_backend":"django.contrib.auth.backends.ModelBackend","_auth_user_id":"1","wagtail-preview-blog,blogpage,3":["main_image=&search_description=&body-0-value=%7B%22blocks%22%3A%5B%7B%22key%22%3A%22k9icf%22%2C%22text%22%3A%22%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%221aloa%22%2C%22text%22%3A%22Over+this+time+I+have+found+that+there+are+a+few+principles+that+cloud+providers+who+are+hoping+to+penetrate+the+higher+education+market+would+do+well+to+adhere+to.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%226640g%22%2C%22text%22%3A%22If+cloud+providers+follow+these+principles+when+developing+their+tools+and+services+for+educational+use+then+I+believe+they+have+a+much+greater+chance+of+being+adopted+in+the+classroom.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%2245r62%22%2C%22text%22%3A%22Most+CS+courses+teach+concepts%2C+not+tools%22%2C%22type%22%3A%22header-two%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%7B%22offset%22%3A0%2C%22length%22%3A41%2C%22style%22%3A%22BOLD%22%7D%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22dgj17%22%2C%22text%22%3A%22The+aim+of+most+courses+that+utilize+the+cloud+is+not+to+teach+the+use+of+a+particular+tool+or+platform%2C+but+to+teach+a+computing+concept+and+to+demonstrate+its+application.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22af3ts%22%2C%22text%22%3A%22When+teaching+these+concepts%2C+teaching+time+is+invaluable.+If+you+spend+2+weeks+trying+to+make+sure+everybody+in+a+class+has+an+account+and+basic+knowledge+of+how+to+use+your+platform%2C+then+that+time+is+lost+and+cannot+be+made+up.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%228p1r7%22%2C%22text%22%3A%22For+this+reason%2C+onboarding+for+the+tool+should+be+as+seamless+as+possible.+There+should+be+a+very+clear+pathway+for+students+to+learn+how+to+navigate+your+platform.+A+dedicated+set+of+short+videos+aimed+at+students+that+could+be+assigned+as+a+homework+0+task+would+be+ideal.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22e9b5j%22%2C%22text%22%3A%22It+must+be+easy+for+TFs+and+other+teaching+staff+to+support+their+students%22%2C%22type%22%3A%22header-two%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%7B%22offset%22%3A0%2C%22length%22%3A74%2C%22style%22%3A%22BOLD%22%7D%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22fopat%22%2C%22text%22%3A%22Consider+integrations+with+common+LMS+platforms+to+simplify+adding+students+to+resources+and+easily+enable+SSO.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%221vji%22%2C%22text%22%3A%22Provide+an+administrative+interface+from+which+teaching+staff+can+monitor+students%E2%80%99+spend.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22iq37%22%2C%22text%22%3A%22Be+generous+with+the+amount+of+credit+provided+to+each+student.%22%2C%22type%22%3A%22header-two%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%7B%22offset%22%3A0%2C%22length%22%3A63%2C%22style%22%3A%22BOLD%22%7D%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%2298lnv%22%2C%22text%22%3A%22Every+semester+we+see+students+using+advanced%2C+CPU+and+memory+intensive+operations+earlier+and+earlier+in+their+academic+journey.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%2256gcb%22%2C%22text%22%3A%22Many+entry+level+tasks+these+days+involve+GPUs+which+cost+upwards+of+1%24+per+hour.+It+is+not+unreasonable+to+expect+a+student+to+use+200+-+300+hours+of+GPU+time+over+a+4+month+course.+So%2C+credit+limits+should+be+flexible.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22j1qr%22%2C%22text%22%3A%22It+should+be+very+easy+for+teaching+staff%2C+or+even+students+themselves%2C+to+extend+their+credit+limit+if+necessary.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22e5b8v%22%2C%22text%22%3A%22Students+should+never+be+charged%22%2C%22type%22%3A%22header-two%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%7B%22offset%22%3A0%2C%22length%22%3A32%2C%22style%22%3A%22BOLD%22%7D%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22es6pc%22%2C%22text%22%3A%22I+personally+believe+that+asking+students+to+sign-up+with+a+credit+card+is+potentially+discriminatory.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%22aiav9%22%2C%22text%22%3A%22In+the+past%2C+we+have+seen+students+overspend+on+their+accounts+%28for+example%2C+by+leaving+a+GPU+running%2C+unused%29.+This+has+led+to+many+hours+of+teaching+staff+negotiating+on+their+behalf+to+have+bills+cancelled.+Making+it+as+easy+as+possible+for+students+and+teaching+staff+to+monitor+student+spend+would+help+here.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%2C%7B%22key%22%3A%224na96%22%2C%22text%22%3A%22Tools+should+also+enable+limiting+the+resources+that+a+student+can+use%2C+but+these+limits+should+be+flexible+or+customizable.+%22%2C%22type%22%3A%22unstyled%22%2C%22depth%22%3A0%2C%22inlineStyleRanges%22%3A%5B%5D%2C%22entityRanges%22%3A%5B%5D%2C%22data%22%3A%7B%7D%7D%5D%2C%22entityMap%22%3A%7B%7D%7D&listing_image=&body-count=1&body-0-deleted=&body-0-order=0&slug=thoughts-cloud-providers-classroom&csrfmiddlewaretoken=es77JxqJSZQedH2VEfQ5DmbhcFitgVKiLvPhsD3wTh4pA03V8xCfMSDdSquLQIrA&expire_at=&date=2019-06-27&intro=Over+the+last+two+years%2C+I%E2%80%99ve+been+working+with+Harvard+SEAS+faculty+and+cloud+providers+to+determine+how+we+can+best+facilitate+students+use+of+cloud+technologies+in+the+classroom.+&go_live_at=&listing_intro=%7B%0D%0A++++%22entityMap%22%3A+%7B%7D%2C%0D%0A++++%22blocks%22%3A+%5B%0D%0A++++++++%7B%0D%0A++++++++++++%22entityRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22inlineStyleRanges%22%3A+%5B%5D%2C%0D%0A++++++++++++%22depth%22%3A+0%2C%0D%0A++++++++++++%22type%22%3A+%22unstyled%22%2C%0D%0A++++++++++++%22key%22%3A+%22c9ke7%22%2C%0D%0A++++++++++++%22text%22%3A+%22%22%0D%0A++++++++%7D%0D%0A++++%5D%0D%0A%7D&subtitle=&body-0-id=&next=&seo_title=&tags=&body-0-type=paragraph&title=Thoughts+on+cloud+providers+in+the+classroo",1561659896.8343298],"_auth_user_hash":"6ded2fe41b9e42ce102b1a8626475c4b1a277a92"}	2019-07-11 18:24:56.849726+00
p2hyoucqgaju080yteg4l0dizi4uq34k	ODc0MzI3NDQ4OGRjZGU5OWNiZWJjMTAyYTdkMjlhZTQ0OTMzNDUyMTp7Il9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIiwiX2F1dGhfdXNlcl9pZCI6IjEiLCJfYXV0aF91c2VyX2hhc2giOiI2ZGVkMmZlNDFiOWU0MmNlMTAyYjFhODYyNjQ3NWM0YjFhMjc3YTkyIn0=	2019-08-27 14:51:38.635852+00
z3oafwmzjsw765naqgzhoy35ocfbjf1s	ODc0MzI3NDQ4OGRjZGU5OWNiZWJjMTAyYTdkMjlhZTQ0OTMzNDUyMTp7Il9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIiwiX2F1dGhfdXNlcl9pZCI6IjEiLCJfYXV0aF91c2VyX2hhc2giOiI2ZGVkMmZlNDFiOWU0MmNlMTAyYjFhODYyNjQ3NWM0YjFhMjc3YTkyIn0=	2019-10-08 17:10:33.776523+00
zp4rb4eq07ylvecjfv259twl2ociza8t	ODc0MzI3NDQ4OGRjZGU5OWNiZWJjMTAyYTdkMjlhZTQ0OTMzNDUyMTp7Il9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIiwiX2F1dGhfdXNlcl9pZCI6IjEiLCJfYXV0aF91c2VyX2hhc2giOiI2ZGVkMmZlNDFiOWU0MmNlMTAyYjFhODYyNjQ3NWM0YjFhMjc3YTkyIn0=	2019-12-27 15:29:23.155843+00
\.


--
-- Data for Name: home_homepage; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.home_homepage (page_ptr_id, subtitle) FROM stdin;
3	Explorations and ramblings of Chris Rogers, a technical project manager and developer
\.


--
-- Data for Name: taggit_tag; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.taggit_tag (id, name, slug) FROM stdin;
1	nginx	nginx
2	wagtail	wagtail
3	uwsgi	uwsgi
4	digital ocean	digital-ocean
5	Heroku	heroku
6	logging	logging
7	django	django
8	Tag!	tag
9	database	database
10	evernote	evernote
11	productivity	productivity
13	tag	tag_1
14	tag2	tag2
15	money	money
16	lifestyle	lifestyle
17	saving	saving
18	priming	priming
19	UX	ux
20	research	research
21	success tracking	success-tracking
22	KPIs	kpis
23	analtyics	analtyics
24	measurement	measurement
25	typography	typography
26	inspiration	inspiration
27	prototypes	prototypes
28	design	design
30	ux	ux_1
31	wireframes	wireframes
32	Agile	agile
33	User centred design	user-centred-design
34	UCD	ucd
35	job stories	job-stories
36	planning	planning
37	Project management	project-management
38	user stories	user-stories
39	IA	ia
40	analytics	analytics
41	Content strategy	content-strategy
42	portfolio	portfolio
44	content strategy	content-strategy_1
46	project management	project-management_1
47	python	python
49	agile	agile_1
50	management	management
51	MVP	mvp
52	process	process
53	startup	startup
54	sysadmin	sysadmin
55	memcached	memcached
56	redis	redis
57	caching	caching
58	strategy	strategy
59	user research	user-research
60	ITCSS	itcss
61	CSS	css
62	docker	docker
63	devops	devops
64	tool	tool
65	javascript	javascript
66	information architecture	information-architecture
67	methodology	methodology
68	business	business
69	screenshot	screenshot
70	RSS	rss
71	drupal	drupal
72	elasticsearch	elasticsearch
74	Python	python_1
75	IIIF	iiif
76	Solr	solr
77	SolrCloud	solrcloud
78	Vagrant	vagrant
\.


--
-- Name: taggit_tag_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.taggit_tag_id_seq', 78, true);


--
-- Data for Name: taggit_taggeditem; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.taggit_taggeditem (id, object_id, content_type_id, tag_id) FROM stdin;
\.


--
-- Name: taggit_taggeditem_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.taggit_taggeditem_id_seq', 2, true);


--
-- Data for Name: wagtailcore_collection; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_collection (id, path, depth, numchild, name) FROM stdin;
1	0001	1	0	Root
\.


--
-- Name: wagtailcore_collection_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_collection_id_seq', 1, true);


--
-- Data for Name: wagtailcore_collectionviewrestriction; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_collectionviewrestriction (id, restriction_type, password, collection_id) FROM stdin;
\.


--
-- Data for Name: wagtailcore_collectionviewrestriction_groups; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_collectionviewrestriction_groups (id, collectionviewrestriction_id, group_id) FROM stdin;
\.


--
-- Name: wagtailcore_collectionviewrestriction_groups_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_collectionviewrestriction_groups_id_seq', 1, false);


--
-- Name: wagtailcore_collectionviewrestriction_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_collectionviewrestriction_id_seq', 1, false);


--
-- Data for Name: wagtailcore_groupcollectionpermission; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_groupcollectionpermission (id, collection_id, group_id, permission_id) FROM stdin;
1	1	1	5
2	1	2	5
3	1	1	6
4	1	2	6
5	1	1	1
6	1	2	1
7	1	1	2
8	1	2	2
\.


--
-- Name: wagtailcore_groupcollectionpermission_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_groupcollectionpermission_id_seq', 8, true);


--
-- Data for Name: wagtailcore_grouppagepermission; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_grouppagepermission (id, permission_type, group_id, page_id) FROM stdin;
1	add	1	1
2	edit	1	1
3	publish	1	1
4	add	2	1
5	edit	2	1
6	lock	1	1
\.


--
-- Name: wagtailcore_grouppagepermission_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_grouppagepermission_id_seq', 6, true);


--
-- Data for Name: wagtailcore_page; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_page (id, path, depth, numchild, title, slug, live, has_unpublished_changes, url_path, seo_title, show_in_menus, search_description, go_live_at, expire_at, expired, content_type_id, owner_id, locked, latest_revision_created_at, first_published_at, live_revision_id, last_published_at, draft_title) FROM stdin;
18	00010001000B	3	0	PMs: Know your business objectives	pms-know-your-business-objectives	f	t	/home/pms-know-your-business-objectives/		f		\N	\N	f	27	1	f	2016-03-02 13:17:39.68081+00	\N	\N	\N	PMs: Know your business objectives
17	00010001000A	3	0	Serving AMP content with Wagtail and RoutablePage	serving-amp-content-wagtail-and-routablepage	f	t	/home/serving-amp-content-wagtail-and-routablepage/		f		\N	\N	f	27	1	f	2016-02-26 08:58:36.923591+00	\N	\N	\N	Serving AMP content with Wagtail and RoutablePage
7	000100010003	3	0	Super simple Django error logging configuration, good for Heroku	django-error-logging-configuration-heroku	t	f	/home/django-error-logging-configuration-heroku/	Simple Django error logging configuration for Heroku	f	A simple code block to output Django error messages to the console	\N	\N	f	27	1	f	2016-02-28 19:44:13.497154+00	2015-06-29 19:56:08.690001+00	\N	2016-02-28 19:44:13.497154+00	Super simple Django error logging configuration, good for Heroku
1	0001	1	1	Root	root	t	f	/		f		\N	\N	f	1	\N	f	\N	\N	\N	\N	Root
15	000100010008	3	0	Bookmarks	bookmarks	t	f	/home/bookmarks/		f		\N	\N	f	35	1	f	2016-02-23 13:40:25.715336+00	2016-02-23 13:40:25.770556+00	\N	2016-02-23 13:40:25.715336+00	Bookmarks
20	00010001000D	3	0	What's a project management qualification good for?	what-project-management-qualification-good-for	t	f	/home/what-project-management-qualification-good-for/	What's a project management qualification good for?	f	To Prince2 or not to Prince2	\N	\N	f	27	1	f	2016-03-09 16:09:00.435096+00	2016-03-09 14:09:21.243725+00	\N	2016-03-09 16:09:00.435096+00	What's a project management qualification good for?
9	000100010005	3	0	Separating local and Heroku database settings for Django	separating-local-and-heroku-database-settings-django	t	f	/home/separating-local-and-heroku-database-settings-django/		f	Stop Heroku database settings from breaking your local Django build	\N	\N	f	27	1	f	2016-02-28 19:43:37.946682+00	2015-07-29 21:46:27.84229+00	\N	2016-02-28 19:43:37.946682+00	Separating local and Heroku database settings for Django
16	000100010009	3	0	Simple content import script for Django / Wagtail	simple-content-import-script-django-wagtail	t	f	/home/simple-content-import-script-django-wagtail/		f	A short script that imports content from a CSV file into Django / Wagtail CMS	\N	\N	f	27	1	f	2016-03-04 13:21:18.12479+00	2016-02-22 13:41:14.867862+00	\N	2016-03-04 13:21:18.12479+00	Simple content import script for Django / Wagtail
14	000100010007	3	0	Using Evernote as a to-do list	using-evernote-as-a-to-do-list	t	f	/home/using-evernote-as-a-to-do-list/		f	Organise your day and manage your reading list better using Evernote	\N	\N	f	27	1	f	2016-03-04 13:22:01.671171+00	2015-12-15 22:15:02.212241+00	\N	2016-03-04 13:22:01.671171+00	Using Evernote as a to-do list
19	00010001000C	3	0	Adding an RSS feed to a Wagtail site	adding-rss-feed-wagtail-site	t	f	/home/adding-rss-feed-wagtail-site/		f	Add an RSS feed to Wagtail using Django's out-of-the-box functionality	\N	\N	f	27	1	f	2016-03-09 14:18:05.2226+00	2016-03-04 13:16:09.072213+00	\N	2016-03-09 14:18:05.2226+00	Adding an RSS feed to a Wagtail site
22	00010001000F	3	0	Local IIIF presentation API validator	local-iiif-presentation-api-validator	t	f	/home/local-iiif-presentation-api-validator/		f		\N	\N	f	27	1	f	2016-08-15 19:14:11.71415+00	2016-08-15 19:14:11.739788+00	\N	2016-08-15 19:14:11.71415+00	Local IIIF presentation API validator
6	000100010002	3	0	Who am I?	who-am-i	f	t	/home/who-am-i/		f		\N	\N	f	27	1	f	2018-03-06 20:59:34.412469+00	2015-06-24 12:20:37.428604+00	\N	\N	Who am I?
24	00010001000H	3	0	5 ways to get the best out of your digital agency	5-ways-get-best-out-your-digital-agency	t	f	/home/5-ways-get-best-out-your-digital-agency/		f	When employing a digital agency on a project, you can save time and money with a little pre-project preparation	\N	\N	f	27	1	f	2016-11-19 14:55:18.292503+00	2016-11-14 19:44:46.34184+00	\N	2016-11-19 14:55:18.292503+00	5 ways to get the best out of your digital agency
25	00010001000I	3	0	Where is my added value?	where-my-added-value	t	f	/home/where-my-added-value/		f	Recently I've been thinking about where my added value as a technical project manager lies.	\N	\N	f	27	1	f	2019-04-30 13:09:13.423651+00	2019-04-30 13:07:36.774912+00	204	2019-04-30 13:09:13.445185+00	Where is my added value?
23	00010001000G	3	0	Testing SolrCloud with Vagrant	testing-solrcloud-vagrant	t	f	/home/testing-solrcloud-vagrant/	Testing SolrCloud with Vagrant	f	Easy steps to emulate a multi-machine setup locally	\N	\N	f	27	1	f	2019-04-30 13:16:07.503378+00	2016-08-31 15:57:58.430541+00	208	2019-04-30 13:16:07.536284+00	Testing SolrCloud with Vagrant
26	00010001000J	3	0	Thoughts on cloud providers in the classroom	thoughts-cloud-providers-classroom	t	f	/home/thoughts-cloud-providers-classroom/		f		\N	\N	f	27	1	f	2019-06-27 18:26:24.639454+00	2019-06-27 18:26:09.406065+00	215	2019-06-27 18:26:24.657782+00	Thoughts on cloud providers in the classroom
21	00010001000E	3	0	About me	chris-rogers	t	f	/home/chris-rogers/		f		\N	\N	f	27	1	f	2019-09-24 17:11:21.500185+00	2016-04-01 11:52:47.21496+00	216	2019-09-24 17:11:21.529908+00	About me
4	000100010001	3	0	What is this place?	what-place	t	f	/home/what-place/		f	How this site was made, and what it was made with	\N	\N	f	27	1	f	2019-05-21 02:17:43.176016+00	2015-06-22 12:48:56.803159+00	213	2019-05-21 02:17:43.193459+00	What is this place?
3	00010001	2	17	chrxr.com	home	t	f	/home/	chrxr.com | Digital project management and web development	f	Explorations and ramblings of Chris Rogers, a digital project manager and hobbyist developer	\N	\N	f	3	\N	f	2016-10-23 18:42:56.796359+00	2015-06-25 12:11:06.703575+00	\N	2016-10-23 18:42:56.796359+00	chrxr.com
\.


--
-- Name: wagtailcore_page_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_page_id_seq', 26, true);


--
-- Data for Name: wagtailcore_pagerevision; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_pagerevision (id, submitted_for_moderation, created_at, content_json, approved_go_live_at, page_id, user_id) FROM stdin;
71	f	2016-02-25 17:31:15.960468+00	{"subtitle": "Make Google happy with you by serving news content quickly", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-02-25T07:55:55.486Z", "go_live_at": null, "title": "Serving AMP content with Wagtail and RoutablePage", "seo_title": "", "listing_intro": "", "slug": "serving-amp-content-wagtail-and-routablepage", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The Accelerated Mobile Pages project is an open-source initiative to enable super-fast loading of content on mobile devices. Google are spearheading the effort, but many global news organisations are in on it as well.\\\\u00a0I decided to see if I could get Wagtail to output AMP pages as part of a general content creation workflow.</p><p>First of all, some general info/requirements for building an AMP page:</p><p></p><ul><li>AMP pages use mostly vanilla HTML with a couple of custom elements (see &lt;amp-img&gt; below)<br/></li><li>Only AMP-verified external resources are allowed to be included. So no additional JavaScript or CSS files.</li><li>There are a number of specific elements that are required on an AMP page, without which it won't be valid. <a href=\\\\\\"https://www.ampproject.org/docs/get_started/create/basic_markup.html\\\\\\">See this page for more info on this.</a></li><li>Additional CSS can be added in a singly &lt;style&gt; element, but it must be in the &lt;head&gt;</li></ul><p><br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-25", "path": "00010001000A", "url_path": "/home/serving-amp-content-wagtail-and-routablepage/", "expired": false, "pk": 17, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	17	1
52	f	2015-12-15 22:15:58.430237+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "I'm the type of person who really needs a to-do list but is rubbish at maintaining one. I've tried many different solutions: specific apps, pen and paper, spreadsheets. None of them have lasted for longer than a couple of weeks.", "latest_revision_created_at": "2015-12-15T22:15:02.044Z", "go_live_at": null, "title": "Using Evernote as a to-do list", "seo_title": "", "listing_intro": "", "slug": "using-evernote-as-a-to-do-list", "live": true, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Now though I think I've settled on a system that really works for me. I use Evernote ALL THE TIME. It's an essential tool for me. So I thought, how can I use the tool that I already use to help me organise my time.<br/></p><p>The key was to format it in an appropriate way. At the top of my list I have the 'week beginning' date. Then I have my current list of to-dos, ordered roughly by priority. I add EVERYTHING work-related that I have to do that is a discrete task, even small tasks. If I do something work-related that's not on the to-do list, I add it to the list. I use the checkbox list type, as it soothes my OCD side to see each box ticked off. Once a task has been ticked off, I copy and paste it into another list below the main list, which has today's date as the heading. So as the days go by you end up with an archive of your completed tasks for each individual day.</p><p>Visibility of the list is key. One of the biggest pains in trying to use a specific tool was that it was one more thing to have open on the screen. With Evernote, I just add it to my 'Shortcuts' list. Then it's always there in the top-left of Evernote, a tool that I would have open all the time anyway!</p><p><embed alt=\\\\\\"Lunchtime_reads.png\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"2\\\\\\"/><br/></p><p>At the end of the month, I'll archive that particular note in a separate notebook, take it out of my shortcuts, and create a new to-do note for the month.</p><p>I've now started to expand the format to include other types of to-dos. I often review relevant newsletters and blogs for content as soon as I get into work, but then I like to actually read them over lunch. So I've created a 'Lunchtime reading' list, again with tick-boxes, and a 'Reading archive' at the bottom of the to-do note.</p><p><embed alt=\\\\\\"Tasks_and_archive.png\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"3\\\\\\"/><br/></p><p>Anyway, it's a very simple solution, that appeals to me for it's 'Collect the completed tasks!' nature.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-12-07", "path": "000100010007", "url_path": "/home/using-evernote-do-list/", "expired": false, "pk": 14, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-12-15T22:15:02.212Z", "expire_at": null}	\N	14	1
38	f	2015-07-29 21:46:00.173097+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "", "slug": "separating-local-and-heroku-database-settings-django", "live": false, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES is improperly configured. Please supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	9	1
113	f	2016-03-04 15:59:28.350037+00	{"subtitle": "", "search_description": "Add an RSS feed to Wagtail using Django's out-of-the-box functionality", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T13:20:31.469Z", "go_live_at": null, "title": "Adding an RSS feed to a Wagtail site", "seo_title": "", "listing_intro": "<p>Add an RSS feed to Wagtail using Django's out-of-the-box functionality</p>", "slug": "adding-rss-feed-wagtail-site", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented an RSS feed on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define three standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply import your Feed class into the\\\\u00a0<b>urls.py </b>file,\\\\u00a0\\\\u00a0then\\\\u00a0add the line below to the urlpatterns section.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from blog_feed.feeds import BlogsFeed\\\\r\\\\n\\\\r\\\\nurlpatterns = [\\\\r\\\\n    #.... lots of URLs\\\\r\\\\n\\\\r\\\\n    url(r'blog-feed/$', BlogsFeed()),\\\\r\\\\n\\\\r\\\\n   # .... more URLs\\\\r\\\\n]\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>So, when someone visits the URL http://chrxr.com/blog-feed, Wagtail initialises a new instance of the class BlogFeed, which responds to the browser with the feed of links defined in the class methods.</p><p>If you have an RSS browser extension installed (like me), this might display the links in the feed as a nice list. If not you'll likely just see a load of XML. Generally though, as long as you don't see an error, then it's working!</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 6}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>See it on GitHub</h3><p>This blog uses GitHub for version control, so the source code for anything I mention can generally <a href=\\\\\\"http://github.com/chrxr/blog_project\\\\\\">be found there</a>.</p><p></p><ul><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_feed/feeds.py#L25\\\\\\">feed.py file</a><br/></li><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_project/urls.py#L22\\\\\\">URL configuration</a></li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-04", "path": "00010001000C", "url_path": "/home/adding-rss-feed-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-04T13:16:09.072Z", "expire_at": null}	\N	19	1
126	f	2016-03-09 14:57:30.733257+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T14:56:08.018Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>\\\\u00a0A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>\\\\u00a0Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
128	f	2016-03-09 14:59:07.815217+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T14:58:58.108Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in a future post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
130	f	2016-03-09 15:47:34.913839+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T15:47:07.317Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists -- open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in a future post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
118	f	2016-03-09 13:17:09.398139+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T13:16:41.216Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "", "slug": "it-worth-doing-project-management-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager' for over three years, and have been running projects in other roles for over five years. In that time I've never had a project catastrophically fail, or be cancelled. I've had projects go over budget, I've delivered projects late, but I've always delivered.</p><p>But here's the thing: I'm pretty sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes with a lack of discipline. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to DO Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum an effective framework is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h3>A note for employers</h3><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB!</p><p>Not an entry level one anyway. Discipline is just one of the four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skill and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty to the organisation, resulting in a lower rate of staff turnover.</p><p>* My four pillars of good project management are:<br/></p><p></p><ol><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ol><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000D", "url_path": "/home/it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
133	f	2016-03-09 16:08:21.473356+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T15:49:47.095Z", "go_live_at": null, "title": "What's a project management qualification good for?", "seo_title": "What's a project management qualification good for?", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists \\\\u2014 open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in a future post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
80	f	2016-02-28 19:49:28.553925+00	{"subtitle": "Explorations and ramblings of a digital project manager and hobbyist developer", "search_description": "Explorations and ramblings of a digital project manager and hobbyist developer", "owner": null, "latest_revision_created_at": "2016-02-28T19:35:04.388Z", "go_live_at": null, "title": "chrxr.com", "seo_title": "chrxr.com | Wagtail CMS and Digital project management", "slug": "home", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "numchild": 9, "content_type": 3, "show_in_menus": false, "path": "00010001", "url_path": "/home/", "expired": false, "pk": 3, "locked": false, "depth": 2, "first_published_at": "2015-06-25T12:11:06.703Z", "expire_at": null}	\N	3	1
193	f	2016-11-14 18:10:30.059683+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-10-23T20:17:51.396Z", "go_live_at": null, "title": "5 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "5-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>There are a number of things you can do before embarking on a project with a digital agency that will save time when the project begins, save you money over the life of the project, and make your agency love you as a client!</p><h3>1 - Establish clear lines of communications</h3><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\\\u00a0meetings\\\\u00a0with the top-stakeholders\\\\u00a0at the beginning of the process\\\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\\\\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\\\\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/10-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
194	f	2016-11-14 18:11:24.579295+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-11-14T18:10:30.059Z", "go_live_at": null, "title": "5 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "5-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>There are a number of things you can do before embarking on a project with a digital agency that will save time when the project begins, save you money over the life of the project, and make your agency love you as a client! Here are my five most important:</p><h3>1 - Establish clear lines of communications</h3><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\\\u00a0meetings\\\\u00a0with the top-stakeholders\\\\u00a0at the beginning of the process\\\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\\\\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\\\\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/5-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
69	f	2016-02-23 13:40:25.715336+00	{"search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-02-22T10:35:52.279Z", "go_live_at": null, "title": "Bookmarks", "seo_title": "", "slug": "bookmarks", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "numchild": 0, "content_type": 35, "show_in_menus": false, "path": "000100010008", "url_path": "/home/bookmarks/", "expired": false, "pk": 15, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	15	1
53	f	2015-12-15 22:16:35.933408+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "I'm the type of person who really needs a to-do list but is rubbish at maintaining one. I've tried many different solutions: specific apps, pen and paper, spreadsheets. None of them have lasted for longer than a couple of weeks.", "latest_revision_created_at": "2015-12-15T22:15:58.430Z", "go_live_at": null, "title": "Using Evernote as a to-do list", "seo_title": "", "listing_intro": "", "slug": "using-evernote-as-a-to-do-list", "live": true, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Now though I think I've settled on a system that really works for me. I use Evernote ALL THE TIME. It's an essential tool for me. So I thought, how can I use the tool that I already use to help me organise my time.<br/></p><p>The key was to format it in an appropriate way. At the top of my list I have the 'week beginning' date. Then I have my current list of to-dos, ordered roughly by priority. I add EVERYTHING work-related that I have to do that is a discrete task, even small tasks. If I do something work-related that's not on the to-do list, I add it to the list. I use the checkbox list type, as it soothes my OCD side to see each box ticked off. Once a task has been ticked off, I copy and paste it into another list below the main list, which has today's date as the heading. So as the days go by you end up with an archive of your completed tasks for each individual day.</p><p>Visibility of the list is key. One of the biggest pains in trying to use a specific tool was that it was one more thing to have open on the screen. With Evernote, I just add it to my 'Shortcuts' list. Then it's always there in the top-left of Evernote, a tool that I would have open all the time anyway!</p><p><embed alt=\\\\\\"Lunchtime_reads.png\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"2\\\\\\"/><br/></p><p>At the end of the month, I'll archive that particular note in a separate notebook, take it out of my shortcuts, and create a new to-do note for the month.</p><p>I've now started to expand the format to include other types of to-dos. I often review relevant newsletters and blogs for content as soon as I get into work, but then I like to actually read them over lunch. So I've created a 'Lunchtime reading' list, again with tick-boxes, and a 'Reading archive' at the bottom of the to-do note.</p><p><embed alt=\\\\\\"Tasks_and_archive.png\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"3\\\\\\"/><br/></p><p>Anyway, it's a very simple solution, that appeals to me for it's 'Collect the completed tasks!' nature.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-12-14", "path": "000100010007", "url_path": "/home/using-evernote-as-a-to-do-list/", "expired": false, "pk": 14, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-12-15T22:15:02.212Z", "expire_at": null}	\N	14	1
127	f	2016-03-09 14:58:58.10871+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T14:57:30.733Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
131	f	2016-03-09 15:49:09.570904+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T15:47:34.913Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists \\\\u2013 open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in a future post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
195	f	2016-11-14 18:16:31.299537+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-11-14T18:11:24.579Z", "go_live_at": null, "title": "5 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "5-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>1 - Establish clear lines of communications<br/></p><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\\\u00a0meetings\\\\u00a0with the top-stakeholders\\\\u00a0at the beginning of the process\\\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\\\\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\\\\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-11-14", "path": "00010001000H", "url_path": "/home/5-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
185	f	2016-10-22 17:03:27.495976+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "10 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "10-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>When working with a digital agency, there are a number of things you can do to get the best out of them. By doing these things you can make the your life easier, their life easier, and save a bunch of money as well.</p><h3>Establish lines of communications</h3><h3>Gather all the user information that you have</h3><h3>Focus on the discovery phase</h3><h3>What are your business goals?</h3><h3>Start tracking things as soon as possible</h3><h3>Got brand guidelines? Hand'em over!</h3>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/10-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
184	f	2016-09-06 06:58:13.438412+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "Easy steps to emulate a multi-machine setup locally", "owner": 1, "intro": "", "latest_revision_created_at": "2016-09-06T06:56:17.825Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "Testing SolrCloud with Vagrant", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "slug": "testing-solrcloud-vagrant", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'd been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distributions. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. Queries arrive at one of the nodes and the query is then forwarded to the node where a replica of the appropriate shard is located.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>Our aim for this test is to have each element of the SolrCloud setup running on its own virtual machine. Our setup will have three Solr nodes with which we can store and query the data, and a single Zookeeper instance to manage the nodes. We could have multiple Zookeeper instances to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that we can split our data into two shards with two replicas of each, and if one of the Solr nodes goes down we'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Our test network will be built using Ubuntu virtual machines. The first thing we're going to do is create a new directory for our test VMs on our host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. I'm going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). So, we need  to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building the four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If we run this command on each box, we should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node we currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then we restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 -replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into great detail on how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p><embed alt=\\\\\\"SolrCloud finishing screen\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"7\\\\\\"/><br/></p><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-08-31T15:57:58.430Z", "expire_at": null}	\N	23	1
179	f	2016-09-01 08:30:42.512736+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "Easy steps to emulate a multi-machine setup locally", "owner": 1, "intro": "", "latest_revision_created_at": "2016-09-01T07:08:35.680Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "slug": "testing-solrcloud-vagrant", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building your four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p><embed alt=\\\\\\"SolrCloud finishing screen\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"7\\\\\\"/><br/></p><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-08-31T15:57:58.430Z", "expire_at": null}	\N	23	1
177	f	2016-09-01 07:07:46.767932+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "Easy steps to emulate a multi-machine setup locally", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T15:59:20.830Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building your four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.\\\\u00a0</p><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-08-31T15:57:58.430Z", "expire_at": null}	\N	23	1
180	f	2016-09-01 09:01:09.392714+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "Easy steps to emulate a multi-machine setup locally", "owner": 1, "intro": "", "latest_revision_created_at": "2016-09-01T08:30:42.512Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "slug": "testing-solrcloud-vagrant", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'd been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distributions. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. Queries arrive at one of the nodes and the query is then forwarded to the node where a replica of the appropriate shard is located.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>Our aim for this test is to have each element of the SolrCloud setup running on its own virtual machine. Our setup will have three Solr nodes with which we can store and query the data, and a single Zookeeper instance to manage the nodes. We could have multiple Zookeeper instances to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that we can split our data into two shards with two replicas of each, and if one of the Solr nodes goes down we'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Our test network will be built using Ubuntu virtual machines. The first thing we're going to do is create a new directory for our test VMs on our host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. I'm going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). So, we need  to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building the four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If we run this command on each box, we should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node we currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then we restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into great detail on how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p><embed alt=\\\\\\"SolrCloud finishing screen\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"7\\\\\\"/><br/></p><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-08-31T15:57:58.430Z", "expire_at": null}	\N	23	1
40	f	2015-07-29 21:46:55.989491+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-29T21:46:27.821Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "", "slug": "separating-local-and-heroku-database-settings-django", "live": true, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\\\r\\\\nis improperly configured. Please supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-07-29T21:46:27.842Z", "expire_at": null}	\N	9	1
82	f	2016-02-28 19:55:49.468326+00	{"subtitle": "", "search_description": "Organise your day and manage your reading list better using Evernote", "owner": 1, "intro": "I'm the type of person who really needs a to-do list but is rubbish at maintaining one. I've tried many different solutions: specific apps, pen and paper, spreadsheets. None of them have lasted for longer than a couple of weeks.", "latest_revision_created_at": "2016-02-28T19:41:16.053Z", "go_live_at": null, "title": "Using Evernote as a to-do list", "seo_title": "", "listing_intro": "", "slug": "using-evernote-as-a-to-do-list", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Now though I think I've settled on a system that really works for me. I use Evernote ALL THE TIME. It's an essential tool for me. So I thought, how can I use the tool that I already use to help me organise my time.<br/></p><p>The key was to format it in an appropriate way. At the top of my list I have the 'week beginning' date. Then I have my current list of to-dos, ordered roughly by priority. I add EVERYTHING work-related that I have to do that is a discrete task, even small tasks. If I do something work-related that's not on the to-do list, I add it to the list. I use the checkbox list type, as it soothes my OCD side to see each box ticked off. Once a task has been ticked off, I copy and paste it into another list below the main list, which has today's date as the heading. So as the days go by you end up with an archive of your completed tasks for each individual day.</p><p>Visibility of the list is key. One of the biggest pains in trying to use a specific tool was that it was one more thing to have open on the screen. With Evernote, I just add it to my 'Shortcuts' list. Then it's always there in the top-left of Evernote, a tool that I would have open all the time anyway!</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 2}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>At the end of the month, I'll archive that particular note in a separate notebook, take it out of my shortcuts, and create a new to-do note for the month.</p><p>I've now started to expand the format to include other types of to-dos. I often review relevant newsletters and blogs for content as soon as I get into work, but then I like to actually read them over lunch. So I've created a 'Lunchtime reading' list, again with tick-boxes, and a 'Reading archive' at the bottom of the to-do note.</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 3}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Anyway, it's a very simple solution, that appeals to me for it's 'Collect the completed tasks!' nature.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-12-14", "path": "000100010007", "url_path": "/home/using-evernote-as-a-to-do-list/", "expired": false, "pk": 14, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-12-15T22:15:02.212Z", "expire_at": null}	\N	14	1
115	f	2016-03-09 13:11:46.487836+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T12:47:41.560Z", "go_live_at": null, "title": "Is it worth doing a PM qualification?", "seo_title": "", "listing_intro": "", "slug": "it-worth-doing-pm-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working officially as a project manager for over three years, and have been running projects in other roles for over five years. In that time I've never had a project catastrophically fail, or be cancelled. I've had projects go over budget, I've delivered projects late, but I've always successfully delivered.</p><p>But here's the thing: I'm pretty sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes with a lack of discipline. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to DO Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum an effective framework is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h3>A note for employers</h3><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB!</p><p>Not an entry level one anyway. Discipline is just one of the four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skill and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them on a PM training course if necessary.</p><p><br/></p><p>* More about these in another post.<br/></p><p></p><ol><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ol><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000D", "url_path": "/home/it-worth-doing-pm-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
117	f	2016-03-09 13:16:41.216863+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T13:16:21.118Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "", "slug": "it-worth-doing-project-management-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager' for over three years, and have been running projects in other roles for over five years. In that time I've never had a project catastrophically fail, or be cancelled. I've had projects go over budget, I've delivered projects late, but I've always successfully delivered.</p><p>But here's the thing: I'm pretty sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes with a lack of discipline. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to DO Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum an effective framework is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h3>A note for employers</h3><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB!</p><p>Not an entry level one anyway. Discipline is just one of the four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skill and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty to the organisation, resulting in a lower rate of staff turnover.</p><p>* My four pillars of good project management are:<br/></p><p></p><ol><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ol><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000D", "url_path": "/home/it-worth-doing-pm-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
119	f	2016-03-09 13:19:51.231651+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T13:17:09.398Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "", "slug": "it-worth-doing-project-management-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager' for over three years, and have been running projects in other roles for over five years. In that time I've never had a project catastrophically fail, or be cancelled. I've had projects go over budget, I've delivered projects late, but I've always delivered.</p><p>But here's the thing: I'm pretty sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes with a lack of discipline. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to DO Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum an effective framework is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h3>A note for employers</h3><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB!</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty to the organisation, resulting in a lower rate of staff turnover.</p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000D", "url_path": "/home/it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
132	f	2016-03-09 15:49:47.095892+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T15:49:09.570Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists \\\\u2014 open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in a future post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
129	f	2016-03-09 15:47:07.317553+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T14:59:07.815Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists \\\\u00a0open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in a future post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
41	f	2015-07-29 21:47:13.818221+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-29T21:46:55.989Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "", "slug": "separating-local-and-heroku-database-settings-django", "live": true, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\\\r\\\\nis improperly configured. \\\\r\\\\nPlease supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-07-29T21:46:27.842Z", "expire_at": null}	\N	9	1
7	f	2015-06-24 12:20:37.39919+00	{"subtitle": "A brief history of me", "search_description": "", "owner": 1, "intro": "My name is Chris Rogers and I'm currently working as a project manager at Torchbox, a digital agency based in Oxford, England.", "latest_revision_created_at": null, "go_live_at": null, "title": "Who am I?", "seo_title": "", "listing_intro": "", "slug": "who-am-i", "live": true, "has_unpublished_changes": false, "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<ul><li>Previously worked in digital publishing for 5 years, at Oxford University Press and Penguin Books.</li><li>Now a hands on project manager at Torchbox, developing websites and digital strategies for large multi-national charities.</li><li>Like to get my hands dirty helping with development where necessary.</li><li>Main experience is with Python, Django, JavaScript, CSS and putting these all together into Wagtail</li></ul>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-24", "path": "0001000100010002", "url_path": "/home/what-place/who-am-i/", "expired": false, "pk": 6, "main_image": null, "locked": false, "depth": 4, "first_published_at": null, "expire_at": null}	\N	6	1
116	f	2016-03-09 13:16:21.118099+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T13:11:46.487Z", "go_live_at": null, "title": "Is it worth doing a PM qualification?", "seo_title": "", "listing_intro": "", "slug": "it-worth-doing-pm-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager' for over three years, and have been running projects in other roles for over five years. In that time I've never had a project catastrophically fail, or be cancelled. I've had projects go over budget, I've delivered projects late, but I've always successfully delivered.</p><p>But here's the thing: I'm pretty sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes with a lack of discipline. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to DO Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum an effective framework is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h3>A note for employers</h3><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB!</p><p>Not an entry level one anyway. Discipline is just one of the four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skill and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty to the organisation, resulting in a lower rate of staff turnover.</p><p>* My four pillars of good project management are:<br/></p><p></p><ol><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ol><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000D", "url_path": "/home/it-worth-doing-pm-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
186	f	2016-10-22 17:58:11.48204+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-10-22T17:03:27.495Z", "go_live_at": null, "title": "10 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "10-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>When working with a digital agency, there are a number of things you can do to get the best out of them. By doing these things you can make the your life easier, their life easier, and save a bunch of money as well.</p><h3>Establish clear lines of communications</h3><h3>Gather all the user information that you have</h3><h3>Focus on the discovery phase</h3><h3>What are your business goals?</h3><h3>Start tracking things as soon as possible</h3><h3>Got brand guidelines? Hand'em over!</h3>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/10-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
196	f	2016-11-14 18:34:49.333532+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-11-14T18:16:31.299Z", "go_live_at": null, "title": "5 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "5-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>1 - Establish clear lines of communications<br/></p><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\\\u00a0meetings\\\\u00a0with the top-stakeholders\\\\u00a0at the beginning of the process\\\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\\\\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\\\\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-11-14", "path": "00010001000H", "url_path": "/home/5-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
85	f	2016-03-01 12:47:53.210906+00	{"subtitle": "It's not the designer's job to know what business objectives are, it's yours", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "PMs: Know your business objectives", "seo_title": "", "listing_intro": "", "slug": "pms-know-your-business-objectives", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across <a href=\\\\\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\\\\\">this article</a> by designer, Joshua Taylor, on Medium.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-01", "path": "00010001000B", "url_path": "/home/pms-know-your-business-objectives/", "expired": false, "pk": 18, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	18	1
100	f	2016-03-03 19:51:34.176468+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T19:48:28.177Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\\\r\\\\n\\\\r\\\\n    def item_link(self, item):\\\\r\\\\n        base_url = item.get_absolute_url()\\\\r\\\\n        return base_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title><link>https://chrxr.com/blogs-feed/</link><description>All of my blogs as they are published</description><atom:link href=\\\\\\"https://chrxr.com/blog-feed/\\\\\\" rel=\\\\\\"self\\\\\\"></atom:link>\\", \\"language\\": \\"html\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
43	f	2015-07-29 21:52:51.70012+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-29T21:47:52.483Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "", "slug": "separating-local-and-heroku-database-settings-django", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 9, "content_object": 9}, {"pk": null, "tag": 5, "content_object": 9}, {"pk": null, "tag": 7, "content_object": 9}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\\\r\\\\nis improperly configured. \\\\r\\\\nPlease supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to settings.py to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-07-29T21:46:27.842Z", "expire_at": null}	\N	9	1
39	f	2015-07-29 21:46:27.821253+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-29T21:46:00.173Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "", "slug": "separating-local-and-heroku-database-settings-django", "live": false, "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES is improperly configured. Please supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	9	1
120	f	2016-03-09 13:24:07.120899+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T13:19:51.231Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "", "slug": "it-worth-doing-project-management-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager' for over three years, and have been running projects in other roles for over five years. In that time I've never had a project catastrophically fail, or be cancelled. I've had projects go over budget, I've delivered projects late, but I've always delivered.</p><p>But here's the thing: I'm pretty sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes with a lack of discipline. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to DO Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum an effective framework is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h3>A note for employers</h3><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB!</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty to the organisation, resulting in a lower rate of staff turnover.</p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000D", "url_path": "/home/it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
134	f	2016-03-09 16:09:00.435096+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T16:08:21.473Z", "go_live_at": null, "title": "What's a project management qualification good for?", "seo_title": "What's a project management qualification good for?", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "what-project-management-qualification-good-for", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing:<b> I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</b></p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: <b>discipline</b>.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists \\\\u2014 open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in a future post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
42	f	2015-07-29 21:47:52.483543+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-29T21:47:13.818Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "", "slug": "separating-local-and-heroku-database-settings-django", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 9, "content_object": 9}, {"pk": null, "tag": 5, "content_object": 9}, {"pk": null, "tag": 7, "content_object": 9}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\\\r\\\\nis improperly configured. \\\\r\\\\nPlease supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-07-29T21:46:27.842Z", "expire_at": null}	\N	9	1
81	f	2016-02-28 19:49:52.824654+00	{"subtitle": "Explorations and ramblings of a digital project manager and hobbyist developer", "search_description": "Explorations and ramblings of a digital project manager and hobbyist developer", "owner": null, "latest_revision_created_at": "2016-02-28T19:49:28.553Z", "go_live_at": null, "title": "chrxr.com", "seo_title": "chrxr.com | Digital project management and web development", "slug": "home", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "numchild": 9, "content_type": 3, "show_in_menus": false, "path": "00010001", "url_path": "/home/", "expired": false, "pk": 3, "locked": false, "depth": 2, "first_published_at": "2015-06-25T12:11:06.703Z", "expire_at": null}	\N	3	1
109	f	2016-03-04 13:20:31.469427+00	{"subtitle": "", "search_description": "Add an RSS feed to Wagtail using Django's out-of-the-box functionality", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T13:16:09.033Z", "go_live_at": null, "title": "Adding an RSS feed to a Wagtail site", "seo_title": "", "listing_intro": "<p>Add an RSS feed to Wagtail using Django's out-of-the-box functionality</p>", "slug": "adding-rss-feed-wagtail-site", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented an RSS feed on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define four standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply import your Feed class into the\\\\u00a0<b>urls.py </b>file,\\\\u00a0\\\\u00a0then\\\\u00a0add the line below to the urlpatterns section.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from blog_feed.feeds import BlogsFeed\\\\r\\\\n\\\\r\\\\nurlpatterns = [\\\\r\\\\n    #.... lots of URLs\\\\r\\\\n\\\\r\\\\n    url(r'blog-feed/$', BlogsFeed()),\\\\r\\\\n\\\\r\\\\n   # .... more URLs\\\\r\\\\n]\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>So, when someone visits the URL http://chrxr.com/blog-feed, Wagtail initialises a new instance of the class BlogFeed, which responds to the browser with the feed of links defined in the class methods.</p><p>If you have an RSS browser extension installed (like me), this might display the links in the feed as a nice list. If not you'll likely just see a load of XML. Generally though, as long as you don't see an error, then it's working!</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 6}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>See it on GitHub</h3><p>This blog uses GitHub for version control, so the source code for anything I mention can generally <a href=\\\\\\"http://github.com/chrxr/blog_project\\\\\\">be found there</a>.</p><p></p><ul><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_feed/feeds.py#L25\\\\\\">feed.py file</a><br/></li><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_project/urls.py#L22\\\\\\">URL configuration</a></li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-04", "path": "00010001000C", "url_path": "/home/adding-rss-feed-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-04T13:16:09.072Z", "expire_at": null}	\N	19	1
121	f	2016-03-09 14:00:59.587553+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T13:24:07.120Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "", "slug": "it-worth-doing-project-management-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen . Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing: I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>\\\\u00a0A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>\\\\u00a0Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000D", "url_path": "/home/it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
197	f	2016-11-14 18:35:31.767405+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-11-14T18:34:49.333Z", "go_live_at": null, "title": "5 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "5-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>There are a number of things you can do before embarking on a project with a digital agency that will save time when the project begins, save you money over the life of the project, and make your agency love you as a client! Here are my five most important:</p><h3>1 - Establish clear lines of communications</h3><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\\\u00a0meetings\\\\u00a0with the top-stakeholders\\\\u00a0at the beginning of the process\\\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\\\\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\\\\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-11-14", "path": "00010001000H", "url_path": "/home/5-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
1	f	2015-06-22 12:48:56.774984+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "", "slug": "what-place", "live": true, "has_unpublished_changes": false, "body": "[]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "00010002", "url_path": "/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 2, "first_published_at": null, "expire_at": null}	\N	4	1
183	f	2016-09-06 06:56:17.825692+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "Easy steps to emulate a multi-machine setup locally", "owner": 1, "intro": "", "latest_revision_created_at": "2016-09-01T09:01:09.392Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "Testing SolrCloud with Vagrant", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "slug": "testing-solrcloud-vagrant", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'd been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distributions. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. Queries arrive at one of the nodes and the query is then forwarded to the node where a replica of the appropriate shard is located.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>Our aim for this test is to have each element of the SolrCloud setup running on its own virtual machine. Our setup will have three Solr nodes with which we can store and query the data, and a single Zookeeper instance to manage the nodes. We could have multiple Zookeeper instances to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that we can split our data into two shards with two replicas of each, and if one of the Solr nodes goes down we'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Our test network will be built using Ubuntu virtual machines. The first thing we're going to do is create a new directory for our test VMs on our host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. I'm going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). So, we need  to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building the four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If we run this command on each box, we should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node we currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then we restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into great detail on how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p><embed alt=\\\\\\"SolrCloud finishing screen\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"7\\\\\\"/><br/></p><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-08-31T15:57:58.430Z", "expire_at": null}	\N	23	1
148	f	2016-08-27 17:34:15.714288+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T17:24:31.166Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile in my code editor and add in the following just below the `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` line.\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
147	f	2016-08-27 17:24:31.166839+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T17:24:00.185Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"$ mkdir -p ~/solrcloud-test\\\\r\\\\n$ cd ~/solrcloud-test\\\\r\\\\n$ vagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
146	f	2016-08-27 17:24:00.185273+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T17:18:29.859Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
145	f	2016-08-27 17:18:29.859405+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T17:12:09.318Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Vagrant\\\\u00a0</p><p>\\\\u00a0</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
176	f	2016-08-31 15:59:20.830381+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T15:57:58.398Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building your four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.\\\\u00a0</p><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-08-31T15:57:58.430Z", "expire_at": null}	\N	23	1
72	f	2016-02-26 08:58:36.923591+00	{"subtitle": "Make Google happy with you by serving news content quickly", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-02-25T17:31:15.960Z", "go_live_at": null, "title": "Serving AMP content with Wagtail and RoutablePage", "seo_title": "", "listing_intro": "", "slug": "serving-amp-content-wagtail-and-routablepage", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The Accelerated Mobile Pages project is an open-source initiative to enable super-fast loading of content on mobile devices. Google are spearheading the effort, but many global news organisations are in on it as well.\\\\u00a0I decided to see if I could get Wagtail to output AMP pages as part of a general content creation workflow.</p><p>First of all, some general info/requirements for building an AMP page:</p><p></p><ul><li>AMP pages use mostly vanilla HTML with a couple of custom elements (see &lt;amp-img&gt; below)<br/></li><li>Only AMP-verified external resources are allowed to be included. So no additional JavaScript or CSS files.</li><li>There are a number of specific elements that are required on an AMP page, without which it won't be valid. <a href=\\\\\\"https://www.ampproject.org/docs/get_started/create/basic_markup.html\\\\\\">See this page for more info on this.</a></li><li>Additional CSS can be added in a single &lt;style&gt; element, but it must be in the &lt;head&gt;</li></ul><p><br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-25", "path": "00010001000A", "url_path": "/home/serving-amp-content-wagtail-and-routablepage/", "expired": false, "pk": 17, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	17	1
70	f	2016-02-25 07:55:55.486024+00	{"subtitle": "Make Google happy with you by serving news content quickly", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Serving AMP content with Wagtail and RoutablePage", "seo_title": "", "listing_intro": "", "slug": "serving-amp-content-wagtail-and-routablepage", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-25", "path": "00010001000A", "url_path": "/home/serving-amp-content-wagtail-and-routablepage/", "expired": false, "pk": 17, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	17	1
122	f	2016-03-09 14:09:21.220048+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T14:00:59.587Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen . Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing: I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>\\\\u00a0A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>\\\\u00a0Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
76	f	2016-02-28 19:41:16.053895+00	{"subtitle": "", "search_description": "Organise your day and manage your reading list better using Evernote", "owner": 1, "intro": "I'm the type of person who really needs a to-do list but is rubbish at maintaining one. I've tried many different solutions: specific apps, pen and paper, spreadsheets. None of them have lasted for longer than a couple of weeks.", "latest_revision_created_at": "2015-12-15T22:16:35.933Z", "go_live_at": null, "title": "Using Evernote as a to-do list", "seo_title": "", "listing_intro": "", "slug": "using-evernote-as-a-to-do-list", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Now though I think I've settled on a system that really works for me. I use Evernote ALL THE TIME. It's an essential tool for me. So I thought, how can I use the tool that I already use to help me organise my time.<br/></p><p>The key was to format it in an appropriate way. At the top of my list I have the 'week beginning' date. Then I have my current list of to-dos, ordered roughly by priority. I add EVERYTHING work-related that I have to do that is a discrete task, even small tasks. If I do something work-related that's not on the to-do list, I add it to the list. I use the checkbox list type, as it soothes my OCD side to see each box ticked off. Once a task has been ticked off, I copy and paste it into another list below the main list, which has today's date as the heading. So as the days go by you end up with an archive of your completed tasks for each individual day.</p><p>Visibility of the list is key. One of the biggest pains in trying to use a specific tool was that it was one more thing to have open on the screen. With Evernote, I just add it to my 'Shortcuts' list. Then it's always there in the top-left of Evernote, a tool that I would have open all the time anyway!</p><p><embed alt=\\\\\\"Lunchtime_reads.png\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"2\\\\\\"/><br/></p><p>At the end of the month, I'll archive that particular note in a separate notebook, take it out of my shortcuts, and create a new to-do note for the month.</p><p>I've now started to expand the format to include other types of to-dos. I often review relevant newsletters and blogs for content as soon as I get into work, but then I like to actually read them over lunch. So I've created a 'Lunchtime reading' list, again with tick-boxes, and a 'Reading archive' at the bottom of the to-do note.</p><p><embed alt=\\\\\\"Tasks_and_archive.png\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"3\\\\\\"/><br/></p><p>Anyway, it's a very simple solution, that appeals to me for it's 'Collect the completed tasks!' nature.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-12-14", "path": "000100010007", "url_path": "/home/using-evernote-as-a-to-do-list/", "expired": false, "pk": 14, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-12-15T22:15:02.212Z", "expire_at": null}	\N	14	1
67	f	2016-02-22 10:35:52.279984+00	{"search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Bookmarks", "seo_title": "", "slug": "bookmarks", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "numchild": 0, "content_type": 35, "show_in_menus": false, "path": "000100010008", "url_path": "/home/bookmarks/", "expired": false, "pk": 15, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	15	1
123	f	2016-03-09 14:16:46.869441+00	{"subtitle": "", "search_description": "Add an RSS feed to Wagtail using Django's out-of-the-box functionality", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T15:59:28.350Z", "go_live_at": null, "title": "Adding an RSS feed to a Wagtail site", "seo_title": "", "listing_intro": "<p>Add an RSS feed to Wagtail using Django's out-of-the-box functionality</p>", "slug": "adding-rss-feed-wagtail-site", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented an RSS feed on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at chrxr.com/blog-feed. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define three standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply import your Feed class into the\\\\u00a0<b>urls.py </b>file,\\\\u00a0\\\\u00a0then\\\\u00a0add the line below to the urlpatterns section.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from blog_feed.feeds import BlogsFeed\\\\r\\\\n\\\\r\\\\nurlpatterns = [\\\\r\\\\n    #.... lots of URLs\\\\r\\\\n\\\\r\\\\n    url(r'blog-feed/$', BlogsFeed()),\\\\r\\\\n\\\\r\\\\n   # .... more URLs\\\\r\\\\n]\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>So, when someone visits the URL http://chrxr.com/blog-feed, Wagtail initialises a new instance of the class BlogFeed, which responds to the browser with the feed of links defined in the class methods.</p><p>If you have an RSS browser extension installed (like me), this might display the links in the feed as a nice list. If not you'll likely just see a load of XML. Generally though, as long as you don't see an error, then it's working!</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 6}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>See it on GitHub</h3><p>This blog uses GitHub for version control, so the source code for anything I mention can generally <a href=\\\\\\"http://github.com/chrxr/blog_project\\\\\\">be found there</a>.</p><p></p><ul><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_feed/feeds.py#L25\\\\\\">feed.py file</a><br/></li><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_project/urls.py#L22\\\\\\">URL configuration</a></li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-04", "path": "00010001000C", "url_path": "/home/adding-rss-feed-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-04T13:16:09.072Z", "expire_at": null}	\N	19	1
95	f	2016-03-03 19:24:23.282864+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>The Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view. You can see the class for my blog RSS feed below:</p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
101	f	2016-03-03 19:59:21.050423+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T19:51:34.176Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\\\r\\\\n\\\\r\\\\n    def item_link(self, item):\\\\r\\\\n        base_url = item.get_absolute_url()\\\\r\\\\n        return base_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define four standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p><p><br/></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
102	f	2016-03-03 20:07:02.414474+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T19:59:21.050Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define four standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a get_absolute_url() method.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
198	f	2016-11-14 19:44:46.318254+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-11-14T18:35:31.767Z", "go_live_at": null, "title": "5 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "5-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>There are a number of things you can do before embarking on a project with a digital agency that will save time when the project begins, save you money over the life of the project, and make your agency love you as a client! Here are my five most important:</p><h3>1 - Establish clear lines of communications</h3><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\\\u00a0meetings\\\\u00a0with the top-stakeholders\\\\u00a0at the beginning of the process\\\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\\\\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\\\\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-11-14", "path": "00010001000H", "url_path": "/home/5-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
68	f	2016-02-22 13:41:14.842485+00	{"subtitle": "I wanted to import some bookmarks, so I wrote a little script", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Simple content import script for Django / Wagtail", "seo_title": "", "listing_intro": "", "slug": "simple-content-import-script-django-wagtail", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": 40, "tag": 2, "content_object": 16}, {"pk": 41, "tag": 47, "content_object": 16}, {"pk": 42, "tag": 7, "content_object": 16}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been collating links as part of my <a id=\\\\\\"14\\\\\\" linktype=\\\\\\"page\\\\\\">reading list</a>\\\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that find individual ones has become difficult.</p><p>So! I created a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\\\\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\\\\\">quick python script</a> to import the bookmarks from a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\\\\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-22", "path": "000100010009", "url_path": "/home/simple-content-import-script-django-wagtail/", "expired": false, "pk": 16, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	16	1
99	f	2016-03-03 19:48:28.177131+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T19:43:52.800Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\\\r\\\\n\\\\r\\\\n    def item_link(self, item):\\\\r\\\\n        base_url = item.get_absolute_url()\\\\r\\\\n        return base_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
124	f	2016-03-09 14:18:05.2226+00	{"subtitle": "", "search_description": "Add an RSS feed to Wagtail using Django's out-of-the-box functionality", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T14:16:46.869Z", "go_live_at": null, "title": "Adding an RSS feed to a Wagtail site", "seo_title": "", "listing_intro": "<p>Add an RSS feed to Wagtail using Django's out-of-the-box functionality</p>", "slug": "adding-rss-feed-wagtail-site", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented an RSS feed on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define three standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply import your Feed class into the\\\\u00a0<b>urls.py </b>file,\\\\u00a0\\\\u00a0then\\\\u00a0add the line below to the urlpatterns section.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from blog_feed.feeds import BlogsFeed\\\\r\\\\n\\\\r\\\\nurlpatterns = [\\\\r\\\\n    #.... lots of URLs\\\\r\\\\n\\\\r\\\\n    url(r'blog-feed/$', BlogsFeed()),\\\\r\\\\n\\\\r\\\\n   # .... more URLs\\\\r\\\\n]\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>So, when someone visits the URL http://chrxr.com/blog-feed, Wagtail initialises a new instance of the class BlogFeed, which responds to the browser with the feed of links defined in the class methods.</p><p>If you have an RSS browser extension installed (like me), this might display the links in the feed as a nice list. If not you'll likely just see a load of XML. Generally though, as long as you don't see an error, then it's working!</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 6}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>See it on GitHub</h3><p>This blog uses GitHub for version control, so the source code for anything I mention can generally <a href=\\\\\\"http://github.com/chrxr/blog_project\\\\\\">be found there</a>.</p><p></p><ul><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_feed/feeds.py#L25\\\\\\">feed.py file</a><br/></li><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_project/urls.py#L22\\\\\\">URL configuration</a></li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-04", "path": "00010001000C", "url_path": "/home/adding-rss-feed-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-04T13:16:09.072Z", "expire_at": null}	\N	19	1
97	f	2016-03-03 19:31:12.737185+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T19:25:04.021Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty __init__.py file</li><li>A file called forms.py, which will contain our code</li></ul><p></p><p>\\\\u00a0You can see the class for my blog RSS feed below:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\\\r\\\\n\\\\r\\\\n    def item_link(self, item):\\\\r\\\\n        base_url = item.get_absolute_url()\\\\r\\\\n        return base_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that. We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
125	f	2016-03-09 14:56:08.018544+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "To Prince2 or not to Prince2", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-09T14:09:21.220Z", "go_live_at": null, "title": "Is it worth doing a project management qualification?", "seo_title": "", "listing_intro": "<p>To Prince2 or not to Prince2</p>", "slug": "is-it-worth-doing-project-management-qualification", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 46, "content_object": 20}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working with the job title of 'project manager\\\\u2019 for over three years, and running projects in other roles for over five years. In that time I've always delivered on my projects. Very occasionally they have been late, or over budget; these things sometimes happen. Still, somehow I\\\\u2019ve managed to not just get by, but actually progress to more senior project manager roles, all without a printed certificate framed on my desk.</p><p>But here's the thing: I'm sure I would have done better if I had done some kind of project management qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p>Over time I've learned discipline through experiencing the pain that comes when that vital skill is lacking. This pain usually manifests itself as stress, as you react to situations for which you had not adequately planned.</p><p>You can learn how to do Scrum, or any of the other brands of PM, without being a certified practitioner. \\\\u00a0Let's face it: It's not that hard!</p><p>But having the discipline to enforce the rules and ceremonies that make Scrum or Prince2 effective frameworks is not something that comes naturally to many people. Investing time in studying and learning, and having that knowledge tested, enshrines that core element of discipline as a central tenet of your practice.</p><h2>A note for employers</h2><p>\\\\u00a0A PM QUALIFICATION SHOULD NOT BE A REQUIREMENT FOR GETTING A PM JOB.</p><p>\\\\u00a0Not an entry level one anyway. Discipline is just one of my four pillars of good project management*. \\\\u00a0Whilst discipline is something that can be enforced with training and checklists, open-mindedness, communication skills and people management skills are much harder to develop from scratch. My advice to employers would be to find someone with these traits, then send them to do qualifications if necessary. You'll very likely find that this investment in an employees future will generate a sense of loyalty towards your organisation, resulting in retention of talented and skilled staff.</p><p><br/></p><p>* My four pillars of good project management are:<br/></p><p></p><ul><li>Discipline</li><li>Open-mindedness / willingness to learn</li><li>Customer relationship management / communication skills</li><li>People management skills</li></ul><p>More about these in another post.<br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-09", "path": "00010001000D", "url_path": "/home/is-it-worth-doing-project-management-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-03-09T14:09:21.243Z", "expire_at": null}	\N	20	1
114	f	2016-03-09 12:47:41.560404+00	{"subtitle": "To Prince2 or not to Prince2", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Is it worth doing a PM qualification?", "seo_title": "", "listing_intro": "", "slug": "it-worth-doing-pm-qualification", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I have no official project management qualifications. I have been working officially as a project manager for over three years, and have been running projects in other roles for over five years. In that time I've never had a project catastrophically fail, or be cancelled. I've had projects go over budget, I've delivered projects late, but I've always successfully delivered.</p><p>But here's the thing: I'm pretty sure I would have done better if I had done some kind of PM qualification at the beginning of my career.</p><p>I don't even think it matters what qualification or certification you gain. Prince2, Agile, Scrum; they all teach something that otherwise takes a long time to develop naturally: discipline.</p><p><br/></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000D", "url_path": "/home/it-worth-doing-pm-qualification/", "expired": false, "pk": 20, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	20	1
44	f	2015-07-29 21:53:49.255026+00	{"subtitle": "For when Heroku settings break your local build", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-29T21:52:51.700Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "", "slug": "separating-local-and-heroku-database-settings-django", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 9, "content_object": 9}, {"pk": null, "tag": 5, "content_object": 9}, {"pk": null, "tag": 7, "content_object": 9}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\\\r\\\\nis improperly configured. \\\\r\\\\nPlease supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to settings.py to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-07-29T21:46:27.842Z", "expire_at": null}	\N	9	1
86	f	2016-03-01 12:51:11.402093+00	{"subtitle": "... and communicate them to your team", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-01T12:47:53.210Z", "go_live_at": null, "title": "PMs: Know your business objectives", "seo_title": "", "listing_intro": "", "slug": "pms-know-your-business-objectives", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across an article\\\\u00a0by designer Joshua Taylor, on Medium, title \\\\\\"<a href=\\\\\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\\\\\">Designers shouldn\\\\u2019t code. They should study business.</a>\\\\\\"\\\\u00a0</p><p>The basic premise is a designer's time is better spent considering how best to translate business aims into designs, rather than designs into production ready code. Now I agree with this as a general statement, but I don't think it is the designers job to\\\\u00a0</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-01", "path": "00010001000B", "url_path": "/home/pms-know-your-business-objectives/", "expired": false, "pk": 18, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	18	1
199	f	2016-11-14 20:16:34.913156+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "Here are my five most effective ways that you can save time and money when employing a digital agency on a project", "latest_revision_created_at": "2016-11-14T19:44:46.318Z", "go_live_at": null, "title": "5 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "5-ways-get-best-out-your-digital-agency", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>1 - Establish clear lines of communications</h3><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\\\u00a0meetings\\\\u00a0with the top-stakeholders\\\\u00a0at the beginning of the process\\\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\\\\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\\\\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-11-14", "path": "00010001000H", "url_path": "/home/5-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-11-14T19:44:46.341Z", "expire_at": null}	\N	24	1
178	f	2016-09-01 07:08:35.680111+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "Easy steps to emulate a multi-machine setup locally", "owner": 1, "intro": "", "latest_revision_created_at": "2016-09-01T07:07:46.767Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "slug": "testing-solrcloud-vagrant", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building your four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.\\\\u00a0</p><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-08-31T15:57:58.430Z", "expire_at": null}	\N	23	1
45	f	2015-07-29 21:54:06.896783+00	{"subtitle": "For when Heroku settings break your local build", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-29T21:53:49.255Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "<p>For when Heroku settings break your local build</p>", "slug": "separating-local-and-heroku-database-settings-django", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 9, "content_object": 9}, {"pk": null, "tag": 5, "content_object": 9}, {"pk": null, "tag": 7, "content_object": 9}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\\\r\\\\nis improperly configured. \\\\r\\\\nPlease supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to settings.py to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-07-29T21:46:27.842Z", "expire_at": null}	\N	9	1
87	f	2016-03-01 13:01:15.074391+00	{"subtitle": "... and communicate them to your team", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-01T12:51:11.402Z", "go_live_at": null, "title": "PMs: Know your business objectives", "seo_title": "", "listing_intro": "", "slug": "pms-know-your-business-objectives", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across an article\\\\u00a0by designer Joshua Taylor, on Medium, title \\\\\\"<a href=\\\\\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\\\\\">Designers shouldn\\\\u2019t code. They should study business.</a>\\\\\\"\\\\u00a0</p><p>The basic premise is a designer's time is better spent considering how best to translate business aims into designs, rather than designs into production ready code. Now I agree with this as a general statement. Everyone on a project team should know what the business aims are for their company / client. I don't think, though, it is the job of the designer or the developer to find these out for themselves.</p><h2>Step 0: What are the business objectives?</h2><p>A project cannot hope to fully succeed without some kind of influence and direction from the company / client's business objectives. Now often I've found in projects that some of the business objectives are obvious, and so make their way into the project without explicitly having to discover or define them.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-01", "path": "00010001000B", "url_path": "/home/pms-know-your-business-objectives/", "expired": false, "pk": 18, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	18	1
103	f	2016-03-04 12:57:14.132366+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T20:07:02.414Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define four standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
89	f	2016-03-01 13:04:05.772853+00	{"subtitle": "... and communicate them to your team", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-01T13:03:10.028Z", "go_live_at": null, "title": "PMs: Know your business objectives", "seo_title": "", "listing_intro": "", "slug": "pms-know-your-business-objectives", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across an article\\\\u00a0by designer Joshua Taylor, on Medium, title \\\\\\"<a href=\\\\\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\\\\\">Designers shouldn\\\\u2019t code. They should study business.</a>\\\\\\"\\\\u00a0</p><p>The basic premise is a designer's time is better spent considering how best to translate business aims into designs, rather than designs into production ready code. Now I agree with this as a general statement. Everyone on a project team should know what the business aims are for their company / client. I don't think, though, it is the job of the designer or the developer to find these out for themselves.</p><h2>Step 0: What are the business objectives?</h2><p>A project cannot hope to fully succeed without some kind of influence and direction from the company / client's business objectives. \\\\u00a0Often I've found in projects that some of the business objectives are obvious, and so make their way into the project without explicitly having to discover or define them. For example, a campaigning charity would want to communicate their campaign messages effectively and elicit donations. But their are nuances and priorities that are almost always missed when business objectives are not explicitly defined with the client.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-01", "path": "00010001000B", "url_path": "/home/pms-know-your-business-objectives/", "expired": false, "pk": 18, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	18	1
23	f	2015-06-29 19:56:08.632577+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-29T19:54:00.577Z", "go_live_at": null, "title": "Super simple Django error logging configuration for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "", "slug": "django-error-logging-configuration-heroku", "live": false, "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 5, "content_object": 7}, {"pk": null, "tag": 6, "content_object": 7}, {"pk": null, "tag": 7, "content_object": 7}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nLOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nheroku logs --source app --tail\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>For more information on Django logging <a href=\\\\\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\\\\\">see the documentation</a>.</p><p>For more information on Heroku logging <a href=\\\\\\"https://devcenter.heroku.com/articles/logging\\\\\\">see their support centre</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	7	1
88	f	2016-03-01 13:03:10.028031+00	{"subtitle": "... and communicate them to your team", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-01T13:01:15.074Z", "go_live_at": null, "title": "PMs: Know your business objectives", "seo_title": "", "listing_intro": "", "slug": "pms-know-your-business-objectives", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across an article\\\\u00a0by designer Joshua Taylor, on Medium, title \\\\\\"<a href=\\\\\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\\\\\">Designers shouldn\\\\u2019t code. They should study business.</a>\\\\\\"\\\\u00a0</p><p>The basic premise is a designer's time is better spent considering how best to translate business aims into designs, rather than designs into production ready code. Now I agree with this as a general statement. Everyone on a project team should know what the business aims are for their company / client. I don't think, though, it is the job of the designer or the developer to find these out for themselves.</p><h2>Step 0: What are the business objectives?</h2><p>A project cannot hope to fully succeed without some kind of influence and direction from the company / client's business objectives. Now often I've found in projects that some of the business objectives are obvious, and so make their way into the project without explicitly having to discover or define them. For example, a campaigning charity would want to communicate their campaign messages effectively and elicit donations. But their are nuances and priorities that are almost always missed when business objectives are not explicitly defined with the client.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-01", "path": "00010001000B", "url_path": "/home/pms-know-your-business-objectives/", "expired": false, "pk": 18, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	18	1
104	f	2016-03-04 13:02:17.428348+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T12:57:14.132Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define four standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply add the line below to the urlpatterns section of the\\\\u00a0<b>urls.py </b>file.\\\\u00a0</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    url(r'blog-feed/$', BlogsFeed()),\\", \\"language\\": \\"python\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
105	f	2016-03-04 13:05:07.922495+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T13:02:17.428Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define four standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply import your Feed class into the\\\\u00a0<b>urls.py </b>file,\\\\u00a0\\\\u00a0then\\\\u00a0add the line below to the urlpatterns section.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from blog_feed.feeds import BlogsFeed, BlogsFeedAmp\\\\r\\\\n\\\\r\\\\nurlpatterns = [\\\\r\\\\n    #.... lots of URLs\\\\r\\\\n\\\\r\\\\n    url(r'blog-feed/$', BlogsFeed()),\\\\r\\\\n\\\\r\\\\n   # .... more URLs\\\\r\\\\n]\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>So, when someone visits the URL http://chrxr.com/blog-feed, Wagtail\\\\u00a0</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
31	f	2015-07-07 21:38:41.045281+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-29T19:58:23.849Z", "go_live_at": null, "title": "Super simple Django error logging configuration, good for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "<p>A simple code block to output Django error messages to the console</p>", "slug": "django-error-logging-configuration-heroku", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 5, "content_object": 7}, {"pk": null, "tag": 6, "content_object": 7}, {"pk": null, "tag": 7, "content_object": 7}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"LOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"heroku logs --source app --tail\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>For more information on Django logging <a href=\\\\\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\\\\\">see the documentation</a>.</p><p>For more information on Heroku logging <a href=\\\\\\"https://devcenter.heroku.com/articles/logging\\\\\\">see their support centre</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-29", "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-29T19:56:08.690Z", "expire_at": null}	\N	7	1
90	f	2016-03-01 14:38:14.547373+00	{"subtitle": "... and communicate them to your team", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-01T13:04:05.772Z", "go_live_at": null, "title": "PMs: Know your business objectives", "seo_title": "", "listing_intro": "", "slug": "pms-know-your-business-objectives", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across an article\\\\u00a0by designer Joshua Taylor, on Medium, title \\\\\\"<a href=\\\\\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\\\\\">Designers shouldn\\\\u2019t code. They should study business.</a>\\\\\\"\\\\u00a0</p><p>The basic premise is a designer's time is better spent considering how best to translate business aims into designs, rather than designs into production ready code. Now I agree with this as a general statement. Everyone on a project team should know what the business aims are for their company / client. I don't think, though, it is the job of the designer or the developer to find these out for themselves.</p><h2>Step 0: What are the business objectives?</h2><p>A project cannot hope to fully succeed without some kind of influence and direction from the company / client's business objectives. \\\\u00a0Often I've found in projects that some of the business objectives are obvious, and so make their way into the project without explicitly having to discover or define them. For example, a campaigning charity would want to communicate their campaign messages effectively and elicit donations. But their are nuances and priorities that are almost always missed when business objectives are not explicitly defined with the client.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-01", "path": "00010001000B", "url_path": "/home/pms-know-your-business-objectives/", "expired": false, "pk": 18, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	18	1
106	f	2016-03-04 13:14:19.357305+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T13:05:07.922Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented an RSS feed on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define four standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply import your Feed class into the\\\\u00a0<b>urls.py </b>file,\\\\u00a0\\\\u00a0then\\\\u00a0add the line below to the urlpatterns section.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from blog_feed.feeds import BlogsFeed\\\\r\\\\n\\\\r\\\\nurlpatterns = [\\\\r\\\\n    #.... lots of URLs\\\\r\\\\n\\\\r\\\\n    url(r'blog-feed/$', BlogsFeed()),\\\\r\\\\n\\\\r\\\\n   # .... more URLs\\\\r\\\\n]\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>So, when someone visits the URL http://chrxr.com/blog-feed, Wagtail initialises a new instance of the class BlogFeed, which responds to the browser with the feed of links defined in the class methods.</p><p>If you have an RSS browser extension installed (like me), this might display the links in the feed as a nice list. If not you'll likely just see a load of XML. Generally though, as long as you don't see an error, then it's working!</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 6}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>See it on GitHub</h3><p>This blog uses GitHub for version control, so the source code for anything I mention can generally <a href=\\\\\\"http://github.com/chrxr/blog_project\\\\\\">be found there</a>.</p><p><ul><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_feed/feeds.py#L25\\\\\\">feed.py file</a><br/></li><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_project/urls.py#L22\\\\\\">URL configuration</a></li></ul></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
91	f	2016-03-02 12:16:25.328682+00	{"subtitle": "... and communicate them to your team", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-01T14:38:14.547Z", "go_live_at": null, "title": "PMs: Know your business objectives", "seo_title": "", "listing_intro": "", "slug": "pms-know-your-business-objectives", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across an article\\\\u00a0by designer Joshua Taylor, on Medium, title \\\\\\"<a href=\\\\\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\\\\\">Designers shouldn\\\\u2019t code. They should study business.</a>\\\\\\"\\\\u00a0</p><p>The basic premise is a designer's time is better spent considering how best to translate business aims into designs, rather than designs into production ready code. Now I agree with this as a general statement. Everyone on a project team should know what the business aims are for their company / client. I don't think, though, it is the job of the designer or the developer to find these out for themselves.</p><h2>Step 0: What are the business objectives?</h2><p>A project cannot hope to fully succeed without some kind of influence and direction from the company / client's business objectives. \\\\u00a0Often I've found in projects that some of the business objectives are obvious, and so make their way into the project without explicitly having to discover or define them. For example, a campaigning charity would want to communicate their campaign messages effectively and elicit donations. But their are nuances and priorities that are almost always missed when business objectives are not explicitly defined with the client.</p><h4>Good example:</h4><p></p><ul><li>Project begins with discussions regarding KPIs</li><li>Quickly uncover a lack of quality business objectives</li><li>Client confers with management to define objectives, resulting in buy-in from management and a clear direction for the project</li><li>This is communicated effectively to UX specialist, designers and developers, so everyone is working from the same playbook.</li></ul><h4>Bad example:</h4><p><br/></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-01", "path": "00010001000B", "url_path": "/home/pms-know-your-business-objectives/", "expired": false, "pk": 18, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	18	1
107	f	2016-03-04 13:16:09.033998+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T13:14:19.357Z", "go_live_at": null, "title": "Adding an RSS feed to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feed-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented an RSS feed on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty<b> __init__.py</b> file</li><li>A file called<b> forms.py</b>, which will contain our code</li></ul><p>I then added the app 'blog_feed' to the INSTALLED_APPS section of my <b>base.py </b>settings file.</p><p></p><p>With that done, back to the <b>forms.py</b> file. Below you can see the final code for my first RSS feed:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that.\\\\u00a0</p><p>We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p><p>Then we create the class itself. As you can see in the first line, we subclass\\\\u00a0django.contrib.syndication.views.Feed by including it in the class definition.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"class BlogsFeed(Feed):\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next three lines define the standard <b>title, link </b>and <b>description</b>\\\\u00a0values that begin all RSS feeds. You can see them in the output at <a href=\\\\\\"http://chrxr.com/blog-feed\\\\\\">chrxr.com/blog-feed</a>. These are required!</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"<title>My blog articles</title>\\\\r\\\\n<link>https://chrxr.com/blogs-feed/</link>\\\\r\\\\n<description>All of my blogs as they are published</description>\\", \\"language\\": \\"html\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Following that we define four standard methods for our Feed class. Again, all four of these are required to provide all the information necessary to build the feed.</p><p>The first method, <b>items</b>, gets the objects (pages) which we're going to list in the RSS feed from the database.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The next two methods retrieve specific field information from each individual object. The first is populating the title for the link in the RSS feed. Usually this will be <b>self.title</b>, but it might be <b>self.seo_title</b>, or you may have another field on your blog model that you want to use.</p><p>The second method gets the description to go along with the link. This is more open to interpretation. I've used the <b>self.intro</b> field as it is often populated.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>Defining the links</h3><p>There's one final ingredient missing from this RSS pie, the actual links to the articles. Django's Feeds module has the power to extract these links itself, but only if the model listed in the RSS feed has a <b>get_absolute_url()</b> method.</p><p>This isn't a big problem though. I simply added the following line to my BlogPost model:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    def get_absolute_url(self):\\\\r\\\\n        return self.full_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! The code for generating the feed is complete. Now onto the task of defining a URL to access the feed.</p><h3>Adding the RSS feed URL</h3><p>This bit is easy. You simply import your Feed class into the\\\\u00a0<b>urls.py </b>file,\\\\u00a0\\\\u00a0then\\\\u00a0add the line below to the urlpatterns section.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from blog_feed.feeds import BlogsFeed\\\\r\\\\n\\\\r\\\\nurlpatterns = [\\\\r\\\\n    #.... lots of URLs\\\\r\\\\n\\\\r\\\\n    url(r'blog-feed/$', BlogsFeed()),\\\\r\\\\n\\\\r\\\\n   # .... more URLs\\\\r\\\\n]\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>So, when someone visits the URL http://chrxr.com/blog-feed, Wagtail initialises a new instance of the class BlogFeed, which responds to the browser with the feed of links defined in the class methods.</p><p>If you have an RSS browser extension installed (like me), this might display the links in the feed as a nice list. If not you'll likely just see a load of XML. Generally though, as long as you don't see an error, then it's working!</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 6}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>See it on GitHub</h3><p>This blog uses GitHub for version control, so the source code for anything I mention can generally <a href=\\\\\\"http://github.com/chrxr/blog_project\\\\\\">be found there</a>.</p><p></p><ul><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_feed/feeds.py#L25\\\\\\">feed.py file</a><br/></li><li><a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog_project/urls.py#L22\\\\\\">URL configuration</a></li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-04", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
25	f	2015-06-29 19:57:52.657357+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-29T19:56:27.083Z", "go_live_at": null, "title": "Super simple Django error logging configuration for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "<p>A simple code block to output Django error messages to the console</p>", "slug": "django-error-logging-configuration-heroku", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 5, "content_object": 7}, {"pk": null, "tag": 6, "content_object": 7}, {"pk": null, "tag": 7, "content_object": 7}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nLOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nheroku logs --source app --tail\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>For more information on Django logging <a href=\\\\\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\\\\\">see the documentation</a>.</p><p>For more information on Heroku logging <a href=\\\\\\"https://devcenter.heroku.com/articles/logging\\\\\\">see their support centre</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-29", "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-29T19:56:08.690Z", "expire_at": null}	\N	7	1
33	f	2015-07-11 10:09:28.015411+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-11T10:08:31.992Z", "go_live_at": null, "title": "Super simple Django error logging configuration, good for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "<p>A simple code block to output Django error messages to the console</p>", "slug": "django-error-logging-configuration-heroku", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 5, "content_object": 7}, {"pk": null, "tag": 6, "content_object": 7}, {"pk": null, "tag": 7, "content_object": 7}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"LOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"heroku logs --source app --tail\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3><hr/></h3><h3>What's going on here?</h3><p>We'll break this down into each individual section:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"'version': 1\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Identifies the format of the logging dictionary. Currently there is only 1 version available, but there could be more in the future.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"'disable_existing_loggers': False,\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This indicates that we shouldn't disable the default logging configuration. The default is 'True', but it's not recommended to do this, as the default logs can be useful. Instead we will keep the default logs and redefine certain elements of them to output logs to the console.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>There are four elements of a logging configuration:</p><p></p><ul><li>Loggers</li><li>Handlers</li><li>Filters</li><li>Formatters</li></ul><p>For our configuration we are only interested in the first two. The code above sets up our logging handler. We are declaring the console as a logging stream handler.</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then finally we set up the logger itself, instruct it to utilise the 'console' handler, and set the minimum level of logging to 'ERROR'. This way we only see when things are actually wrong with our application, rather than logging all messages coming out of Django (the 'DEBUG' logging level).</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>For more information on Django logging\\\\u00a0<a href=\\\\\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\\\\\">see the documentation</a>.</p><p>For more information on Heroku logging\\\\u00a0<a href=\\\\\\"https://devcenter.heroku.com/articles/logging\\\\\\">see their support centre</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-29", "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-29T19:56:08.690Z", "expire_at": null}	\N	7	1
32	f	2015-07-11 10:08:31.992423+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-07T21:38:41.045Z", "go_live_at": null, "title": "Super simple Django error logging configuration, good for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "<p>A simple code block to output Django error messages to the console</p>", "slug": "django-error-logging-configuration-heroku", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 5, "content_object": 7}, {"pk": null, "tag": 6, "content_object": 7}, {"pk": null, "tag": 7, "content_object": 7}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"LOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"heroku logs --source app --tail\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3>What's going on here?</h3><p>We'll break this down into each individual line:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"'version': 1\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Identifies the format of the logging dictionary. Currently there is only 1 version available, but there could be more in the future.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"'disable_existing_loggers': False,\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This indicates that we shouldn't disable the default logging configuration. The default is 'True', but it's not recommended to do this, as the default logs can be useful. Instead we will keep the default logs and redefine certain elements of them to output logs to the console.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>There are four elements of a logging configuration:</p><p></p><ul><li>Loggers</li><li>Handlers</li><li>Filters</li><li>Formatters</li></ul><p>For our configuration we are only interested in the first two. The code above sets up our logging handler. We are declaring the console as a logging stream handler.</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then finally we set up the logger itself, instruct it to utilise the 'console' handler, and set the minimum level of logging to 'ERROR'. This way we only see when things are actually wrong with our application, rather than logging all messages coming out of Django (the 'DEBUG' logging level).</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>For more information on Django logging\\\\u00a0<a href=\\\\\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\\\\\">see the documentation</a>.</p><p>For more information on Heroku logging\\\\u00a0<a href=\\\\\\"https://devcenter.heroku.com/articles/logging\\\\\\">see their support centre</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-29", "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-29T19:56:08.690Z", "expire_at": null}	\N	7	1
92	f	2016-03-02 13:17:39.68081+00	{"subtitle": "... and communicate them to your team", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-02T12:16:25.328Z", "go_live_at": null, "title": "PMs: Know your business objectives", "seo_title": "", "listing_intro": "", "slug": "pms-know-your-business-objectives", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Whilst doing my daily rounds of the various UX feeds / blogs / newsletters that I follow, I came across an article\\\\u00a0by designer Joshua Taylor, on Medium, title \\\\\\"<a href=\\\\\\"https://medium.com/@joshuantaylor/designers-shouldn-t-code-they-should-study-business-dc3e7e203d39#.dr0mdq20v\\\\\\">Designers shouldn\\\\u2019t code. They should study business.</a>\\\\\\"\\\\u00a0</p><p>The basic premise is a designer's time is better spent considering how best to translate business aims into designs, rather than designs into production ready code. Now I agree with this as a general statement. Everyone on a project team should know what the business aims are for their company / client. I don't think, though, it is the job of the designer or the developer to find these out for themselves.</p><h2>Step 0: What are the business objectives?</h2><p>A project cannot hope to fully succeed without some kind of influence and direction from the company / client's business objectives. \\\\u00a0Often I've found in projects that some of the business objectives are obvious, and so make their way into the project without explicitly having to discover or define them. For example, a campaigning charity would want to communicate their campaign messages effectively and elicit donations. But their are nuances and priorities that are almost always missed when business objectives are not explicitly defined with the client.</p><h4>Good example:</h4><p></p><ul><li>Project begins with discussions regarding KPIs</li><li>Quickly uncover a lack of quality business objectives</li><li>Client confers with management to define objectives, resulting in buy-in from management and a clear direction for the project</li><li>This is communicated effectively to UX specialist, designers and developers, so everyone is working from the same playbook.</li></ul><h4>Bad example:</h4><p></p><ul><li>Project begins with assumed business objectives</li><li>KPIs are based on these</li><li>Not adequately communicated with the rest of team</li><li>Design direction starts off on the wrong path and continues to diverge away from goals</li><li>Client unhappy, more money needs to be spent on course correction.</li><li>Sad faces all round</li></ul><h2>How do you work out business objectives?</h2><p><ul><li>Existing documentation</li><li>Talk to the bosses (+ gets buy-in)</li><li>Specific exercises to draw out business objectives if they are not yet formally documented</li></ul></p><p></p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-01", "path": "00010001000B", "url_path": "/home/pms-know-your-business-objectives/", "expired": false, "pk": 18, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	18	1
21	f	2015-06-29 19:53:44.119689+00	{"subtitle": "", "search_description": "A simple script to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-29T19:52:20.351Z", "go_live_at": null, "title": "Super simple Django error logging configuration for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "", "slug": "django-error-logging-configuration-heroku", "live": false, "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nLOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nheroku logs --source app --tail\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "000100010003", "url_path": "/home/super-simple-django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	7	1
50	f	2015-12-07 22:09:47.089604+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Using Evernote as a to-do list", "seo_title": "", "listing_intro": "", "slug": "using-evernote-do-list", "live": false, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm the type of person who really needs a to-do list but is rubbish at maintaining one. I've tried many different solutions: specific apps, pen and paper, spreadsheets. None of them have lasted for longer than a couple of weeks.</p><p>Now though I think I've settled on a system that really works for me. I use Evernote ALL THE TIME. It's an essential tool for me. So I thought, how can I use the tool that I already use to help me organise my time.</p><p>The key was to format it in an appropriate way. At the top of my list I have the 'week beginning' date. Then I have my current list of to-dos, ordered roughly by priority. I add EVERYTHING work-related that I have to do that is a discrete task, even small tasks. If I do something work-related that's not on the to-do list, I add it to the list. I use the checkbox list type, as it soothes my OCD side to see each box ticked off. Once a task has been ticked off, I copy and paste it into another list below the main list, which has today's date as the heading. So as the days go by you end up with an archive of your completed tasks for each individual day.</p><p>Visibility of the list is key. One of the biggest pains in trying to use a specific tool was that it was one more thing to have open on the screen. With Evernote, I just add it to my 'Shortcuts' list. Then it's always there in the top-left of Evernote, a tool that I would have open all the time anyway!</p><p>At the end of the month, I'll archive that particular note in a separate notebook, take it out of my shortcuts, and create a new to-do note for the month.</p><p>I've now started to expand the format to include other types of to-dos. I often review relevant newsletters and blogs for content as soon as I get into work, but then I like to actually read them over lunch. So I've created a 'Lunchtime reading' list, again with tick-boxes, and a 'Reading archive' at the bottom of the to-do note.</p><p>Anyway, it's a very simple solution, that appeals to me for it's 'Collect the completed tasks!' nature.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-12-07", "path": "000100010007", "url_path": "/home/using-evernote-do-list/", "expired": false, "pk": 14, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	14	1
51	f	2015-12-15 22:15:02.044079+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-12-07T22:09:47.089Z", "go_live_at": null, "title": "Using Evernote as a to-do list", "seo_title": "", "listing_intro": "", "slug": "using-evernote-do-list", "live": false, "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm the type of person who really needs a to-do list but is rubbish at maintaining one. I've tried many different solutions: specific apps, pen and paper, spreadsheets. None of them have lasted for longer than a couple of weeks.</p><p>Now though I think I've settled on a system that really works for me. I use Evernote ALL THE TIME. It's an essential tool for me. So I thought, how can I use the tool that I already use to help me organise my time.</p><p>The key was to format it in an appropriate way. At the top of my list I have the 'week beginning' date. Then I have my current list of to-dos, ordered roughly by priority. I add EVERYTHING work-related that I have to do that is a discrete task, even small tasks. If I do something work-related that's not on the to-do list, I add it to the list. I use the checkbox list type, as it soothes my OCD side to see each box ticked off. Once a task has been ticked off, I copy and paste it into another list below the main list, which has today's date as the heading. So as the days go by you end up with an archive of your completed tasks for each individual day.</p><p>Visibility of the list is key. One of the biggest pains in trying to use a specific tool was that it was one more thing to have open on the screen. With Evernote, I just add it to my 'Shortcuts' list. Then it's always there in the top-left of Evernote, a tool that I would have open all the time anyway!</p><p><embed alt=\\\\\\"Lunchtime_reads.png\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"2\\\\\\"/><br/></p><p>At the end of the month, I'll archive that particular note in a separate notebook, take it out of my shortcuts, and create a new to-do note for the month.</p><p>I've now started to expand the format to include other types of to-dos. I often review relevant newsletters and blogs for content as soon as I get into work, but then I like to actually read them over lunch. So I've created a 'Lunchtime reading' list, again with tick-boxes, and a 'Reading archive' at the bottom of the to-do note.</p><p><embed alt=\\\\\\"Tasks_and_archive.png\\\\\\" embedtype=\\\\\\"image\\\\\\" format=\\\\\\"fullwidth\\\\\\" id=\\\\\\"3\\\\\\"/><br/></p><p>Anyway, it's a very simple solution, that appeals to me for it's 'Collect the completed tasks!' nature.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-12-07", "path": "000100010007", "url_path": "/home/using-evernote-do-list/", "expired": false, "pk": 14, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	14	1
83	f	2016-02-29 09:39:14.582726+00	{"subtitle": "I wanted to import some bookmarks, so I wrote a little script", "search_description": "A short script that imports content from a CSV file into Django / Wagtail CMS", "owner": 1, "intro": "", "latest_revision_created_at": "2016-02-28T19:40:47.067Z", "go_live_at": null, "title": "Simple content import script for Django / Wagtail", "seo_title": "", "listing_intro": "", "slug": "simple-content-import-script-django-wagtail", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 2, "content_object": 16}, {"pk": null, "tag": 47, "content_object": 16}, {"pk": null, "tag": 7, "content_object": 16}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been collating links as part of my <a id=\\\\\\"14\\\\\\" linktype=\\\\\\"page\\\\\\">reading list</a>\\\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that finding individual ones has become difficult.</p><p>So! I created a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\\\\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\\\\\">quick python script</a> to import the bookmarks from a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\\\\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-22", "path": "000100010009", "url_path": "/home/simple-content-import-script-django-wagtail/", "expired": false, "pk": 16, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-02-22T13:41:14.867Z", "expire_at": null}	\N	16	1
188	f	2016-10-23 18:57:14.970919+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-10-22T17:58:11.482Z", "go_live_at": null, "title": "10 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "10-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>When working with a digital agency, there are a number of things you can do to get the best out of them. By doing these things you can make the your life easier, their life easier, and save a bunch of money as well.</p><h3>Establish clear lines of communications</h3><p>Turning around agency queries quickly with clear decisions can have the biggest direct, positive impact on the overall cost of your project of any of these suggestions.</p><p>There should be one main point of contact in your organisation, who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>Come prepared with buy-in from the top</h3><h3>Gather all the user information that you have</h3><h3>Focus on the discovery phase</h3><h3>What are your business goals?</h3><h3>Start tracking things as soon as possible</h3><h3>Got brand guidelines? Hand'em over!</h3>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/10-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
93	f	2016-03-03 13:06:25.833133+00	{"subtitle": "I wanted to import some bookmarks, so I wrote a little script", "search_description": "A short script that imports content from a CSV file into Django / Wagtail CMS", "owner": 1, "intro": "", "latest_revision_created_at": "2016-02-29T09:39:17.900Z", "go_live_at": null, "title": "Simple content import script for Django / Wagtail", "seo_title": "", "listing_intro": "", "slug": "simple-content-import-script-django-wagtail", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 2, "content_object": 16}, {"pk": null, "tag": 47, "content_object": 16}, {"pk": null, "tag": 7, "content_object": 16}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been collating links as part of my <a id=\\\\\\"14\\\\\\" linktype=\\\\\\"page\\\\\\">reading list</a>\\\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that finding individual ones has become difficult.</p><p>So! I created a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\\\\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\\\\\">quick python script</a> to import the bookmarks from a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\\\\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p><p>By the way, you can now follow my reading <a id=\\\\\\"15\\\\\\" linktype=\\\\\\"page\\\\\\">list here</a>!</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-22", "path": "000100010009", "url_path": "/home/simple-content-import-script-django-wagtail/", "expired": false, "pk": 16, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-02-22T13:41:14.867Z", "expire_at": null}	\N	16	1
187	f	2016-10-23 18:42:56.796359+00	{"subtitle": "Explorations and ramblings of Chris Rogers, a technical project manager and developer", "search_description": "Explorations and ramblings of Chris Rogers, a digital project manager and hobbyist developer", "owner": null, "latest_revision_created_at": "2016-04-01T12:22:13.404Z", "go_live_at": null, "title": "chrxr.com", "seo_title": "chrxr.com | Digital project management and web development", "slug": "home", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "numchild": 16, "content_type": 3, "show_in_menus": false, "path": "00010001", "url_path": "/home/", "expired": false, "pk": 3, "locked": false, "depth": 2, "first_published_at": "2015-06-25T12:11:06.703Z", "expire_at": null}	\N	3	1
77	f	2016-02-28 19:43:37.946682+00	{"subtitle": "For when Heroku settings break your local build", "search_description": "Stop Heroku database settings from breaking your local Django build", "owner": 1, "intro": "", "latest_revision_created_at": "2015-09-27T16:36:27.771Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "<p>For when Heroku settings break your local build</p>", "slug": "separating-local-and-heroku-database-settings-django", "live": true, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 9, "content_object": 9}, {"pk": null, "tag": 5, "content_object": 9}, {"pk": null, "tag": 7, "content_object": 9}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\\\r\\\\nis improperly configured. \\\\r\\\\nPlease supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to settings.py to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-07-29T21:46:27.842Z", "expire_at": null}	\N	9	1
49	f	2015-09-27 16:36:27.771893+00	{"subtitle": "For when Heroku settings break your local build", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-29T21:54:06.896Z", "go_live_at": null, "title": "Separating local and Heroku database settings for Django", "seo_title": "", "listing_intro": "<p>For when Heroku settings break your local build</p>", "slug": "separating-local-and-heroku-database-settings-django", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 9, "content_object": 9}, {"pk": null, "tag": 5, "content_object": 9}, {"pk": null, "tag": 7, "content_object": 9}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently building a small Django app, and I wanted to deploy it to Heroku. To do this, Heroku asks you to implement some specific database settings at the bottom of your settings.py file:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"import dj_database_url\\\\r\\\\nDATABASES['default'] =  dj_database_url.config()\\\\r\\\\n\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The problem with this was that although these settings worked when the project was deployed to Heroku, they broke my local build with the following error:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"raise ImproperlyConfigured(\\\\\\"settings.DATABASES is improperly configured. \\\\\\"\\\\r\\\\ndjango.core.exceptions.ImproperlyConfigured: settings.DATABASES \\\\r\\\\nis improperly configured. \\\\r\\\\nPlease supply the ENGINE value. Check settings documentation for more details.\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>The reason I was getting this on my local build is that dj_database expects an environment variable, \\\\\\"DATABASE_URL\\\\\\", to be present on the local system, like it is when running on Heroku.</p><p>The simple solution I came up with was to add a an IF statement in to settings.py to detect whether or not the environment variable was present:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"env = os.environ.copy()\\\\r\\\\ndb_url = env.get('DATABASE_URL', False)\\\\r\\\\n\\\\r\\\\nif db_url != False:\\\\r\\\\n    import dj_database_url\\\\r\\\\n    DATABASES['default'] =  dj_database_url.config()\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>You can't just say 'if env['DATABASE_URL']' as this raises a <a href=\\\\\\"https://wiki.python.org/moin/KeyError\\\\\\">KeyError</a>. So instead I've used the 'get' method to either assign the value of the environment variable if it exists, or the default, 'False', if it doesn't. And voil\\\\u00e0! My app now runs on both my local machine and Heroku.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-07-29", "path": "000100010005", "url_path": "/home/separating-local-and-heroku-database-settings-django/", "expired": false, "pk": 9, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-07-29T21:46:27.842Z", "expire_at": null}	\N	9	1
24	f	2015-06-29 19:56:27.083102+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-29T19:56:08.632Z", "go_live_at": null, "title": "Super simple Django error logging configuration for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "", "slug": "django-error-logging-configuration-heroku", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 5, "content_object": 7}, {"pk": null, "tag": 6, "content_object": 7}, {"pk": null, "tag": 7, "content_object": 7}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nLOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nheroku logs --source app --tail\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>For more information on Django logging <a href=\\\\\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\\\\\">see the documentation</a>.</p><p>For more information on Heroku logging <a href=\\\\\\"https://devcenter.heroku.com/articles/logging\\\\\\">see their support centre</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-29", "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-29T19:56:08.690Z", "expire_at": null}	\N	7	1
79	f	2016-02-28 19:44:13.497154+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-07-11T10:09:28.015Z", "go_live_at": null, "title": "Super simple Django error logging configuration, good for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "<p>A simple code block to output Django error messages to the console</p>", "slug": "django-error-logging-configuration-heroku", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 5, "content_object": 7}, {"pk": null, "tag": 6, "content_object": 7}, {"pk": null, "tag": 7, "content_object": 7}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"LOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"heroku logs --source app --tail\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h3><hr/></h3><h3>What's going on here?</h3><p>We'll break this down into each individual section:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"'version': 1\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Identifies the format of the logging dictionary. Currently there is only 1 version available, but there could be more in the future.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"'disable_existing_loggers': False,\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This indicates that we shouldn't disable the default logging configuration. The default is 'True', but it's not recommended to do this, as the default logs can be useful. Instead we will keep the default logs and redefine certain elements of them to output logs to the console.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>There are four elements of a logging configuration:</p><p></p><ul><li>Loggers</li><li>Handlers</li><li>Filters</li><li>Formatters</li></ul><p>For our configuration we are only interested in the first two. The code above sets up our logging handler. We are declaring the console as a logging stream handler.</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then finally we set up the logger itself, instruct it to utilise the 'console' handler, and set the minimum level of logging to 'ERROR'. This way we only see when things are actually wrong with our application, rather than logging all messages coming out of Django (the 'DEBUG' logging level).</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>For more information on Django logging\\\\u00a0<a href=\\\\\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\\\\\">see the documentation</a>.</p><p>For more information on Heroku logging\\\\u00a0<a href=\\\\\\"https://devcenter.heroku.com/articles/logging\\\\\\">see their support centre</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-29", "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-29T19:56:08.690Z", "expire_at": null}	\N	7	1
94	f	2016-03-03 15:22:03.878964+00	{"subtitle": "I wanted to import some bookmarks, so I wrote a little script", "search_description": "A short script that imports content from a CSV file into Django / Wagtail CMS", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T13:06:25.833Z", "go_live_at": null, "title": "Simple content import script for Django / Wagtail", "seo_title": "", "listing_intro": "", "slug": "simple-content-import-script-django-wagtail", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 2, "content_object": 16}, {"pk": null, "tag": 47, "content_object": 16}, {"pk": null, "tag": 7, "content_object": 16}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been collating links as part of my <a id=\\\\\\"14\\\\\\" linktype=\\\\\\"page\\\\\\">reading list</a>\\\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that finding individual ones has become difficult.</p><p>So! I created a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\\\\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\\\\\">quick python script</a> to import the bookmarks from a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\\\\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p><p>By the way, you can now\\\\u00a0<a id=\\\\\\"15\\\\\\" linktype=\\\\\\"page\\\\\\">follow my reading list here</a>!</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-22", "path": "000100010009", "url_path": "/home/simple-content-import-script-django-wagtail/", "expired": false, "pk": 16, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-02-22T13:41:14.867Z", "expire_at": null}	\N	16	1
19	f	2015-06-29 12:30:40.681003+00	{"body": "", "locked": false, "title": "chrxr.com", "numchild": 2, "show_in_menus": false, "live": true, "seo_title": "chrxr.com | Digital project management, Wagtail and skateboarding", "search_description": "A blog mostly about Wagtail development, but also the occasional bit of skateboarding.", "depth": 2, "latest_revision_created_at": "2015-06-25T12:15:05.821Z", "has_unpublished_changes": false, "content_type": 3, "path": "00010001", "owner": null, "pk": 3, "first_published_at": "2015-06-25T12:11:06.703Z", "url_path": "/home/", "expired": false, "slug": "home", "expire_at": null, "go_live_at": null}	\N	3	1
200	f	2016-11-19 14:55:18.292503+00	{"subtitle": "", "search_description": "When employing a digital agency on a project, you can save time and money with a little pre-project preparation", "owner": 1, "intro": "When employing a digital agency on a project, you can save time and money with a little pre-project preparation", "latest_revision_created_at": "2016-11-14T20:16:34.913Z", "go_live_at": null, "title": "5 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "<p>When employing a digital agency on a project, you can save time and money with a little pre-project preparation</p>", "slug": "5-ways-get-best-out-your-digital-agency", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Having been a project manager on both the agency and client side of big website builds, I've found that the smoothest projects and best agency client relationships are a result of good preparation on the client side. Here are five simple ways to get your relationship with your agency off to the best possible start.</p><h3>1 - Establish clear lines of communications</h3><p>Of any of these suggestions, turning around agency queries quickly, with clear decisions, can have the most direct, and positive impact on the overall cost of your project.</p><p>There should be one main point of contact in your organisation who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app. Usually this person will be the project manager on the business side. If this is you, make sure the limits of your authority are clearly defined with senior stakeholder in your business.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>2 - Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>3 - Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one\\\\u00a0meetings\\\\u00a0with the top-stakeholders\\\\u00a0at the beginning of the process\\\\u00a0to make them feel like their ideas and concerns are being taken into consideration. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. Don't over-promise though. This is also a good time to discuss business objectives.</p><h3>4 - What are your business objectives for the project?</h3><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know\\\\u00a0the concrete results that you want to see from your project. Business objectives are the start of that.\\\\u00a0You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. Discussing these internally will also generate buy-in for your project.<br/></p><p>A good agency will subject these initial objectives to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>A great tool to help you develop your business objectives and related key performance indicators (KPIs) is the <a href=\\\\\\"http://www.kaushik.net/avinash/digital-marketing-and-measurement-model/\\\\\\">Digital Marketing and Measurement Model</a>, created by digital marketing and analytics guru, Avinash Kaushik. Read this, follow his instructions, and you'll end up with objectives and KPIs ready to hand straight over to your agency.</p><p></p><h3>5 - Start capturing data as soon as possible</h3><p>Tracking the performance of a new website or app against existing solutions will be very difficult if you have no data on how your existing solutions are performing. Collecting this data should be a top priority.</p><p>Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-11-14", "path": "00010001000H", "url_path": "/home/5-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-11-14T19:44:46.341Z", "expire_at": null}	\N	24	1
96	f	2016-03-03 19:25:04.021058+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T19:24:23.282Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>The Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view. You can see the class for my blog RSS feed below:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\\\r\\\\n\\\\r\\\\n    def item_link(self, item):\\\\r\\\\n        base_url = item.get_absolute_url()\\\\r\\\\n        return base_url\\", \\"language\\": \\"python\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
74	f	2016-02-28 19:35:04.38815+00	{"subtitle": "A blog dealing mostly with Wagtail CMS and digital project management", "search_description": "A blog dealing mostly with Wagtail CMS and digital project management", "owner": null, "latest_revision_created_at": "2016-02-28T19:31:50.325Z", "go_live_at": null, "title": "chrxr.com", "seo_title": "chrxr.com | Wagtail CMS and Digital project management", "slug": "home", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "numchild": 9, "content_type": 3, "show_in_menus": false, "path": "00010001", "url_path": "/home/", "expired": false, "pk": 3, "locked": false, "depth": 2, "first_published_at": "2015-06-25T12:11:06.703Z", "expire_at": null}	\N	3	1
110	f	2016-03-04 13:21:03.248314+00	{"subtitle": "I wanted to import some bookmarks, so I wrote a little script", "search_description": "A short script that imports content from a CSV file into Django / Wagtail CMS", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T15:22:03.878Z", "go_live_at": null, "title": "Simple content import script for Django / Wagtail", "seo_title": "", "listing_intro": "<h3>A short script that imports content from a CSV file into Django / Wagtail CMS</h3>", "slug": "simple-content-import-script-django-wagtail", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 2, "content_object": 16}, {"pk": null, "tag": 47, "content_object": 16}, {"pk": null, "tag": 7, "content_object": 16}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been collating links as part of my <a id=\\\\\\"14\\\\\\" linktype=\\\\\\"page\\\\\\">reading list</a>\\\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that finding individual ones has become difficult.</p><p>So! I created a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\\\\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\\\\\">quick python script</a> to import the bookmarks from a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\\\\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p><p>By the way, you can now\\\\u00a0<a id=\\\\\\"15\\\\\\" linktype=\\\\\\"page\\\\\\">follow my reading list here</a>!</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-22", "path": "000100010009", "url_path": "/home/simple-content-import-script-django-wagtail/", "expired": false, "pk": 16, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-02-22T13:41:14.867Z", "expire_at": null}	\N	16	1
111	f	2016-03-04 13:21:18.12479+00	{"subtitle": "I wanted to import some bookmarks, so I wrote a little script", "search_description": "A short script that imports content from a CSV file into Django / Wagtail CMS", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T13:21:03.248Z", "go_live_at": null, "title": "Simple content import script for Django / Wagtail", "seo_title": "", "listing_intro": "<p>A short script that imports content from a CSV file into Django / Wagtail CMS</p>", "slug": "simple-content-import-script-django-wagtail", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 2, "content_object": 16}, {"pk": null, "tag": 47, "content_object": 16}, {"pk": null, "tag": 7, "content_object": 16}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been collating links as part of my <a id=\\\\\\"14\\\\\\" linktype=\\\\\\"page\\\\\\">reading list</a>\\\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that finding individual ones has become difficult.</p><p>So! I created a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\\\\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\\\\\">quick python script</a> to import the bookmarks from a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\\\\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p><p>By the way, you can now\\\\u00a0<a id=\\\\\\"15\\\\\\" linktype=\\\\\\"page\\\\\\">follow my reading list here</a>!</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-22", "path": "000100010009", "url_path": "/home/simple-content-import-script-django-wagtail/", "expired": false, "pk": 16, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-02-22T13:41:14.867Z", "expire_at": null}	\N	16	1
149	f	2016-08-27 17:36:54.363798+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T17:34:15.714Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile in my code editor and add in the following just below the `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` line.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"Vagrant.configure(\\\\\\"2\\\\\\") do |config|\\\\r\\\\n  config.vm.provision \\\\\\"shell\\\\\\", inline: \\\\\\"echo Hello\\\\\\"\\\\r\\\\n\\\\r\\\\n  config.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n    zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  end\\\\r\\\\n\\\\r\\\\n  config.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n    solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  end\\\\r\\\\n\\\\r\\\\n  config.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n    solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  end\\\\r\\\\n\\\\r\\\\n  config.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n    solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  end\\\\r\\\\nend\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
84	f	2016-02-29 09:39:17.900385+00	{"subtitle": "I wanted to import some bookmarks, so I wrote a little script", "search_description": "A short script that imports content from a CSV file into Django / Wagtail CMS", "owner": 1, "intro": "", "latest_revision_created_at": "2016-02-29T09:39:14.582Z", "go_live_at": null, "title": "Simple content import script for Django / Wagtail", "seo_title": "", "listing_intro": "", "slug": "simple-content-import-script-django-wagtail", "live": true, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 2, "content_object": 16}, {"pk": null, "tag": 47, "content_object": 16}, {"pk": null, "tag": 7, "content_object": 16}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been collating links as part of my <a id=\\\\\\"14\\\\\\" linktype=\\\\\\"page\\\\\\">reading list</a>\\\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that finding individual ones has become difficult.</p><p>So! I created a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\\\\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\\\\\">quick python script</a> to import the bookmarks from a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\\\\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-22", "path": "000100010009", "url_path": "/home/simple-content-import-script-django-wagtail/", "expired": false, "pk": 16, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-02-22T13:41:14.867Z", "expire_at": null}	\N	16	1
20	f	2015-06-29 19:52:20.351772+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Super simple Django error logging configuration for Heroku", "seo_title": "", "listing_intro": "", "slug": "super-simple-django-error-logging-configuration-heroku", "live": false, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nLOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nheroku logs --source app --tail\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "000100010003", "url_path": "/home/super-simple-django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	7	1
22	f	2015-06-29 19:54:00.57723+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-29T19:53:44.119Z", "go_live_at": null, "title": "Super simple Django error logging configuration for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "", "slug": "django-error-logging-configuration-heroku", "live": false, "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nLOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nheroku logs --source app --tail\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	7	1
26	f	2015-06-29 19:58:23.849112+00	{"subtitle": "", "search_description": "A simple code block to output Django error messages to the console", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-29T19:57:52.657Z", "go_live_at": null, "title": "Super simple Django error logging configuration, good for Heroku", "seo_title": "Simple Django error logging configuration for Heroku", "listing_intro": "<p>A simple code block to output Django error messages to the console</p>", "slug": "django-error-logging-configuration-heroku", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 5, "content_object": 7}, {"pk": null, "tag": 6, "content_object": 7}, {"pk": null, "tag": 7, "content_object": 7}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Here's a really simple logging configuration for Django to output anything of the level 'Error' and up to the console. Heroku's logging functionality allows you to monitor application messaging in real time in your production environment. Just put the code below in the bottom of your settings file.</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nLOGGING = {\\\\r\\\\n    'version': 1,\\\\r\\\\n    'disable_existing_loggers': False,\\\\r\\\\n    'handlers': {\\\\r\\\\n        'console': {\\\\r\\\\n            'class': 'logging.StreamHandler',\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n    'loggers': {\\\\r\\\\n        'django': {\\\\r\\\\n            'handlers': ['console'],\\\\r\\\\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'ERROR'),\\\\r\\\\n        },\\\\r\\\\n    },\\\\r\\\\n}\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Then run the following Heroku command to display application messages in real time:</p>\\"}, {\\"type\\": \\"codeblock\\", \\"value\\": \\"\\\\r\\\\nheroku logs --source app --tail\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>For more information on Django logging <a href=\\\\\\"https://docs.djangoproject.com/en/1.8/topics/logging/\\\\\\">see the documentation</a>.</p><p>For more information on Heroku logging <a href=\\\\\\"https://devcenter.heroku.com/articles/logging\\\\\\">see their support centre</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-29", "path": "000100010003", "url_path": "/home/django-error-logging-configuration-heroku/", "expired": false, "pk": 7, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-29T19:56:08.690Z", "expire_at": null}	\N	7	1
112	f	2016-03-04 13:22:01.671171+00	{"subtitle": "", "search_description": "Organise your day and manage your reading list better using Evernote", "owner": 1, "intro": "I'm the type of person who really needs a to-do list but is rubbish at maintaining one. I've tried many different solutions: specific apps, pen and paper, spreadsheets. None of them have lasted for longer than a couple of weeks.", "latest_revision_created_at": "2016-02-28T19:55:49.468Z", "go_live_at": null, "title": "Using Evernote as a to-do list", "seo_title": "", "listing_intro": "<p>Organise your day and manage your reading list better using Evernote</p>", "slug": "using-evernote-as-a-to-do-list", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Now though I think I've settled on a system that really works for me. I use Evernote ALL THE TIME. It's an essential tool for me. So I thought, how can I use the tool that I already use to help me organise my time.<br/></p><p>The key was to format it in an appropriate way. At the top of my list I have the 'week beginning' date. Then I have my current list of to-dos, ordered roughly by priority. I add EVERYTHING work-related that I have to do that is a discrete task, even small tasks. If I do something work-related that's not on the to-do list, I add it to the list. I use the checkbox list type, as it soothes my OCD side to see each box ticked off. Once a task has been ticked off, I copy and paste it into another list below the main list, which has today's date as the heading. So as the days go by you end up with an archive of your completed tasks for each individual day.</p><p>Visibility of the list is key. One of the biggest pains in trying to use a specific tool was that it was one more thing to have open on the screen. With Evernote, I just add it to my 'Shortcuts' list. Then it's always there in the top-left of Evernote, a tool that I would have open all the time anyway!</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 2}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>At the end of the month, I'll archive that particular note in a separate notebook, take it out of my shortcuts, and create a new to-do note for the month.</p><p>I've now started to expand the format to include other types of to-dos. I often review relevant newsletters and blogs for content as soon as I get into work, but then I like to actually read them over lunch. So I've created a 'Lunchtime reading' list, again with tick-boxes, and a 'Reading archive' at the bottom of the to-do note.</p>\\"}, {\\"type\\": \\"image\\", \\"value\\": 3}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>Anyway, it's a very simple solution, that appeals to me for it's 'Collect the completed tasks!' nature.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-12-14", "path": "000100010007", "url_path": "/home/using-evernote-as-a-to-do-list/", "expired": false, "pk": 14, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-12-15T22:15:02.212Z", "expire_at": null}	\N	14	1
189	f	2016-10-23 19:12:57.871962+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-10-23T18:57:14.970Z", "go_live_at": null, "title": "10 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "10-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>When working with a digital agency, there are a number of things you can do to get the best out of them. By doing these things you can make the your life easier, their life easier, and save a bunch of money as well.</p><h3>Establish clear lines of communications</h3><p>Turning around agency queries quickly with clear decisions can have the biggest direct, positive impact on the overall cost of your project of any of these suggestions.</p><p>There should be one main point of contact in your organisation, who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses.</p><h3>Gather all the user information that you have</h3><h3>Focus on the discovery phase</h3><h3>What are your business goals?</h3><h3>Start tracking things as soon as possible</h3><h3>Got brand guidelines? Hand'em over!</h3>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/10-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
8	f	2015-06-25 12:11:06.670882+00	{"body": "", "locked": false, "title": "chrxr.com", "numchild": 1, "show_in_menus": false, "live": true, "seo_title": "", "search_description": "", "depth": 2, "latest_revision_created_at": null, "has_unpublished_changes": false, "content_type": 3, "path": "00010001", "owner": null, "pk": 3, "first_published_at": null, "url_path": "/home/", "expired": false, "slug": "home", "expire_at": null, "go_live_at": null}	\N	3	1
10	f	2015-06-25 12:14:14.316016+00	{"body": "", "locked": false, "title": "chrxr.com", "numchild": 2, "show_in_menus": false, "live": true, "seo_title": "chrxr.com | A blog written by Chris Rogers", "search_description": "", "depth": 2, "latest_revision_created_at": "2015-06-25T12:11:06.670Z", "has_unpublished_changes": false, "content_type": 3, "path": "00010001", "owner": null, "pk": 3, "first_published_at": "2015-06-25T12:11:06.703Z", "url_path": "/home/", "expired": false, "slug": "home", "expire_at": null, "go_live_at": null}	\N	3	1
73	f	2016-02-28 19:31:50.325242+00	{"subtitle": "A blog dealing mostly with Wagtail CMS and digital project management", "search_description": "A blog mostly about Wagtail development, but also the occasional bit of skateboarding.", "owner": null, "latest_revision_created_at": "2015-06-29T12:30:40.681Z", "go_live_at": null, "title": "chrxr.com", "seo_title": "chrxr.com | Digital project management, Wagtail and skateboarding", "slug": "home", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "numchild": 9, "content_type": 3, "show_in_menus": false, "path": "00010001", "url_path": "/home/", "expired": false, "pk": 3, "locked": false, "depth": 2, "first_published_at": "2015-06-25T12:11:06.703Z", "expire_at": null}	\N	3	1
138	f	2016-04-01 12:21:30.205154+00	{"subtitle": "Explorations and ramblings of Chris Rogers, a digital project manager and hobbyist developer", "search_description": "Explorations and ramblings of a digital project manager and hobbyist developer", "owner": null, "latest_revision_created_at": "2016-02-28T19:49:52.824Z", "go_live_at": null, "title": "chrxr.com", "seo_title": "chrxr.com | Digital project management and web development", "slug": "home", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "numchild": 13, "content_type": 3, "show_in_menus": false, "path": "00010001", "url_path": "/home/", "expired": false, "pk": 3, "locked": false, "depth": 2, "first_published_at": "2015-06-25T12:11:06.703Z", "expire_at": null}	\N	3	1
11	f	2015-06-25 12:15:05.821578+00	{"body": "", "locked": false, "title": "chrxr.com", "numchild": 2, "show_in_menus": false, "live": true, "seo_title": "chrxr.com | Digital project management, Wagtail and skateboarding", "search_description": "", "depth": 2, "latest_revision_created_at": "2015-06-25T12:14:14.316Z", "has_unpublished_changes": false, "content_type": 3, "path": "00010001", "owner": null, "pk": 3, "first_published_at": "2015-06-25T12:11:06.703Z", "url_path": "/home/", "expired": false, "slug": "home", "expire_at": null, "go_live_at": null}	\N	3	1
139	f	2016-04-01 12:22:13.404802+00	{"subtitle": "Explorations and ramblings of Chris Rogers, a digital project manager and hobbyist developer", "search_description": "Explorations and ramblings of Chris Rogers, a digital project manager and hobbyist developer", "owner": null, "latest_revision_created_at": "2016-04-01T12:21:30.205Z", "go_live_at": null, "title": "chrxr.com", "seo_title": "chrxr.com | Digital project management and web development", "slug": "home", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "numchild": 13, "content_type": 3, "show_in_menus": false, "path": "00010001", "url_path": "/home/", "expired": false, "pk": 3, "locked": false, "depth": 2, "first_published_at": "2015-06-25T12:11:06.703Z", "expire_at": null}	\N	3	1
75	f	2016-02-28 19:40:47.067972+00	{"subtitle": "I wanted to import some bookmarks, so I wrote a little script", "search_description": "A short script that imports content from a CSV file into Django / Wagtail CMS", "owner": 1, "intro": "", "latest_revision_created_at": "2016-02-22T13:41:14.842Z", "go_live_at": null, "title": "Simple content import script for Django / Wagtail", "seo_title": "", "listing_intro": "", "slug": "simple-content-import-script-django-wagtail", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 2, "content_object": 16}, {"pk": null, "tag": 47, "content_object": 16}, {"pk": null, "tag": 7, "content_object": 16}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been collating links as part of my <a id=\\\\\\"14\\\\\\" linktype=\\\\\\"page\\\\\\">reading list</a>\\\\u00a0for the last few months. Occasionally I like to look back through them to find something particularly interesting. However, I've got so many now that find individual ones has become difficult.</p><p>So! I created a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/blog/models.py#L31\\\\\\">Bookmark snippet in Wagtail</a>, then wrote a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/importer.py\\\\\\">quick python script</a> to import the bookmarks from a <a href=\\\\\\"https://github.com/chrxr/blog_project/blob/master/csvtest.csv\\\\\\">CSV file</a>. It took about 30 minutes in all. The hardest part was working out that I needed to initialise Django by setting the DJANGO_SETTINGS_MODULE environment variable.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-02-22", "path": "000100010009", "url_path": "/home/simple-content-import-script-django-wagtail/", "expired": false, "pk": 16, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-02-22T13:41:14.867Z", "expire_at": null}	\N	16	1
201	t	2018-03-06 20:59:34.412469+00	{"subtitle": "A brief history of me", "search_description": "", "owner": 1, "intro": "My name is Chris Rogers and I'm currently working as a project manager at Torchbox, a digital agency based in Oxford, England.", "latest_revision_created_at": "2015-06-24T12:20:37.399Z", "go_live_at": null, "title": "Who am I?", "seo_title": "", "listing_intro": "", "draft_title": "Who am I?", "live": false, "last_published_at": null, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"id\\": \\"52c7eb26-85d2-4c33-86c5-a6bff24d8e9f\\", \\"value\\": \\"<ul><li>Previously worked in digital publishing for 5 years, at Oxford University Press and Penguin Books.</li><li>Now a hands on project manager at Torchbox, developing websites and digital strategies for large multi-national charities.</li><li>Like to get my hands dirty helping with development where necessary.</li><li>Main experience is with Python, Django, JavaScript, CSS and putting these all together into Wagtail</li></ul>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-24", "path": "000100010002", "url_path": "/home/who-am-i/", "expired": false, "slug": "who-am-i", "main_image": null, "locked": false, "pk": 6, "depth": 3, "first_published_at": "2015-06-24T12:20:37.428Z", "expire_at": null, "live_revision": null}	\N	6	1
98	f	2016-03-03 19:43:52.800595+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-03T19:31:12.737Z", "go_live_at": null, "title": "Adding RSS feeds to a Wagtail site", "seo_title": "", "listing_intro": "", "slug": "adding-rss-feeds-wagtail-site", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>First of all, it's exactly how you would do it in a normal Django-powered site. The official docs for creating RSS or Atom feeds <a href=\\\\\\"https://docs.djangoproject.com/en/1.9/ref/contrib/syndication/\\\\\\">can be found here</a>. That's one of the beauties of Wagtail, it keeps hold of many of the great features of Django.</p><p>But if you're not too familiar with Django, or the commonalities between vanilla Django and Wagtail, then hopefully this description of how I implemented two RSS feeds on this site will help you.</p><p>There are two elements to an RSS feed in Django/Wagtail:</p><p></p><ol><li>The Feed class</li><li>The URL configuration</li></ol><h3>Writing the Feed class</h3><p>The Feed class gets all the relevant bits of data needed to serve the RSS feed to the client. It's essentially a class-based view.</p><p>The code for this can live anywhere within your codebase. I chose to create a new app for it. To do this I created a new folder in the root of my Wagtail project, called 'blog_feed' (it could be called whatever you want). Within this are two files:</p><p></p><ul><li>An empty __init__.py file</li><li>A file called forms.py, which will contain our code</li></ul><p></p><p>\\\\u00a0You can see the class for my blog RSS feed below:</p><p></p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"from django.db import models\\\\r\\\\nfrom django.contrib.syndication.views import Feed\\\\r\\\\nfrom blog.models import BlogPage\\\\r\\\\n\\\\r\\\\nclass BlogsFeed(Feed):\\\\r\\\\n    title = \\\\\\"My blog articles\\\\\\"\\\\r\\\\n    link = \\\\\\"/blogs-feed/\\\\\\"\\\\r\\\\n    description = \\\\\\"All of my blogs as they are published\\\\\\"\\\\r\\\\n\\\\r\\\\n    def items(self):\\\\r\\\\n        return BlogPage.objects.live().order_by('-date')\\\\r\\\\n\\\\r\\\\n    def item_title(self, item):\\\\r\\\\n        return item.title\\\\r\\\\n\\\\r\\\\n    def item_description(self, item):\\\\r\\\\n        return item.intro\\\\r\\\\n\\\\r\\\\n    def item_link(self, item):\\\\r\\\\n        base_url = item.get_absolute_url()\\\\r\\\\n        return base_url\\", \\"language\\": \\"python\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>In our first three lines we import the essential elements needed to create our Feed class. It will inherit the functionality of the basic Django Feed class, so we import that. We also import the BlogPage model so that we can access the URLs, titles and descriptions that will make up the feed itself.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-03-03", "path": "00010001000C", "url_path": "/home/adding-rss-feeds-wagtail-site/", "expired": false, "pk": 19, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	19	1
142	f	2016-08-15 19:14:11.71415+00	{"subtitle": "I made a thingumabob ", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-15T19:14:08.497Z", "go_live_at": null, "title": "Local IIIF presentation API validator", "seo_title": "", "listing_intro": "", "slug": "local-iiif-presentation-api-validator", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 74, "content_object": 22}, {"pk": null, "tag": 75, "content_object": 22}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I wrote a small python script to allow the validation of single IIIF manifests, or a folder of manifests. It can be run via the command line, or imported as a module into another Python application.</p><p>The code, and full usage instructions can be <a href=\\\\\\"https://github.com/chrxr/IIIF-local-validator\\\\\\">found in the GitHub repository</a>.</p><p>The script utilises the IIIF manifest loader and factory scripts from the <a href=\\\\\\"https://github.com/IIIF/presentation-api\\\\\\">IIIF Presentation Implementations repository</a>, so mucho credit to <a href=\\\\\\"https://github.com/azaroth42\\\\\\">Rob Sanderson at Stanford for those</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-15", "path": "00010001000F", "url_path": "/home/local-iiif-presentation-api-validator/", "expired": false, "pk": 22, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	22	1
141	f	2016-08-15 19:14:08.497675+00	{"subtitle": "I made a thingumabob ", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Local IIIF presentation API validator", "seo_title": "", "listing_intro": "", "slug": "local-iiif-presentation-api-validator", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": 78, "tag": 74, "content_object": 22}, {"pk": 79, "tag": 75, "content_object": 22}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I wrote a small python script to allow the validation of single IIIF manifests, or a folder of manifests. It can be run via the command line, or imported as a module into another Python application.</p><p>The code, and full usage instructions can be <a href=\\\\\\"https://github.com/chrxr/IIIF-local-validator\\\\\\">found in the GitHub repository</a>.</p><p>The script utilises the IIIF manifest loader and factory scripts from the <a href=\\\\\\"https://github.com/IIIF/presentation-api\\\\\\">IIIF Presentation Implementations repository</a>, so mucho credit to <a href=\\\\\\"https://github.com/azaroth42\\\\\\">Rob Sanderson at Stanford for those</a>.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-15", "path": "00010001000F", "url_path": "/home/local-iiif-presentation-api-validator/", "expired": false, "pk": 22, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	22	1
190	f	2016-10-23 20:00:26.38528+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-10-23T19:12:57.871Z", "go_live_at": null, "title": "10 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "10-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>When working with a digital agency, there are a number of things you can do to get the best out of them. By doing these things you can make the your life easier, their life easier, and save a bunch of money as well.</p><h3>Establish clear lines of communications</h3><p>Turning around agency queries quickly with clear decisions can have the biggest direct, positive impact on the overall cost of your project of any of these suggestions.</p><p>There should be one main point of contact in your organisation, who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one meetings at the beginning of the process. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. This is also a good time to discuss business objectives.</p><h3>What are your business objectives for the project?</h3><p>What are the concrete results that you want to see from your project? You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. A good agency will subject these to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business, and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know what you want from the project. Business objectives are the start of that. From there you can develop key performance indicators (KPIs) based on these business objectives. Once you've got KPIs, you'll know what to track to ensure you're meeting your business objectives!</p><p><h3>Start tracking things as soon as possible</h3><p>Once you have those KPIs, you should start tracking your current performance against them as soon as possible. Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p><p>This information will help your agency make better decisions about the design and build of your new website. These better decisions mean you are more likely to meet your company's business objectives. From a cost saving perspective, having clear objectives will help your agency get things right first time. Every time you need to go back and change something, the project budget gets lower and lower.</p><h3>Gather all the user information that you have</h3><h3>Focus on the discovery phase</h3></p><h3><br/></h3><h3>Got brand guidelines? Hand'em over!</h3>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/10-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
5	f	2015-06-22 12:59:46.704737+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-22T12:58:09.840Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "", "slug": "what-place", "live": true, "has_unpublished_changes": false, "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
191	f	2016-10-23 20:14:27.737463+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-10-23T20:00:26.385Z", "go_live_at": null, "title": "10 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "10-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>When working with a digital agency, there are a number of things you can do to get the best out of them. By doing these things you can make the your life easier, their life easier, and save a bunch of money as well.</p><h3>Establish clear lines of communications</h3><p>Turning around agency queries quickly with clear decisions can have the biggest direct, positive impact on the overall cost of your project of any of these suggestions.</p><p>There should be one main point of contact in your organisation, who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one meetings at the beginning of the process. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. This is also a good time to discuss business objectives.</p><h3>What are your business objectives for the project?</h3><p>What are the concrete results that you want to see from your project? You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. A good agency will subject these to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business, and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know what you want from the project. Business objectives are the start of that. From there you can develop key performance indicators (KPIs) based on these business objectives. Once you've got KPIs, you'll know what to track to ensure you're meeting your business objectives!</p><p></p><h3>Start tracking KPIs as soon as possible</h3><p>Once you have those KPIs, you should start tracking your current performance against them as soon as possible. Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p><p>This information will help your agency make better decisions about the design and build of your new website. These better decisions mean you are more likely to meet your company's business objectives. From a cost saving perspective, having clear objectives will help your agency get things right first time. Every time you need to go back and change something, the project budget gets lower and lower.</p><h3>Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work, only to find that\\\\u00a0</p><h3>Gather all the user information that you have</h3><h3>Focus on the discovery phase</h3><p></p><h3><br/></h3><h3><br/></h3>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/10-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
192	f	2016-10-23 20:17:51.396505+00	{"subtitle": "", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-10-23T20:14:27.737Z", "go_live_at": null, "title": "10 ways to get the best out of your digital agency", "seo_title": "", "listing_intro": "", "slug": "10-ways-get-best-out-your-digital-agency", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>When working with a digital agency, there are a number of things you can do to get the best out of them. By doing these things you can make the your life easier, their life easier, and save a bunch of money as well.</p><h3>Establish clear lines of communications</h3><p>Turning around agency queries quickly with clear decisions can have the biggest direct, positive impact on the overall cost of your project of any of these suggestions.</p><p>There should be one main point of contact in your organisation, who will liaise directly with the project manager at your digital agency. That person should have the power to make at least reasonably important decisions. For example, that person should be able to make a call on a design query for a website, or on the detailed functionality of a feature of an app.</p><p>When it comes to big decisions that need wider consultation within your business, you should have a clearly defined route for getting these decisions made. Perhaps you have a weekly meeting with a project board, where agency queries could be a standing agenda item.</p><p>For big, complicated projects, you might need multiple points of contacts, each responsible for a specific feature or area of the project. Each of these people should have decision making powers over their area. They will also need to communicate with each other very regularly, to make sure that they aren't sending mixed messages back to the agency. This should be avoided at all costs, as confusion on the agency side will lead to higher costs. Stick to one point of contact if at all possible.</p><h3>Got brand guidelines? Hand'em over!</h3><p>If your project involves design of any kind, make sure your agency has your branding guidelines as soon as possible. If you don't have a company endorsed, branding guidelines document, don't worry, your agency should help you to define what you expect from the design element of your project, and this can be based on your existing corporate identity as necessary.</p><p>Branding guidelines are often very expensive documents to but together. If your company has one, then you'll almost certainly be expected to at least pay it lip-service. I've seen projects spend thousands of pounds on design work only to find, once the branding guidelines were finally delivered, that almost of all of the work had to be redone. Avoid this at all costs! Find the guidelines and hand them over.</p><h3>Come prepared with buy-in from the top</h3><p>Getting buy-in from the senior execs in a business from the outset can only help smooth the communication process as the project progresses. Hold one-to-one meetings at the beginning of the process. Try to get them excited about the results your project could deliver, and discuss what they think should be the priority goals for the project. This is also a good time to discuss business objectives.</p><h3>What are your business objectives for the project?</h3><p>What are the concrete results that you want to see from your project? You can save quite a bit of time and discussion with your agency by being able to provide these business objectives for the project up-front. A good agency will subject these to scrutiny, and you should be willing to accept change if strong arguments are made. However, the act of compiling these objectives will allow you to focus on the most important desired benefits that you want to get out of a project.</p><p>Sometimes, projects can seem large and abstract, making it hard to focus down your objectives to a manageable list. For example, with a website design, it can often feel like just finishing the project is success enough in itself. I mean, you've got a whole brand new website, right? But this mentality can lead to loosing track of the things that really matter for your business, and your site visitors, like usability, search engine and speed optimisation, and conversion rates.</p><p>Your agency should help you through the process of discovering who your users are, and what exactly they need, but before they can do that, they'll need to know what you want from the project. Business objectives are the start of that. From there you can develop key performance indicators (KPIs) based on these business objectives. Once you've got KPIs, you'll know what to track to ensure you're meeting your business objectives!</p><p></p><h3>Start tracking KPIs as soon as possible</h3><p>Once you have those KPIs, you should start tracking your current performance against them as soon as possible. Historical data will allow your agency to see how much course correction is required to meet your business objectives. For a website redesign or build, make sure you have analytics installed on your current site, and start tracking performance (new users, session times, goal conversion rates etc) against your KPIs in a spreadsheet.Your agency project manager's eyes will light up when you hand this over to them at the beginning of the project.</p><p>This information will help your agency make better decisions about the design and build of your new website. These better decisions mean you are more likely to meet your company's business objectives. From a cost saving perspective, having clear objectives will help your agency get things right first time. Every time you need to go back and change something, the project budget gets lower and lower.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": null, "path": "00010001000H", "url_path": "/home/10-ways-get-best-out-your-digital-agency/", "expired": false, "pk": 24, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	24	1
16	f	2015-06-25 22:03:03.321563+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-25T22:02:56.512Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
175	f	2016-08-31 15:57:58.398771+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T15:57:36.951Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building your four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.\\\\u00a0<br/></h2><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
174	f	2016-08-31 15:57:36.95133+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T15:57:02.846Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building your four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.\\\\u00a0<br/></h2><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
173	f	2016-08-31 15:57:02.846063+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T12:55:51.331Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.\\\\u00a0<br/></h2><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
172	f	2016-08-31 12:55:51.33126+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T12:55:41.897Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.\\\\u00a0</p><p>There you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
171	f	2016-08-31 12:55:41.897267+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T12:46:28.227Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distros. I'm not a Windows user so you'll have to do your own research if that's what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.\\\\u00a0</p><p>So there you have it, a working SolrCloud setup using Vagrant. We've got no data in our test collection, but adding in data isn't SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
170	f	2016-08-31 12:46:28.227598+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T12:33:00.594Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 2` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Conclusion and further reading</h2><p>So now if you go to the 'Cloud' section of your Solr admin on any of your connected nodes, you should see something similar to the screenshot below.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
169	f	2016-08-31 12:33:00.594903+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-31T11:58:41.841Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Creating a test collection</h2><p>A 'Collection' in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 - replicationFactor 2\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"I'm not going to go into the detail of how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n- `-shards 2` : This defines how many shards the Collection should be split into.\\\\r\\\\n- `-replicationFactor` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
168	f	2016-08-31 11:58:41.841347+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-29T17:44:18.624Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node you currently have running.\\\\r\\\\n\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr stop\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then you restart in cloud mode with the following command:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the machines IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
155	f	2016-08-28 08:35:13.973305+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T08:27:45.149Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"config.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Each block in the code above is defining a separate virtual machine and giving each box a name. The `zoo1.vm.box` command is telling Vagrant which template to use for creating each box, and the `zoo1.vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
154	f	2016-08-28 08:27:45.149659+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T08:18:51.826Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"config.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Each block in the code above is defining a separate virtual machine and giving each box a name. The `zoo1.vm.box` command is telling Vagrant which template to use for creating each box, and the `zoo1.vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ip addr show` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
153	f	2016-08-28 08:18:51.82622+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T08:09:04.544Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"config.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Each block in the code above is defining a separate virtual machine and giving each box a name. The `zoo1.vm.box` command is telling Vagrant which template to use for creating each box, and the `zoo1.vm.network` command instructs Vagrant to create a private_network of type [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\"). \\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each.\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
152	f	2016-08-28 08:09:04.544995+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T17:40:15.470Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"config.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p><b>Note</b>: Some older versions of Vagrant has an issue when using the DCHP network type. They fail with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error should will disappear.<br/></p><p><br/></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
151	f	2016-08-27 17:40:15.470606+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T17:38:03.388Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile in my code editor and add in the following just below the `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` line.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"config.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
150	f	2016-08-27 17:38:03.388281+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T17:36:54.363Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile in my code editor and add in the following just below the `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` line.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"config.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
205	f	2019-04-30 13:11:43.50378+00	{"date": "2016-08-27", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Testing SolrCloud with Vagrant", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "pk": 23, "expired": false, "live_revision": null, "go_live_at": null, "path": "00010001000G", "last_published_at": "2016-09-06T06:58:13.438Z", "first_published_at": "2016-08-31T15:57:58.430Z", "subtitle": "Easy steps to emulate a multi-machine setup locally", "tagged_items": [{"content_object": 23, "tag": 76, "pk": null}, {"content_object": 23, "tag": 77, "pk": null}, {"content_object": 23, "tag": 78, "pk": null}, {"content_object": 23, "tag": 63, "pk": null}], "seo_title": "Testing SolrCloud with Vagrant", "show_in_menus": false, "title": "Testing SolrCloud with Vagrant", "body": "[{\\"value\\": \\"<p>I&#x27;d been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn&#x27;t much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I&#x27;m going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distributions. I&#x27;m not a Windows user so you&#x27;ll have to do your own research if that&#x27;s what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The &#x27;Cloud&#x27; part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. Queries arrive at one of the nodes and the query is then forwarded to the node where a replica of the appropriate shard is located.</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I&#x27;m going to assume you are aware of the basics.</p><p>Our aim for this test is to have each element of the SolrCloud setup running on its own virtual machine. Our setup will have three Solr nodes with which we can store and query the data, and a single Zookeeper instance to manage the nodes. We could have multiple Zookeeper instances to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we&#x27;re going to stick with just the one.</p><p>Having three Solr nodes means that we can split our data into two shards with two replicas of each, and if one of the Solr nodes goes down we&#x27;ll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Our test network will be built using Ubuntu virtual machines. The first thing we&#x27;re going to do is create a new directory for our test VMs on our host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\", \\"id\\": \\"1ebd142a-84f7-4091-b466-a2a5f35d7e5b\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\"}, \\"id\\": \\"56c7cf60-a22a-4f40-8bcc-a80ed71895fb\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. I'm going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). So, we need  to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\", \\"id\\": \\"98d24dbb-a9f4-420a-938d-54ce844c019b\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"config.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\"}, \\"id\\": \\"a02ee92d-450b-4aa7-93b6-8d25a6eba49e\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\", \\"id\\": \\"5ed02442-b4c9-4f90-b906-b7bd8971a9be\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\"}, \\"id\\": \\"9ff9e702-22b3-4b2f-a3dc-f695acad6f3b\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The process of building the four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\", \\"id\\": \\"49257c11-ebf1-4e9c-abdc-580053599135\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\"}, \\"id\\": \\"bf12a1b4-011d-4b16-b593-c044d0bfc94f\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\", \\"id\\": \\"ebb36154-589f-4d1d-bb3f-5d3c53c903ff\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\"}, \\"id\\": \\"a7c14ea9-9146-437a-9543-d82995ec9b16\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If we run this command on each box, we should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\", \\"id\\": \\"0ed9675b-32ae-4e0f-b03b-6c20603c915e\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"ssh vagrant@172.28.128.4\\"}, \\"id\\": \\"a5763017-deef-4016-a54a-2c5fd6aa7ca7\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\", \\"id\\": \\"16f82195-41cf-4938-9439-fba31d853438\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we&#x27;re going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\", \\"id\\": \\"29e8e843-d444-4821-8df4-5c043f5556a4\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\"}, \\"id\\": \\"8cddfd50-aba7-4de7-83fc-1bd7f3b85d76\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle&#x27;s official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\", \\"id\\": \\"1a1f1530-7ce6-477d-9abe-b76aae8edaae\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\", \\"id\\": \\"907250f0-0090-49e8-b9b6-ab569008eb1b\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\", \\"id\\": \\"4d07ca3d-4882-4a29-9569-b63e249a2f3c\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\"}, \\"id\\": \\"81ae7536-2df1-4c67-be41-592ac029af5b\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\", \\"id\\": \\"eccafba7-ccf9-4256-bcf9-793c6fc28d44\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\"}, \\"id\\": \\"11d0dd08-e15e-443f-9163-91b194fff0f7\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Now copy the following three lines into that file and save it.\\", \\"id\\": \\"f946537a-f152-463e-a7ee-9b94503082b7\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\"}, \\"id\\": \\"e95324f9-720a-4d4c-91b1-58d44ca63945\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\", \\"id\\": \\"5b3b1d8c-c536-4257-925b-6e98dd9fcd08\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\"}, \\"id\\": \\"c9584d5b-68bf-4f31-a678-8eccf2160862\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\", \\"id\\": \\"3271c0b7-d9fa-4a8e-ae69-4d82004970c6\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\"}, \\"id\\": \\"4effec9c-fb7e-419a-985c-0646c4e5eafd\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\", \\"id\\": \\"4c97f7fc-eed4-4852-856f-fe2a66b72747\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\"}, \\"id\\": \\"6436cc2c-2e1d-4fc5-aeb7-60fa82be5fb1\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\", \\"id\\": \\"31cd0aa9-b7a9-4b4b-a3da-7db5d15e7dfd\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\"}, \\"id\\": \\"f7f6949a-c5f9-4aab-9a57-ab60c4e94ba4\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node we currently have running.\\", \\"id\\": \\"b3006f8d-ead9-49bb-88f2-5d8d6dcd065f\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr stop\\"}, \\"id\\": \\"3523c902-135a-4429-8ef1-e59f5a7cf8e9\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Then we restart in cloud mode with the following command:\\", \\"id\\": \\"57e87a15-9b69-458e-81ff-900cfbfc967f\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\"}, \\"id\\": \\"ab95c913-da3b-432e-b84f-b1a71f5a62d1\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\", \\"id\\": \\"fa1e9d45-627d-4290-b681-98acf7b8d380\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\", \\"id\\": \\"511a0930-15e8-4e0a-82a4-32cd0264743a\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Creating a test collection</h2><p>A &#x27;Collection&#x27; in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\", \\"id\\": \\"a693e420-4a1e-4d4d-8535-e986c1df0031\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 -replicationFactor 2\\"}, \\"id\\": \\"66f233de-d788-43e9-a77e-aa2ecaea7f8a\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"I'm not going to go into great detail on how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\", \\"id\\": \\"6bc42cef-db3f-40b0-97b0-2c00e944a631\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the &#x27;Cloud&#x27; section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p>There you have it, a working SolrCloud setup using Vagrant. We&#x27;ve got no data in our test collection, but adding in data isn&#x27;t SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\", \\"id\\": \\"999c03d5-8c5d-4ffa-89b0-246414e750d2\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "testing-solrcloud-vagrant", "search_description": "Easy steps to emulate a multi-machine setup locally", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/testing-solrcloud-vagrant/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2016-09-06T06:58:13.438Z", "live": true}	\N	23	1
140	f	2016-05-30 11:10:57.210561+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "How this site was made, and what it was made with", "owner": 1, "intro": "", "latest_revision_created_at": "2016-03-04T13:19:02.055Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.5rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>The site is cached using <a href=\\\\\\"http://memcached.org/\\\\\\">Memcached</a>.</li><li>For the styling I've tried to stick to the<a href=\\\\\\"https://smacss.com/\\\\\\">\\\\u00a0SMACSS\\\\u00a0</a>methodology.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
213	f	2019-05-21 02:17:43.176016+00	{"date": "2015-06-22", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "What is this place?", "listing_intro": "<p>How this site was made, and what it was made with</p>", "pk": 4, "expired": false, "live_revision": null, "go_live_at": null, "path": "000100010001", "last_published_at": "2016-09-01T09:31:47.564Z", "first_published_at": "2015-06-22T12:48:56.803Z", "subtitle": "How this site was made, and what it was made with", "tagged_items": [], "seo_title": "", "show_in_menus": false, "title": "What is this place?", "body": "[{\\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v2.4)</a></li><li>It is hosted on a AWS t2.micro ec2 instance, running Ubuntu 16.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>The site is cached using <a href=\\\\\\"http://memcached.org/\\\\\\">Memcached</a>.</li><li>For the styling I&#x27;ve tried to stick to the<a href=\\\\\\"https://smacss.com/\\\\\\"> SMACSS</a> methodology.</li><li>It was originally deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\", \\"id\\": \\"450074c1-7864-431f-aaa3-b8272c3ee2ee\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "what-place", "search_description": "How this site was made, and what it was made with", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/what-place/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2016-09-01T09:31:47.564Z", "live": true}	\N	4	1
181	f	2016-09-01 09:31:34.002434+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "How this site was made, and what it was made with", "owner": 1, "intro": "", "latest_revision_created_at": "2016-05-30T11:10:57.210Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.6)</a><p><p></p><p>http://www.github.com/torchbox/wagtail</p></p>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>The site is cached using <a href=\\\\\\"http://memcached.org/\\\\\\">Memcached</a>.</li><li>For the styling I've tried to stick to the<a href=\\\\\\"https://smacss.com/\\\\\\">\\\\u00a0SMACSS\\\\u00a0</a>methodology.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
163	f	2016-08-28 09:43:45.335257+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T09:43:03.046Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
162	f	2016-08-28 09:43:03.046209+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T09:31:39.146Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
161	f	2016-08-28 09:31:39.146204+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T09:07:22.275Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
160	f	2016-08-28 09:07:22.275447+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T09:06:32.224Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
159	f	2016-08-28 09:06:32.224961+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T09:06:09.955Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"heading\\", \\"value\\": \\"Installing Zookeeper\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'])(https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
158	f	2016-08-28 09:06:09.955462+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T08:56:49.838Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"heading\\", \\"value\\": \\"Installing Zookeeper\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'])(https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
157	f	2016-08-28 08:56:49.838073+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T08:37:17.359Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>And that's it! We have our machines up and running. Next we need to install the relevant software.</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
156	f	2016-08-28 08:37:17.359833+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T08:35:13.973Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"config.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Each block in the code above is defining a separate virtual machine and giving each box a name. The `zoo1.vm.box` command is telling Vagrant which template to use for creating each box, and the `zoo1.vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
207	f	2019-04-30 13:13:42.285641+00	{"date": "2016-08-27", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Testing SolrCloud with Vagrant", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "pk": 23, "expired": false, "live_revision": 206, "go_live_at": null, "path": "00010001000G", "last_published_at": "2019-04-30T13:12:44.975Z", "first_published_at": "2016-08-31T15:57:58.430Z", "subtitle": "Easy steps to emulate a multi-machine setup locally", "tagged_items": [{"content_object": 23, "tag": 76, "pk": null}, {"content_object": 23, "tag": 77, "pk": null}, {"content_object": 23, "tag": 78, "pk": null}, {"content_object": 23, "tag": 63, "pk": null}], "seo_title": "Testing SolrCloud with Vagrant", "show_in_menus": false, "title": "Testing SolrCloud with Vagrant", "body": "[{\\"value\\": \\"<p>I&#x27;d been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn&#x27;t much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I&#x27;m going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distributions. I&#x27;m not a Windows user so you&#x27;ll have to do your own research if that&#x27;s what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The &#x27;Cloud&#x27; part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. Queries arrive at one of the nodes and the query is then forwarded to the node where a replica of the appropriate shard is located.</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I&#x27;m going to assume you are aware of the basics.</p><p>Our aim for this test is to have each element of the SolrCloud setup running on its own virtual machine. Our setup will have three Solr nodes with which we can store and query the data, and a single Zookeeper instance to manage the nodes. We could have multiple Zookeeper instances to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we&#x27;re going to stick with just the one.</p><p>Having three Solr nodes means that we can split our data into two shards with two replicas of each, and if one of the Solr nodes goes down we&#x27;ll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Our test network will be built using Ubuntu virtual machines. The first thing we&#x27;re going to do is create a new directory for our test VMs on our host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\", \\"id\\": \\"1ebd142a-84f7-4091-b466-a2a5f35d7e5b\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\"}, \\"id\\": \\"56c7cf60-a22a-4f40-8bcc-a80ed71895fb\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. I'm going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). So, we need  to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\", \\"id\\": \\"98d24dbb-a9f4-420a-938d-54ce844c019b\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"config.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\"}, \\"id\\": \\"a02ee92d-450b-4aa7-93b6-8d25a6eba49e\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\", \\"id\\": \\"5ed02442-b4c9-4f90-b906-b7bd8971a9be\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\"}, \\"id\\": \\"9ff9e702-22b3-4b2f-a3dc-f695acad6f3b\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The process of building the four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\", \\"id\\": \\"49257c11-ebf1-4e9c-abdc-580053599135\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\"}, \\"id\\": \\"bf12a1b4-011d-4b16-b593-c044d0bfc94f\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\", \\"id\\": \\"ebb36154-589f-4d1d-bb3f-5d3c53c903ff\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\"}, \\"id\\": \\"a7c14ea9-9146-437a-9543-d82995ec9b16\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If we run this command on each box, we should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\", \\"id\\": \\"0ed9675b-32ae-4e0f-b03b-6c20603c915e\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"ssh vagrant@172.28.128.4\\"}, \\"id\\": \\"a5763017-deef-4016-a54a-2c5fd6aa7ca7\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\", \\"id\\": \\"16f82195-41cf-4938-9439-fba31d853438\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we&#x27;re going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\", \\"id\\": \\"29e8e843-d444-4821-8df4-5c043f5556a4\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\"}, \\"id\\": \\"8cddfd50-aba7-4de7-83fc-1bd7f3b85d76\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle&#x27;s official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\", \\"id\\": \\"1a1f1530-7ce6-477d-9abe-b76aae8edaae\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": \\"<h2>Installing Zookeeper</h2>\\", \\"id\\": \\"907250f0-0090-49e8-b9b6-ab569008eb1b\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\", \\"id\\": \\"4d07ca3d-4882-4a29-9569-b63e249a2f3c\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\"}, \\"id\\": \\"81ae7536-2df1-4c67-be41-592ac029af5b\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\", \\"id\\": \\"eccafba7-ccf9-4256-bcf9-793c6fc28d44\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\"}, \\"id\\": \\"11d0dd08-e15e-443f-9163-91b194fff0f7\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Now copy the following three lines into that file and save it.\\", \\"id\\": \\"f946537a-f152-463e-a7ee-9b94503082b7\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\"}, \\"id\\": \\"e95324f9-720a-4d4c-91b1-58d44ca63945\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\", \\"id\\": \\"5b3b1d8c-c536-4257-925b-6e98dd9fcd08\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\"}, \\"id\\": \\"c9584d5b-68bf-4f31-a678-8eccf2160862\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\", \\"id\\": \\"3271c0b7-d9fa-4a8e-ae69-4d82004970c6\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\"}, \\"id\\": \\"4effec9c-fb7e-419a-985c-0646c4e5eafd\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\", \\"id\\": \\"4c97f7fc-eed4-4852-856f-fe2a66b72747\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\"}, \\"id\\": \\"6436cc2c-2e1d-4fc5-aeb7-60fa82be5fb1\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\", \\"id\\": \\"31cd0aa9-b7a9-4b4b-a3da-7db5d15e7dfd\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\"}, \\"id\\": \\"f7f6949a-c5f9-4aab-9a57-ab60c4e94ba4\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node we currently have running.\\", \\"id\\": \\"b3006f8d-ead9-49bb-88f2-5d8d6dcd065f\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr stop\\"}, \\"id\\": \\"3523c902-135a-4429-8ef1-e59f5a7cf8e9\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Then we restart in cloud mode with the following command:\\", \\"id\\": \\"57e87a15-9b69-458e-81ff-900cfbfc967f\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\"}, \\"id\\": \\"ab95c913-da3b-432e-b84f-b1a71f5a62d1\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\", \\"id\\": \\"fa1e9d45-627d-4290-b681-98acf7b8d380\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\", \\"id\\": \\"511a0930-15e8-4e0a-82a4-32cd0264743a\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Creating a test collection</h2><p>A &#x27;Collection&#x27; in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\", \\"id\\": \\"a693e420-4a1e-4d4d-8535-e986c1df0031\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 -replicationFactor 2\\"}, \\"id\\": \\"66f233de-d788-43e9-a77e-aa2ecaea7f8a\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"I'm not going to go into great detail on how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\", \\"id\\": \\"6bc42cef-db3f-40b0-97b0-2c00e944a631\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the &#x27;Cloud&#x27; section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p>There you have it, a working SolrCloud setup using Vagrant. We&#x27;ve got no data in our test collection, but adding in data isn&#x27;t SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\", \\"id\\": \\"999c03d5-8c5d-4ffa-89b0-246414e750d2\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "testing-solrcloud-vagrant", "search_description": "Easy steps to emulate a multi-machine setup locally", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/testing-solrcloud-vagrant/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2019-04-30T13:12:44.945Z", "live": true}	\N	23	1
215	f	2019-06-27 18:26:24.639454+00	{"date": "2019-06-27", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Thoughts on cloud providers in the classroo", "listing_intro": "<p></p>", "pk": 26, "expired": false, "live_revision": 214, "go_live_at": null, "path": "00010001000J", "last_published_at": "2019-06-27T18:26:09.406Z", "first_published_at": "2019-06-27T18:26:09.406Z", "subtitle": null, "tagged_items": [], "seo_title": "", "show_in_menus": false, "title": "Thoughts on cloud providers in the classroom", "body": "[{\\"value\\": \\"<p></p><p>Over this time I have found that there are a few principles that cloud providers (e.g. AWS and GCP) who are hoping to penetrate the higher education market would do well to adhere to.</p><p>If cloud providers follow these principles when developing their tools and services for educational use then I believe they have a much greater chance of being adopted in the classroom.</p><h2><b>Most CS courses teach concepts, not tools</b></h2><p>The aim of most courses that utilize the cloud is not to teach the use of a particular tool or platform, but to teach a computing concept and to demonstrate its application.</p><p>When teaching these concepts, teaching time is invaluable. If you spend 2 weeks trying to make sure everybody in a class has an account and basic knowledge of how to use your platform, then that time is lost and cannot be made up.</p><p>For this reason, onboarding for the tool should be as seamless as possible. There should be a very clear pathway for students to learn how to navigate your platform. A dedicated set of short videos aimed at students that could be assigned as a homework 0 task would be ideal.</p><h2><b>It must be easy for TFs and other teaching staff to support their students</b></h2><p>Consider integrations with common LMS platforms to simplify adding students to resources and easily enable SSO.</p><p>Provide an administrative interface from which teaching staff can monitor students\\\\u2019 spend.</p><h2><b>Be generous with the amount of credit provided to each student.</b></h2><p>Every semester we see students using advanced, CPU and memory intensive operations earlier and earlier in their academic journey.</p><p>Many entry level tasks these days involve GPUs which cost upwards of 1$ per hour. It is not unreasonable to expect a student to use 200 - 300 hours of GPU time over a 4 month course. So, credit limits should be flexible.</p><p>It should be very easy for teaching staff, or even students themselves, to extend their credit limit if necessary.</p><h2><b>Students should never be charged</b></h2><p>I personally believe that asking students to sign-up with a credit card is potentially discriminatory.</p><p>In the past, we have seen students overspend on their accounts (for example, by leaving a GPU running, unused). This has led to many hours of teaching staff negotiating on their behalf to have bills cancelled. Making it as easy as possible for students and teaching staff to monitor student spend would help here.</p><p>Tools should also enable limiting the resources that a student can use, but these limits should be flexible or customizable.</p>\\", \\"id\\": \\"9ff852dd-1f32-41f8-9664-7e560501ac8b\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "thoughts-cloud-providers-classroom", "search_description": "", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/thoughts-cloud-providers-classroom/", "numchild": 0, "locked": false, "intro": "Over the last two years, I\\u2019ve been working with Harvard SEAS faculty and cloud providers to determine how we can best facilitate students use of cloud technologies in the classroom.", "latest_revision_created_at": "2019-06-27T18:26:09.388Z", "live": true}	\N	26	1
214	f	2019-06-27 18:26:09.388456+00	{"date": "2019-06-27", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Thoughts on cloud providers in the classroo", "listing_intro": "<p></p>", "pk": 26, "expired": false, "live_revision": null, "go_live_at": null, "path": "00010001000J", "last_published_at": null, "first_published_at": null, "subtitle": null, "tagged_items": [], "seo_title": "", "show_in_menus": false, "title": "Thoughts on cloud providers in the classroo", "body": "[{\\"value\\": \\"<p></p><p>Over this time I have found that there are a few principles that cloud providers (e.g. AWS and GCP) who are hoping to penetrate the higher education market would do well to adhere to. </p><p>If cloud providers follow these principles when developing their tools and services for educational use then I believe they have a much greater chance of being adopted in the classroom. </p><h2><b>Most CS courses teach concepts, not tools</b></h2><p>The aim of most courses that utilize the cloud is not to teach the use of a particular tool or platform, but to teach a computing concept and to demonstrate its application. </p><p>When teaching these concepts, teaching time is invaluable. If you spend 2 weeks trying to make sure everybody in a class has an account and basic knowledge of how to use your platform, then that time is lost and cannot be made up. </p><p>For this reason, onboarding for the tool should be as seamless as possible. There should be a very clear pathway for students to learn how to navigate your platform. A dedicated set of short videos aimed at students that could be assigned as a homework 0 task would be ideal. </p><h2><b>It must be easy for TFs and other teaching staff to support their students</b></h2><p>Consider integrations with common LMS platforms to simplify adding students to resources and easily enable SSO. </p><p>Provide an administrative interface from which teaching staff can monitor students\\\\u2019 spend. </p><h2><b>Be generous with the amount of credit provided to each student.</b></h2><p>Every semester we see students using advanced, CPU and memory intensive operations earlier and earlier in their academic journey. </p><p>Many entry level tasks these days involve GPUs which cost upwards of 1$ per hour. It is not unreasonable to expect a student to use 200 - 300 hours of GPU time over a 4 month course. So, credit limits should be flexible. </p><p>It should be very easy for teaching staff, or even students themselves, to extend their credit limit if necessary. </p><h2><b>Students should never be charged</b></h2><p>I personally believe that asking students to sign-up with a credit card is potentially discriminatory. </p><p>In the past, we have seen students overspend on their accounts (for example, by leaving a GPU running, unused). This has led to many hours of teaching staff negotiating on their behalf to have bills cancelled. Making it as easy as possible for students and teaching staff to monitor student spend would help here. </p><p>Tools should also enable limiting the resources that a student can use, but these limits should be flexible or customizable. </p>\\", \\"id\\": \\"9ff852dd-1f32-41f8-9664-7e560501ac8b\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "thoughts-cloud-providers-classroom", "search_description": "", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/thoughts-cloud-providers-classroom/", "numchild": 0, "locked": false, "intro": "Over the last two years, I\\u2019ve been working with Harvard SEAS faculty and cloud providers to determine how we can best facilitate students use of cloud technologies in the classroom.", "latest_revision_created_at": null, "live": true}	\N	26	1
203	f	2019-04-30 13:07:36.755571+00	{"date": "2019-04-30", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Where is my added value?", "listing_intro": "<p></p>", "pk": 25, "expired": false, "live_revision": null, "go_live_at": null, "path": "00010001000I", "last_published_at": null, "first_published_at": null, "subtitle": null, "tagged_items": [{"content_object": 25, "tag": 46, "pk": null}], "seo_title": "", "show_in_menus": false, "title": "Where is my added value?", "body": "[{\\"value\\": \\"<p>As a project manager, getting things done quicker is often the thing that&#x27;s most on my mind. This is great and as it should be!</p><p>However, as someone who likes to get his hands dirty, I&#x27;m often tempted to get into the nitty gritty of the work, to try to move things along faster. This almost always actually leads to projects going slower!</p><p>Whilst having the technical expertise that I have is a great advantage as a PM, it does not make me a professional developer. By switching my focus away from facilitating the work of others, I&#x27;m actually reducing their potential productivity. And one developer at full productivity is going to produce a bunch more quality work than me trying my best!</p><p>So, nowadays I like to say that the less I need to get involved in development, the more successful the project is. I know that my added value is in my ability to see the big picture, tying people and ideas together, motivating a team, and keeping a focus on customer needs.</p>\\", \\"id\\": \\"31333200-43d1-4217-aa54-9237cc62c961\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "where-my-added-value", "search_description": "", "content_type": 27, "has_unpublished_changes": true, "owner": 1, "url_path": "/home/where-my-added-value/", "numchild": 0, "locked": false, "intro": "Recently I've been thinking about where my added value as a technical project manager lies.", "latest_revision_created_at": "2019-04-30T13:07:32.275Z", "live": false}	\N	25	1
202	f	2019-04-30 13:07:32.275748+00	{"date": "2019-04-30", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Where is my added value?", "listing_intro": "<p></p>", "pk": 25, "expired": false, "live_revision": null, "go_live_at": null, "path": "00010001000I", "last_published_at": null, "first_published_at": null, "subtitle": null, "tagged_items": [{"content_object": 25, "tag": 46, "pk": 118}], "seo_title": "", "show_in_menus": false, "title": "Where is my added value?", "body": "[{\\"value\\": \\"<p>As a project manager, getting things done quicker is often the thing that&#x27;s most on my mind. This is great and as it should be!  </p><p>However, as someone who likes to get his hands dirty, I&#x27;m often tempted to get into the nitty gritty of the work, to try to move things along faster. This almost always actually leads to projects going slower!  </p><p>Whilst having the technical expertise that I have is a great advantage as a PM, it does not make me a professional developer. By switching my focus away from facilitating the work of others, I&#x27;m actually reducing their potential productivity. And one developer at full productivity is going to produce a bunch more quality work than me trying my best!  </p><p>So, nowadays I like to say that the less I need to get involved in development, the more successful the project is. I know that my added value is in my ability to see the big picture, tying people and ideas together, motivating a team, and keeping a focus on customer needs.</p>\\", \\"id\\": \\"31333200-43d1-4217-aa54-9237cc62c961\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "where-my-added-value", "search_description": "", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/where-my-added-value/", "numchild": 0, "locked": false, "intro": "Recently I've been thinking about where my added value as a technical project manager lies.", "latest_revision_created_at": null, "live": false}	\N	25	1
204	f	2019-04-30 13:09:13.423651+00	{"date": "2019-04-30", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Where is my added value?", "listing_intro": "<p>Recently I&#x27;ve been thinking about where my added value as a technical project manager lies.</p>", "pk": 25, "expired": false, "live_revision": 203, "go_live_at": null, "path": "00010001000I", "last_published_at": "2019-04-30T13:07:36.774Z", "first_published_at": "2019-04-30T13:07:36.774Z", "subtitle": null, "tagged_items": [{"content_object": 25, "tag": 46, "pk": null}], "seo_title": "", "show_in_menus": false, "title": "Where is my added value?", "body": "[{\\"value\\": \\"<p>As a project manager, getting things done quicker is often the thing that&#x27;s most on my mind. This is great and as it should be!</p><p>However, as someone who likes to get his hands dirty, I&#x27;m often tempted to get into the nitty gritty of the work, to try to move things along faster. This almost always actually leads to projects going slower!</p><p>Whilst having the technical expertise that I have is a great advantage as a PM, it does not make me a professional developer. By switching my focus away from facilitating the work of others, I&#x27;m actually reducing their potential productivity. And one developer at full productivity is going to produce a bunch more quality work than me trying my best!</p><p>So, nowadays I like to say that the less I need to get involved in development, the more successful the project is. I know that my added value is in my ability to see the big picture, tying people and ideas together, motivating a team, and keeping a focus on customer needs.</p>\\", \\"id\\": \\"31333200-43d1-4217-aa54-9237cc62c961\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "where-my-added-value", "search_description": "Recently I've been thinking about where my added value as a technical project manager lies.", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/where-my-added-value/", "numchild": 0, "locked": false, "intro": "Recently I've been thinking about where my added value as a technical project manager lies.", "latest_revision_created_at": "2019-04-30T13:07:36.755Z", "live": true}	\N	25	1
167	f	2016-08-29 17:44:18.624472+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T10:15:19.404Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nsudo bin/solr start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine.\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
166	f	2016-08-28 10:15:19.404899+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T10:07:32.841Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p><br/></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
135	f	2016-04-01 11:52:15.10818+00	{"subtitle": "About me!", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Chris Rogers", "seo_title": "", "listing_intro": "", "slug": "chris-rogers", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently working at the Bodleian Library, part of Oxford University as the Digital Projects Manager. Previously I've worked as a project manager for the digital agency <a href=\\\\\\"http://torchbox.com\\\\\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><p></p><ul><li><a href=\\\\\\"http://rca.ac.uk\\\\\\">The Royal College of Art</a></li><li><a href=\\\\\\"http://royaldrawingschool.org\\\\\\">The Royal Drawing School</a></li><li><a href=\\\\\\"http://globalwitness.org.uk\\\\\\">Global Witness</a></li><li><a href=\\\\\\"http://plan-international.org.uk\\\\\\">Plan International</a></li><li><a href=\\\\\\"http://events.burton.com/\\\\\\">Burton Snowboards</a></li><li><a href=\\\\\\"http://election.kingsfund.org.uk/\\\\\\">The King's Fund</a></li></ul><p>In my spare time I'm a regular contributor to the <a href=\\\\\\"https://wagtail.io\\\\\\">Wagtail CMS</a> open source project.\\\\u00a0I'm also a skateboarder of 16+ years, and a runner of 5.</p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-10-01", "path": "00010001000E", "url_path": "/home/chris-rogers/", "expired": false, "pk": 21, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	21	1
208	f	2019-04-30 13:16:07.503378+00	{"date": "2016-08-27", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Testing SolrCloud with Vagrant", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "pk": 23, "expired": false, "live_revision": 207, "go_live_at": null, "path": "00010001000G", "last_published_at": "2019-04-30T13:13:42.318Z", "first_published_at": "2016-08-31T15:57:58.430Z", "subtitle": "Easy steps to emulate a multi-machine setup locally", "tagged_items": [{"content_object": 23, "tag": 76, "pk": null}, {"content_object": 23, "tag": 77, "pk": null}, {"content_object": 23, "tag": 78, "pk": null}, {"content_object": 23, "tag": 63, "pk": null}], "seo_title": "Testing SolrCloud with Vagrant", "show_in_menus": false, "title": "Testing SolrCloud with Vagrant", "body": "[{\\"value\\": \\"<p>I&#x27;d been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn&#x27;t much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I&#x27;m going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distributions. I&#x27;m not a Windows user so you&#x27;ll have to do your own research if that&#x27;s what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The &#x27;Cloud&#x27; part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. Queries arrive at one of the nodes and the query is then forwarded to the node where a replica of the appropriate shard is located.</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I&#x27;m going to assume you are aware of the basics.</p><p>Our aim for this test is to have each element of the SolrCloud setup running on its own virtual machine. Our setup will have three Solr nodes with which we can store and query the data, and a single Zookeeper instance to manage the nodes. We could have multiple Zookeeper instances to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we&#x27;re going to stick with just the one.</p><p>Having three Solr nodes means that we can split our data into two shards with two replicas of each, and if one of the Solr nodes goes down we&#x27;ll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Our test network will be built using Ubuntu virtual machines. The first thing we&#x27;re going to do is create a new directory for our test VMs on our host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\", \\"id\\": \\"1ebd142a-84f7-4091-b466-a2a5f35d7e5b\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\"}, \\"id\\": \\"56c7cf60-a22a-4f40-8bcc-a80ed71895fb\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. I'm going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). So, we need  to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\", \\"id\\": \\"98d24dbb-a9f4-420a-938d-54ce844c019b\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"config.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\"}, \\"id\\": \\"a02ee92d-450b-4aa7-93b6-8d25a6eba49e\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\", \\"id\\": \\"5ed02442-b4c9-4f90-b906-b7bd8971a9be\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\"}, \\"id\\": \\"9ff9e702-22b3-4b2f-a3dc-f695acad6f3b\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The process of building the four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\", \\"id\\": \\"49257c11-ebf1-4e9c-abdc-580053599135\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\"}, \\"id\\": \\"bf12a1b4-011d-4b16-b593-c044d0bfc94f\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\", \\"id\\": \\"ebb36154-589f-4d1d-bb3f-5d3c53c903ff\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\"}, \\"id\\": \\"a7c14ea9-9146-437a-9543-d82995ec9b16\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If we run this command on each box, we should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\", \\"id\\": \\"0ed9675b-32ae-4e0f-b03b-6c20603c915e\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"ssh vagrant@172.28.128.4\\"}, \\"id\\": \\"a5763017-deef-4016-a54a-2c5fd6aa7ca7\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\", \\"id\\": \\"16f82195-41cf-4938-9439-fba31d853438\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we&#x27;re going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\", \\"id\\": \\"29e8e843-d444-4821-8df4-5c043f5556a4\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\"}, \\"id\\": \\"8cddfd50-aba7-4de7-83fc-1bd7f3b85d76\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle&#x27;s official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\", \\"id\\": \\"1a1f1530-7ce6-477d-9abe-b76aae8edaae\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": \\"<h2>Installing Zookeeper</h2>\\", \\"id\\": \\"907250f0-0090-49e8-b9b6-ab569008eb1b\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\", \\"id\\": \\"4d07ca3d-4882-4a29-9569-b63e249a2f3c\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\"}, \\"id\\": \\"81ae7536-2df1-4c67-be41-592ac029af5b\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\", \\"id\\": \\"eccafba7-ccf9-4256-bcf9-793c6fc28d44\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\"}, \\"id\\": \\"11d0dd08-e15e-443f-9163-91b194fff0f7\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Now copy the following three lines into that file and save it.\\", \\"id\\": \\"f946537a-f152-463e-a7ee-9b94503082b7\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\"}, \\"id\\": \\"e95324f9-720a-4d4c-91b1-58d44ca63945\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\", \\"id\\": \\"5b3b1d8c-c536-4257-925b-6e98dd9fcd08\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\"}, \\"id\\": \\"c9584d5b-68bf-4f31-a678-8eccf2160862\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\", \\"id\\": \\"3271c0b7-d9fa-4a8e-ae69-4d82004970c6\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\"}, \\"id\\": \\"4effec9c-fb7e-419a-985c-0646c4e5eafd\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\", \\"id\\": \\"4c97f7fc-eed4-4852-856f-fe2a66b72747\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\"}, \\"id\\": \\"6436cc2c-2e1d-4fc5-aeb7-60fa82be5fb1\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\", \\"id\\": \\"31cd0aa9-b7a9-4b4b-a3da-7db5d15e7dfd\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\"}, \\"id\\": \\"f7f6949a-c5f9-4aab-9a57-ab60c4e94ba4\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node we currently have running.\\", \\"id\\": \\"b3006f8d-ead9-49bb-88f2-5d8d6dcd065f\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr stop\\"}, \\"id\\": \\"3523c902-135a-4429-8ef1-e59f5a7cf8e9\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Then we restart in cloud mode with the following command:\\", \\"id\\": \\"57e87a15-9b69-458e-81ff-900cfbfc967f\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\"}, \\"id\\": \\"ab95c913-da3b-432e-b84f-b1a71f5a62d1\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\", \\"id\\": \\"fa1e9d45-627d-4290-b681-98acf7b8d380\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\", \\"id\\": \\"511a0930-15e8-4e0a-82a4-32cd0264743a\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Creating a test collection</h2><p>A &#x27;Collection&#x27; in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\", \\"id\\": \\"a693e420-4a1e-4d4d-8535-e986c1df0031\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 -replicationFactor 2\\"}, \\"id\\": \\"66f233de-d788-43e9-a77e-aa2ecaea7f8a\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"I'm not going to go into great detail on how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\", \\"id\\": \\"6bc42cef-db3f-40b0-97b0-2c00e944a631\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the &#x27;Cloud&#x27; section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p>There you have it, a working SolrCloud setup using Vagrant. We&#x27;ve got no data in our test collection, but adding in data isn&#x27;t SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\", \\"id\\": \\"999c03d5-8c5d-4ffa-89b0-246414e750d2\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "testing-solrcloud-vagrant", "search_description": "Easy steps to emulate a multi-machine setup locally", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/testing-solrcloud-vagrant/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2019-04-30T13:13:42.285Z", "live": true}	\N	23	1
206	f	2019-04-30 13:12:44.945191+00	{"date": "2016-08-27", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "Testing SolrCloud with Vagrant", "listing_intro": "<p>Easy steps to emulate a multi-machine setup locally</p>", "pk": 23, "expired": false, "live_revision": null, "go_live_at": null, "path": "00010001000G", "last_published_at": "2016-09-06T06:58:13.438Z", "first_published_at": "2016-08-31T15:57:58.430Z", "subtitle": "Easy steps to emulate a multi-machine setup locally", "tagged_items": [{"content_object": 23, "tag": 76, "pk": null}, {"content_object": 23, "tag": 77, "pk": null}, {"content_object": 23, "tag": 78, "pk": null}, {"content_object": 23, "tag": 63, "pk": null}], "seo_title": "Testing SolrCloud with Vagrant", "show_in_menus": false, "title": "Testing SolrCloud with Vagrant", "body": "[{\\"value\\": \\"<p>I&#x27;d been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn&#x27;t much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I&#x27;m going to take you through the steps to do just that.</p><h2>Requirements</h2><p>I used OSX to create this setup, but it should also work on major Linux distributions. I&#x27;m not a Windows user so you&#x27;ll have to do your own research if that&#x27;s what you need.</p><p>You will need <a href=\\\\\\"https://www.vagrantup.com/\\\\\\">Vagrant</a> and <a href=\\\\\\"https://www.virtualbox.org\\\\\\">VirtualBox</a> installed on your host machine.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The &#x27;Cloud&#x27; part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy. Queries arrive at one of the nodes and the query is then forwarded to the node where a replica of the appropriate shard is located.</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I&#x27;m going to assume you are aware of the basics.</p><p>Our aim for this test is to have each element of the SolrCloud setup running on its own virtual machine. Our setup will have three Solr nodes with which we can store and query the data, and a single Zookeeper instance to manage the nodes. We could have multiple Zookeeper instances to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we&#x27;re going to stick with just the one.</p><p>Having three Solr nodes means that we can split our data into two shards with two replicas of each, and if one of the Solr nodes goes down we&#x27;ll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Our test network will be built using Ubuntu virtual machines. The first thing we&#x27;re going to do is create a new directory for our test VMs on our host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\", \\"id\\": \\"1ebd142a-84f7-4091-b466-a2a5f35d7e5b\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\"}, \\"id\\": \\"56c7cf60-a22a-4f40-8bcc-a80ed71895fb\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. I'm going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). So, we need  to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\", \\"id\\": \\"98d24dbb-a9f4-420a-938d-54ce844c019b\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"config.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\"}, \\"id\\": \\"a02ee92d-450b-4aa7-93b6-8d25a6eba49e\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow let's get these Vagrant boxes running.\\", \\"id\\": \\"5ed02442-b4c9-4f90-b906-b7bd8971a9be\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"cd ~/solrcloud-test\\\\r\\\\nvagrant up\\"}, \\"id\\": \\"9ff9e702-22b3-4b2f-a3dc-f695acad6f3b\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The process of building the four Vagrant boxes will begin. This could take a few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\", \\"id\\": \\"49257c11-ebf1-4e9c-abdc-580053599135\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\"}, \\"id\\": \\"bf12a1b4-011d-4b16-b593-c044d0bfc94f\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\", \\"id\\": \\"ebb36154-589f-4d1d-bb3f-5d3c53c903ff\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\"}, \\"id\\": \\"a7c14ea9-9146-437a-9543-d82995ec9b16\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If we run this command on each box, we should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\", \\"id\\": \\"0ed9675b-32ae-4e0f-b03b-6c20603c915e\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"ssh vagrant@172.28.128.4\\"}, \\"id\\": \\"a5763017-deef-4016-a54a-2c5fd6aa7ca7\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\", \\"id\\": \\"16f82195-41cf-4938-9439-fba31d853438\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we&#x27;re going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\", \\"id\\": \\"29e8e843-d444-4821-8df4-5c043f5556a4\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\"}, \\"id\\": \\"8cddfd50-aba7-4de7-83fc-1bd7f3b85d76\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle&#x27;s official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\", \\"id\\": \\"1a1f1530-7ce6-477d-9abe-b76aae8edaae\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\", \\"id\\": \\"907250f0-0090-49e8-b9b6-ab569008eb1b\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\", \\"id\\": \\"4d07ca3d-4882-4a29-9569-b63e249a2f3c\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\"}, \\"id\\": \\"81ae7536-2df1-4c67-be41-592ac029af5b\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\", \\"id\\": \\"eccafba7-ccf9-4256-bcf9-793c6fc28d44\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\"}, \\"id\\": \\"11d0dd08-e15e-443f-9163-91b194fff0f7\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Now copy the following three lines into that file and save it.\\", \\"id\\": \\"f946537a-f152-463e-a7ee-9b94503082b7\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\"}, \\"id\\": \\"e95324f9-720a-4d4c-91b1-58d44ca63945\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\", \\"id\\": \\"5b3b1d8c-c536-4257-925b-6e98dd9fcd08\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\"}, \\"id\\": \\"c9584d5b-68bf-4f31-a678-8eccf2160862\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>If all has gone well, you should see the following output in your terminal:</p>\\", \\"id\\": \\"3271c0b7-d9fa-4a8e-ae69-4d82004970c6\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"ZooKeeper JMX enabled by default\\\\r\\\\nUsing config: /home/vagrant/zookeeper-3.4.8/bin/../conf/zoo.cfg\\\\r\\\\nStarting zookeeper ... STARTED\\"}, \\"id\\": \\"4effec9c-fb7e-419a-985c-0646c4e5eafd\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p>We now need to install our three instances of Solr. Like Zookeeper, we need to download a distribution from the Apache Solr website, and unpack it.</p>\\", \\"id\\": \\"4c97f7fc-eed4-4852-856f-fe2a66b72747\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"curl -O http://mirrors.muzzy.org.uk/apache/lucene/solr/6.2.0/solr-6.2.0.tgz\\\\r\\\\ntar -xzf solr-6.2.0.tgz\\"}, \\"id\\": \\"6436cc2c-2e1d-4fc5-aeb7-60fa82be5fb1\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"<p>To test everything is working, try starting Solr in basic standalone mode using the Solr start script provided in the distribution.</p>\\", \\"id\\": \\"31cd0aa9-b7a9-4b4b-a3da-7db5d15e7dfd\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"cd ~/solr-6.2.0\\\\r\\\\nbin/solr start\\"}, \\"id\\": \\"f7f6949a-c5f9-4aab-9a57-ab60c4e94ba4\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Then visit your VM's IP in your host machines browser, appending `:8983/solr` to the end. So for example, `http://172.28.128.4:8983/solr`. If all is successful you should see the Solr admin.\\\\r\\\\n\\\\r\\\\nHowever, we don't want these Solr instances to run in standalone mode, we want them to run in cloud mode. This is just as easy, you just need to know the IP for your Zookeeper machine, and the IP of each connecting Solr VM.\\\\r\\\\n\\\\r\\\\nThe first thing to do is stop the node we currently have running.\\", \\"id\\": \\"b3006f8d-ead9-49bb-88f2-5d8d6dcd065f\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr stop\\"}, \\"id\\": \\"3523c902-135a-4429-8ef1-e59f5a7cf8e9\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Then we restart in cloud mode with the following command:\\", \\"id\\": \\"57e87a15-9b69-458e-81ff-900cfbfc967f\\", \\"type\\": \\"markdown\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr start -c -z 172.28.128.3:2181 -h 172.28.128.4:8983\\"}, \\"id\\": \\"ab95c913-da3b-432e-b84f-b1a71f5a62d1\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"Let's break down the elements of this command:\\\\r\\\\n\\\\r\\\\n- `bin/solr start -c`: This is the familiar start command, with the '-c' modifier which is a shortened version of `-cloud`.\\\\r\\\\n\\\\r\\\\n- `-z 172.28.128.3:2181`: the `-z` modifier instructs Solr to connect to a Zookeeper instance with the following IP and port number.\\\\r\\\\n\\\\r\\\\n- `-h 172.28.128.4:8983`: this defines the hostname and port to start Solr with. This should be set to the specific Solr machine's IP. The port can be anything that doesn't clash with something else, but I'd suggest sticking with the default Solr port of 8983.\\", \\"id\\": \\"fa1e9d45-627d-4290-b681-98acf7b8d380\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"After running this command, you should be able to go to your Solr admin for that node (e.g. http://172.28.128.4:8983/solr/), and you should see the 'Cloud' option in the left-hand menu. If you click this, currently you should only see a blank white area, with a key in the bottom right. For anything to display in this section we need to upload a 'Collection'.\\", \\"id\\": \\"511a0930-15e8-4e0a-82a4-32cd0264743a\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Creating a test collection</h2><p>A &#x27;Collection&#x27; in SolrCloud is the equivalent of a Solr core in standalone mode. We can easily create a simple collection with the following command, run from the root folder of one of your Solr nodes:</p>\\", \\"id\\": \\"a693e420-4a1e-4d4d-8535-e986c1df0031\\", \\"type\\": \\"paragraph\\"}, {\\"value\\": {\\"language\\": \\"bash\\", \\"code\\": \\"bin/solr create -c testCollection -d  data_driven_schema_configs -n testCollection_cfg -shards 2 -replicationFactor 2\\"}, \\"id\\": \\"66f233de-d788-43e9-a77e-aa2ecaea7f8a\\", \\"type\\": \\"real_codeblock\\"}, {\\"value\\": \\"I'm not going to go into great detail on how to create Collections in this blog post, but here's a quick breakdown of the command we've just run:\\\\r\\\\n\\\\r\\\\n- `bin/solr create -c testCollection` : The create command followed by the `-c` modifier which defines the name of the new collection.\\\\r\\\\n\\\\r\\\\n- `-d data_driven_schema_configs` : The `-d` modifier is required to set the config directory for the Collection. This config is uploaded to Zookeeper, which then shares it with the other Solr nodes. In this example I've used `data_driven_schema_configs`, which is one of the example config sets. The default directory in which the Solr create command will look for the config is `/solr-6.2.0/server/solr/configsets/`. If you want to create your own config, you can copy one of the example config sets into a new folder, then provide a relative path to that folder instead. For example, if running from the root directory of your Solr install `server/solr/testCollectionConf/conf`.\\\\r\\\\n\\\\r\\\\n- `-shards 3` : This defines how many shards the Collection should be split into.\\\\r\\\\n\\\\r\\\\n- `-replicationFactor 3` : This defines how many replicas of each Shard are created.\\\\r\\\\n\\\\r\\\\nFor more info on the usage of the 'create' command, [see the Solr docs](\\\\\\"https://cwiki.apache.org/confluence/display/solr/Solr+Start+Script+Reference#SolrStartScriptReference-CollectionsandCores\\\\\\").\\", \\"id\\": \\"6bc42cef-db3f-40b0-97b0-2c00e944a631\\", \\"type\\": \\"markdown\\"}, {\\"value\\": \\"<h2>Conclusion</h2><p>So now if you go to the &#x27;Cloud&#x27; section of your Solr admin on any of your connected nodes, you should now see a graph with your collection name on the left, the split of your shards in the middle, and the locations of the replicas of these shards on the right.</p><p>There you have it, a working SolrCloud setup using Vagrant. We&#x27;ve got no data in our test collection, but adding in data isn&#x27;t SolrCloud specific. You can use any method for pushing in data that you would use when using Solr in standalone mode.</p>\\", \\"id\\": \\"999c03d5-8c5d-4ffa-89b0-246414e750d2\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "testing-solrcloud-vagrant", "search_description": "Easy steps to emulate a multi-machine setup locally", "content_type": 27, "has_unpublished_changes": true, "owner": 1, "url_path": "/home/testing-solrcloud-vagrant/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2019-04-30T13:11:43.503Z", "live": true}	\N	23	1
144	f	2016-08-27 17:12:09.318982+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-27T16:38:21.976Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used Vagrant to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard can also be replicated on different nodes to provide redundancy.</p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three solr nodes means that I can split my data into two shards, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>Vagrant\\\\u00a0</p><p>\\\\u00a0</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
143	f	2016-08-27 16:38:21.976101+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": null, "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [{"pk": 82, "tag": 76, "content_object": 23}, {"pk": 83, "tag": 77, "content_object": 23}, {"pk": 84, "tag": 78, "content_object": 23}, {"pk": 85, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used Vagrant to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><p>\\\\u00a0</p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
165	f	2016-08-28 10:07:32.841287+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T10:04:44.947Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in the configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Solr and starting in cloud mode</h2><p><br/></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
164	f	2016-08-28 10:04:44.947065+00	{"subtitle": "Easy steps to emulate a multi-machine setup locally", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-08-28T09:43:45.335Z", "go_live_at": null, "title": "Testing SolrCloud with Vagrant", "seo_title": "", "listing_intro": "", "slug": "testing-solrcloud-vagrant", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [{"pk": null, "tag": 76, "content_object": 23}, {"pk": null, "tag": 77, "content_object": 23}, {"pk": null, "tag": 78, "content_object": 23}, {"pk": null, "tag": 63, "content_object": 23}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I've been thinking about using SolrCloud for a project at work recently, and I wanted to test it out locally. To do this I used <a href=\\\\\\"https://www.vagrantup.com/docs/why-vagrant/\\\\\\">Vagrant</a> to setup a multi-machine private network with static IPs.</p><p>Getting the machines setup was much easier than I expected, and then getting SolrCloud working wasn't much more complicated. However, there does appear to be a lack of good examples of exactly how to get something like this going, so in this blog post I'm going to take you through the steps to do just that.</p><h2>Designing the setup</h2><p>A SolrCloud setup has two types of component:</p><p></p><ol><li>One or more <a href=\\\\\\"https://zookeeper.apache.org/\\\\\\">Apache Zookeeper</a> instances to manage the distribution of data across the Solr cloud, and the configuration and administration of the Solr nodes.</li><li>One or more Solr nodes on which to store your data and perform your queries.</li></ol><p>The 'Cloud' part of SolrCloud comes from the fact that any data you push into your set of Solr nodes can be split into shards and distributed across the nodes. Each shard is then replicated one or more times on different nodes to provide redundancy.\\\\u00a0</p><p>The amount of shards into which the data is split, and the amount of replicas for each shard is set at the point of creating a collection (more about this later).</p><p></p><p>You can find a basic introduction to how SolrCloud <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works\\\\\\">works on the Solr wiki</a>. For the rest of this article I'm going to assume you are aware of the basics.</p><p>My aim for this test is to have each element of the SolrCloud setup running on its own machine. My setup will have three Solr nodes with which I can store and query the data, and a single Zookeeper instance to manage the nodes. You could have multiple Zookeeper instance to provide further redundancy. This would be called a <a href=\\\\\\"https://cwiki.apache.org/confluence/display/solr/Setting+Up+an+External+ZooKeeper+Ensemble\\\\\\">Zookeeper ensemble</a>. However, for this initial test we're going to stick with just the one.</p><p>Having three Solr nodes means that I can split my data into two shards with two replicas of each, and if one of the Solr nodes goes down I'll still be able to access all of the data.</p><h2>Setting up the machines</h2><p>I'm building my test network using Ubuntu virtual machines. The first thing I'm going to do is create a new directoy for my test VMs on my host machine, and then generate a Vagrant file including the Ubuntu Trusty64 Vagrant box.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"mkdir -p ~/solrcloud-test\\\\r\\\\ncd ~/solrcloud-test\\\\r\\\\nvagrant init ubuntu/trusty64\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"This will generate a file called `Vagrantfile` which includes the instructions for Vagrant to build a basic Ubuntu VM. We're going to use this file to create all four of the necessary VMs for our test. Vagrant includes the ability to [create multi-machine setups out of the box](https://www.vagrantup.com/docs/multi-machine/). I'm going to open the Vagrantfile and replace the line `config.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"` with the instructions below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"\\\\r\\\\nconfig.vm.provider \\\\\\"virtualbox\\\\\\" do |v|\\\\r\\\\n  v.memory = 1024\\\\r\\\\n  v.cpus = 2\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"zoo1\\\\\\" do |zoo1|\\\\r\\\\n  zoo1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  zoo1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr1\\\\\\" do |solr1|\\\\r\\\\n  solr1.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr1.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr2\\\\\\" do |solr2|\\\\r\\\\n  solr2.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr2.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\\\r\\\\nconfig.vm.define \\\\\\"solr3\\\\\\" do |solr3|\\\\r\\\\n  solr3.vm.box = \\\\\\"ubuntu/trusty64\\\\\\"\\\\r\\\\n  solr3.vm.network \\\\\\"private_network\\\\\\", type: \\\\\\"dhcp\\\\\\"\\\\r\\\\nend\\\\r\\\\n\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The first block in configuration above defines the amount of memory and CPUs that should be assigned for each of the VMs below. The default is 512mb, but this isn't enough to run Solr, so we need to bump up to 1024mb.\\\\r\\\\n\\\\r\\\\nEach of the next four blocks defines a separate virtual machine and gives each box a name. The `[name].vm.box` command is telling Vagrant which template to use for creating each box, and the `[name].vm.network` command instructs Vagrant to create a private network using the [DHCP](https://kb.iu.edu/d/adov \\\\\\"What is DCHP?\\\\\\") protocol. This means that each of our boxes will be assigned an IP address that can only be accessed within our private network (the four vagrant boxes and our host machine).\\\\r\\\\n\\\\r\\\\nNow run the `vagrant up` command in your terminal, from the `solrcloud-test` directory. The process of building your four Vagrant boxes will begin. This could a good few minutes, particularly if you haven't used the Ubuntu Trusty64 box before, as Vagrant will download it.\\\\r\\\\n\\\\r\\\\n**Note:** Some older versions of Vagrant have an issue when using the DCHP network type. They fail on `vagrant up` with an error saying a network of that type already exists. Upgrade to the latest version of Vagrant and that error will disappear.\\\\r\\\\n\\\\r\\\\nNow that the Vagrant boxes are built and running, we can SSH into them. Open three additional tabs or windows for your terminal, go to the solrcloud-test directory in each and use the command `vagrant ssh [box name]`, e.g.:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"vagrant ssh zoo1\\\\r\\\\nvagrant ssh solr1\\\\r\\\\netc...\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now that we are working within the virtual machines, the first thing we need to do is make a note of the IP addresses on each. There are a few ways to do this, but I use the `ifconfig -a` command. You should see something similar to the output below:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"eth0      Link encap:Ethernet  HWaddr 08:00:27:55:57:5e  \\\\r\\\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fe55:575e/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:754 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:584 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:79919 (79.9 KB)  TX bytes:69421 (69.4 KB)\\\\r\\\\n\\\\r\\\\neth1      Link encap:Ethernet  HWaddr 08:00:27:c4:24:ec  \\\\r\\\\n          inet addr:172.28.128.3  Bcast:172.28.128.255  Mask:255.255.255.0\\\\r\\\\n          inet6 addr: fe80::a00:27ff:fec4:24ec/64 Scope:Link\\\\r\\\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\\\r\\\\n          RX packets:175 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:1000 \\\\r\\\\n          RX bytes:37752 (37.7 KB)  TX bytes:2538 (2.5 KB)\\\\r\\\\n\\\\r\\\\nlo        Link encap:Local Loopback  \\\\r\\\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\\\r\\\\n          inet6 addr: ::1/128 Scope:Host\\\\r\\\\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\\\\r\\\\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\\\\r\\\\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\\\\r\\\\n          collisions:0 txqueuelen:0 \\\\r\\\\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"The address you're looking for is the `inet addr: 172.28.128.3` bit in the `eth1` block. If you run this command on each box, you should find the IP address is the same for each apart from the final number. This is because the names are taken from the [reserved IP addresses space](https://tools.ietf.org/html/rfc1918#section-3 \\\\\\"RFC 1918 -  Address Allocation for Private Internets\\\\\\"). For example, the addresses generated for my example are:\\\\r\\\\n\\\\r\\\\n* 172.28.128.3\\\\r\\\\n* 172.28.128.4\\\\r\\\\n* 172.28.128.5\\\\r\\\\n* 172.28.128.6\\\\r\\\\n\\\\r\\\\nYou can test your private network by ssh'ing from one Vagrant box into another, with the username `vagrant` and the password `vagrant`:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"ssh vagrant@172.28.128.4\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"And that's it! We have our machines up and running. You could use this setup to test any distributed network setup. You could test security settings on top of an application stack using [iptables](https://help.ubuntu.com/community/IptablesHowTo \\\\\\"Iptables How To\\\\\\"). These things are beyond the scope of this tutorial, but I'd encourage you to play around with this.\\\\r\\\\n\\\\r\\\\nSo, next we need to install the relevant software on each machine.\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Java 8</h2><p>Both Solr and Zookeeper rely on Java 8 in one way or another. So the first thing we're going to do is install this on each of the boxes. Run the following commands in each of the tabs you have open.</p>\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo add-apt-repository ppa:webupd8team/java\\\\r\\\\nsudo apt-get update\\\\r\\\\nsudo apt-get install oracle-java8-installer\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>This installs both the JRE and JDK versions of Oracle's official Java package. If you would prefer to use OpenJDK, <a href=\\\\\\"http://ubuntuhandbook.org/index.php/2015/01/install-openjdk-8-ubuntu-14-04-12-04-lts/\\\\\\">you can follow the instructions here</a>.</p>\\"}, {\\"type\\": \\"paragraph\\", \\"value\\": \\"<h2>Installing Zookeeper<br/></h2>\\"}, {\\"type\\": \\"markdown\\", \\"value\\": \\"[As the website states](https://zookeeper.apache.org/ \\\\\\"Zookeeper home\\\\\\"), \\\\\\"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\\\\\\". For the purposes of SolrCloud, Zookeeper does the following:\\\\r\\\\n\\\\r\\\\n* Stores and distributes configuration files for SolrCloud collections to each node.\\\\r\\\\n* Manages the election of ['leaders'](https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud).\\\\r\\\\n* Ensures the synchronisation of data between replicas of collection shards.\\\\r\\\\n\\\\r\\\\nInstalling and configuring Zookeeper for our SolrCloud test is pretty easy. First, pull down the latest version with the `curl` command and unpack it:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"curl -O http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz\\\\r\\\\ntar -zxf zookeeper-3.4.8.tar.gz\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"We then need to update the Zookeeper configuration with some basics. Zookeeper comes with a sample config file (`conf/zoo_sample.cfg`), but we don't need all the comments and examples that that file provides, so we'll just create a new one using your editor of choice. I'm going to use nano.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"nano ~/zookeeper-3.4.8/conf/zoo.cfg\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"Now copy the following three lines into that file and save it.\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"tickTime=2000\\\\r\\\\ndataDir=/var/lib/zookeeper\\\\r\\\\nclientPort=2181\\", \\"language\\": \\"bash\\"}}, {\\"type\\": \\"markdown\\", \\"value\\": \\"* `tickTime` is the amount of time in milliseconds that Zookeeper will wait before determining that one of your Solr servers is down.\\\\r\\\\n* `dataDir` is where Zookeeper will store the data about your SolrCloud cluster. If this directory doesn't exist then Zookeeper will creat it when it first starts up.\\\\r\\\\n* `clientPort` is the port on which your SolrCloud nodes will connect to Zookeeper.\\\\r\\\\n\\\\r\\\\nFinally, you need to start Zookeeper with the start-up script provided with the installation:\\"}, {\\"type\\": \\"real_codeblock\\", \\"value\\": {\\"code\\": \\"sudo ~/zookeeper-3.4.8/bin/zkServer.sh start\\", \\"language\\": \\"bash\\"}}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2016-08-27", "path": "00010001000G", "url_path": "/home/testing-solrcloud-vagrant/", "expired": false, "pk": 23, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	23	1
211	f	2019-05-21 02:15:00.241763+00	{"date": "2019-05-20", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "About me", "listing_intro": "<p></p>", "pk": 21, "expired": false, "live_revision": 210, "go_live_at": null, "path": "00010001000E", "last_published_at": "2019-05-21T02:14:07.081Z", "first_published_at": "2016-04-01T11:52:47.214Z", "subtitle": "Chris Rogers", "tagged_items": [], "seo_title": "", "show_in_menus": false, "title": "About me", "body": "[{\\"value\\": \\"<p>Since March 2017 I have been working as the Project Manager in the Computing department of the <a href=\\\\\\"https://seas.harvard.edu\\\\\\">Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS)</a>. I manage a multi-disciplinary team of applications developers, dev-ops engineers and computational scientists. We create applications and manage infrastructure to support SEAS staff, faculty and students in both administrative and educational settings.</p><p>Previously I&#x27;ve worked at the Bodleian Library, part of Oxford University, as the Digital Projects Manager. Before that I worked as a project manager for the digital agency <a href=\\\\\\"http://torchbox.com\\\\\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><ul><li><a href=\\\\\\"http://rca.ac.uk\\\\\\">The Royal College of Art</a></li><li><a href=\\\\\\"http://royaldrawingschool.org\\\\\\">The Royal Drawing School</a></li><li><a href=\\\\\\"http://globalwitness.org.uk\\\\\\">Global Witness</a></li><li><a href=\\\\\\"http://plan-international.org.uk\\\\\\">Plan International</a></li><li><a href=\\\\\\"http://events.burton.com/\\\\\\">Burton Snowboards</a></li><li><a href=\\\\\\"http://election.kingsfund.org.uk/\\\\\\">The King&#x27;s Fund</a></li></ul><p>I was the original project manager for the development of the <a href=\\\\\\"https://wagtail.io\\\\\\">Wagtail CMS</a> open source project, and I still occasionally contribute to the project. I&#x27;m also a skateboarder of 18+ years.</p><p></p>\\", \\"id\\": \\"df9dc178-a982-44ef-9de0-59e5c69f5201\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "chris-rogers", "search_description": "", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/chris-rogers/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2019-05-21T02:14:07.061Z", "live": true}	\N	21	1
3	f	2015-06-22 12:55:37.571517+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-22T12:53:21.068Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "", "slug": "what-place", "live": true, "has_unpublished_changes": false, "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a></li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a></li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
136	f	2016-04-01 11:52:47.189216+00	{"subtitle": "About me!", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-04-01T11:52:15.108Z", "go_live_at": null, "title": "Chris Rogers", "seo_title": "", "listing_intro": "", "slug": "chris-rogers", "live": false, "bookmark_placements": [], "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently working at the Bodleian Library, part of Oxford University as the Digital Projects Manager. Previously I've worked as a project manager for the digital agency <a href=\\\\\\"http://torchbox.com\\\\\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><p></p><ul><li><a href=\\\\\\"http://rca.ac.uk\\\\\\">The Royal College of Art</a></li><li><a href=\\\\\\"http://royaldrawingschool.org\\\\\\">The Royal Drawing School</a></li><li><a href=\\\\\\"http://globalwitness.org.uk\\\\\\">Global Witness</a></li><li><a href=\\\\\\"http://plan-international.org.uk\\\\\\">Plan International</a></li><li><a href=\\\\\\"http://events.burton.com/\\\\\\">Burton Snowboards</a></li><li><a href=\\\\\\"http://election.kingsfund.org.uk/\\\\\\">The King's Fund</a></li></ul><p>In my spare time I'm a regular contributor to the <a href=\\\\\\"https://wagtail.io\\\\\\">Wagtail CMS</a> open source project.\\\\u00a0I'm also a skateboarder of 16+ years, and a runner of 5.</p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-10-01", "path": "00010001000E", "url_path": "/home/chris-rogers/", "expired": false, "pk": 21, "main_image": null, "locked": false, "depth": 3, "first_published_at": null, "expire_at": null}	\N	21	1
13	f	2015-06-25 12:17:54.907981+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-25T12:16:55.004Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
27	f	2015-06-30 12:20:01.273588+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-25T22:03:03.321Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 8, "content_object": 4}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	3
9	f	2015-06-25 12:12:49.628817+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-22T12:59:46.704Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
15	f	2015-06-25 22:02:56.512704+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-25T22:01:59.934Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "has_unpublished_changes": true, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
12	f	2015-06-25 12:16:55.00426+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-25T12:12:49.628Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "has_unpublished_changes": false, "tagged_items": [{"pk": null, "tag": 1, "content_object": 4}, {"pk": null, "tag": 2, "content_object": 4}, {"pk": null, "tag": 3, "content_object": 4}, {"pk": null, "tag": 4, "content_object": 4}], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
4	f	2015-06-22 12:58:09.840588+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-22T12:55:37.571Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "", "slug": "what-place", "live": true, "has_unpublished_changes": false, "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
2	f	2015-06-22 12:53:21.068832+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-22T12:48:56.774Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "", "slug": "what-place", "live": true, "has_unpublished_changes": false, "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the Wagtail CMS (v1.0rc1).<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu</li><li>The server is Nginx with uWSGI</li><li>It was deployed over a lunchtime using a combination of the Wagtail docs and this tutorial from Digital Ocean</li></ul><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
14	f	2015-06-25 22:01:59.934304+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-25T12:17:54.907Z", "go_live_at": null, "title": "What is this place??", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
78	f	2016-02-28 19:43:50.719094+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "How this site was made, and what it was made with", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-30T12:25:25.886Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
108	f	2016-03-04 13:19:02.055723+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "How this site was made, and what it was made with", "owner": 1, "intro": "", "latest_revision_created_at": "2016-02-28T19:43:50.719Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.3.1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>The site is cached using <a href=\\\\\\"http://memcached.org/\\\\\\">Memcached</a>.</li><li>For the styling I've tried to stick to the<a href=\\\\\\"https://smacss.com/\\\\\\">\\\\u00a0SMACSS\\\\u00a0</a>methodology.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
28	f	2015-06-30 12:25:25.886965+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2015-06-30T12:20:01.273Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.0rc1)</a>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	3
182	f	2016-09-01 09:31:47.564481+00	{"subtitle": "How this site was made, and what it was made with", "search_description": "How this site was made, and what it was made with", "owner": 1, "intro": "", "latest_revision_created_at": "2016-09-01T09:31:34.002Z", "go_live_at": null, "title": "What is this place?", "seo_title": "", "listing_intro": "<p>How this site was made, and what it was made with</p>", "slug": "what-place", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p></p><ul><li>This site is built using the <a href=\\\\\\"http://www.github.com/torchbox/wagtail\\\\\\">Wagtail CMS (v1.6.1)</a><p></p><p></p><p>http://www.github.com/torchbox/wagtail</p><p></p>.<br/></li><li>It is hosted on a 512mb Digital Ocean box, running Ubuntu 14.04.</li><li>The server is <a href=\\\\\\"http://wiki.nginx.org/Main\\\\\\">Nginx</a> with <a href=\\\\\\"https://uwsgi-docs.readthedocs.org/en/latest/\\\\\\">uWSGI</a>.</li><li>The site is cached using <a href=\\\\\\"http://memcached.org/\\\\\\">Memcached</a>.</li><li>For the styling I've tried to stick to the<a href=\\\\\\"https://smacss.com/\\\\\\">\\\\u00a0SMACSS\\\\u00a0</a>methodology.</li><li>It was deployed over a lunchtime using a combination of the <a href=\\\\\\"http://docs.wagtail.io/en/v0.8.7/getting_started/installation.html\\\\\\">Wagtail docs</a> and <a href=\\\\\\"https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04\\\\\\">this tutorial from Digital Ocean</a>.</li><li>You can see the codebase on <a href=\\\\\\"http://www.github.com/chrxr/blog_project\\\\\\">Github here</a>.</li></ul><p></p>\\"}]", "numchild": 1, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-06-22", "path": "000100010001", "url_path": "/home/what-place/", "expired": false, "pk": 4, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2015-06-22T12:48:56.803Z", "expire_at": null}	\N	4	1
137	f	2016-04-01 11:53:07.991172+00	{"subtitle": "Chris Rogers", "search_description": "", "owner": 1, "intro": "", "latest_revision_created_at": "2016-04-01T11:52:47.189Z", "go_live_at": null, "title": "About me", "seo_title": "", "listing_intro": "", "slug": "chris-rogers", "live": true, "bookmark_placements": [], "has_unpublished_changes": false, "tagged_items": [], "body": "[{\\"type\\": \\"paragraph\\", \\"value\\": \\"<p>I'm currently working at the Bodleian Library, part of Oxford University as the Digital Projects Manager. Previously I've worked as a project manager for the digital agency <a href=\\\\\\"http://torchbox.com\\\\\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><p></p><ul><li><a href=\\\\\\"http://rca.ac.uk\\\\\\">The Royal College of Art</a></li><li><a href=\\\\\\"http://royaldrawingschool.org\\\\\\">The Royal Drawing School</a></li><li><a href=\\\\\\"http://globalwitness.org.uk\\\\\\">Global Witness</a></li><li><a href=\\\\\\"http://plan-international.org.uk\\\\\\">Plan International</a></li><li><a href=\\\\\\"http://events.burton.com/\\\\\\">Burton Snowboards</a></li><li><a href=\\\\\\"http://election.kingsfund.org.uk/\\\\\\">The King's Fund</a></li></ul><p>In my spare time I'm a regular contributor to the <a href=\\\\\\"https://wagtail.io\\\\\\">Wagtail CMS</a> open source project.\\\\u00a0I'm also a skateboarder of 16+ years, and a runner of 5.</p><p></p>\\"}]", "numchild": 0, "listing_image": null, "content_type": 27, "show_in_menus": false, "date": "2015-10-01", "path": "00010001000E", "url_path": "/home/chris-rogers/", "expired": false, "pk": 21, "main_image": null, "locked": false, "depth": 3, "first_published_at": "2016-04-01T11:52:47.214Z", "expire_at": null}	\N	21	1
209	f	2019-05-21 02:13:12.516994+00	{"date": "2015-10-01", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "About me", "listing_intro": "<p></p>", "pk": 21, "expired": false, "live_revision": null, "go_live_at": null, "path": "00010001000E", "last_published_at": "2016-04-01T11:53:07.991Z", "first_published_at": "2016-04-01T11:52:47.214Z", "subtitle": "Chris Rogers", "tagged_items": [], "seo_title": "", "show_in_menus": false, "title": "About me", "body": "[{\\"value\\": \\"<p>Since March 2017 I have been working as the Project Manager in the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS). I manage a multi-disciplinary team of applications developers, dev-ops engineers and computational scientists. We create applications and manage infrastructure to support SEAS staff, faculty and students in both administrative and educational settings.</p><p>Previously I&#x27;ve worked at the Bodleian Library, part of Oxford University, as the Digital Projects Manager. Before that I worked as a project manager for the digital agency <a href=\\\\\\"http://torchbox.com\\\\\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><ul><li><a href=\\\\\\"http://rca.ac.uk\\\\\\">The Royal College of Art</a></li><li><a href=\\\\\\"http://royaldrawingschool.org\\\\\\">The Royal Drawing School</a></li><li><a href=\\\\\\"http://globalwitness.org.uk\\\\\\">Global Witness</a></li><li><a href=\\\\\\"http://plan-international.org.uk\\\\\\">Plan International</a></li><li><a href=\\\\\\"http://events.burton.com/\\\\\\">Burton Snowboards</a></li><li><a href=\\\\\\"http://election.kingsfund.org.uk/\\\\\\">The King&#x27;s Fund</a></li></ul><p>I was the original project manager for the development of the <a href=\\\\\\"https://wagtail.io\\\\\\">Wagtail CMS</a> open source project, and I still occasionally contribute to the project. I&#x27;m also a skateboarder of 18+ years.</p><p></p>\\", \\"id\\": \\"df9dc178-a982-44ef-9de0-59e5c69f5201\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "chris-rogers", "search_description": "", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/chris-rogers/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2016-04-01T11:53:07.991Z", "live": true}	\N	21	1
210	f	2019-05-21 02:14:07.061758+00	{"date": "2015-10-01", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "About me", "listing_intro": "<p></p>", "pk": 21, "expired": false, "live_revision": 209, "go_live_at": null, "path": "00010001000E", "last_published_at": "2019-05-21T02:13:12.544Z", "first_published_at": "2016-04-01T11:52:47.214Z", "subtitle": "Chris Rogers", "tagged_items": [], "seo_title": "", "show_in_menus": false, "title": "About me", "body": "[{\\"value\\": \\"<p>Since March 2017 I have been working as the Project Manager in the Computing department of the <a href=\\\\\\"https://seas.harvard.edu\\\\\\">Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS)</a>. I manage a multi-disciplinary team of applications developers, dev-ops engineers and computational scientists. We create applications and manage infrastructure to support SEAS staff, faculty and students in both administrative and educational settings.</p><p>Previously I&#x27;ve worked at the Bodleian Library, part of Oxford University, as the Digital Projects Manager. Before that I worked as a project manager for the digital agency <a href=\\\\\\"http://torchbox.com\\\\\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><ul><li><a href=\\\\\\"http://rca.ac.uk\\\\\\">The Royal College of Art</a></li><li><a href=\\\\\\"http://royaldrawingschool.org\\\\\\">The Royal Drawing School</a></li><li><a href=\\\\\\"http://globalwitness.org.uk\\\\\\">Global Witness</a></li><li><a href=\\\\\\"http://plan-international.org.uk\\\\\\">Plan International</a></li><li><a href=\\\\\\"http://events.burton.com/\\\\\\">Burton Snowboards</a></li><li><a href=\\\\\\"http://election.kingsfund.org.uk/\\\\\\">The King&#x27;s Fund</a></li></ul><p>I was the original project manager for the development of the <a href=\\\\\\"https://wagtail.io\\\\\\">Wagtail CMS</a> open source project, and I still occasionally contribute to the project. I&#x27;m also a skateboarder of 18+ years.</p><p></p>\\", \\"id\\": \\"df9dc178-a982-44ef-9de0-59e5c69f5201\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "chris-rogers", "search_description": "", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/chris-rogers/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2019-05-21T02:13:12.516Z", "live": true}	\N	21	1
212	f	2019-05-21 02:16:01.992033+00	{"date": "2017-05-20", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "About me", "listing_intro": "<p></p>", "pk": 21, "expired": false, "live_revision": 211, "go_live_at": null, "path": "00010001000E", "last_published_at": "2019-05-21T02:15:00.260Z", "first_published_at": "2016-04-01T11:52:47.214Z", "subtitle": "Chris Rogers", "tagged_items": [], "seo_title": "", "show_in_menus": false, "title": "About me", "body": "[{\\"value\\": \\"<p>Since March 2017 I have been working as the Project Manager in the Computing department of the <a href=\\\\\\"https://seas.harvard.edu\\\\\\">Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS)</a>. I manage a multi-disciplinary team of applications developers, dev-ops engineers and computational scientists. We create applications and manage infrastructure to support SEAS staff, faculty and students in both administrative and educational settings.</p><p>Previously I&#x27;ve worked at the Bodleian Library, part of Oxford University, as the Digital Projects Manager. Before that I worked as a project manager for the digital agency <a href=\\\\\\"http://torchbox.com\\\\\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><ul><li><a href=\\\\\\"http://rca.ac.uk\\\\\\">The Royal College of Art</a></li><li><a href=\\\\\\"http://royaldrawingschool.org\\\\\\">The Royal Drawing School</a></li><li><a href=\\\\\\"http://globalwitness.org.uk\\\\\\">Global Witness</a></li><li><a href=\\\\\\"http://plan-international.org.uk\\\\\\">Plan International</a></li><li><a href=\\\\\\"http://events.burton.com/\\\\\\">Burton Snowboards</a></li><li><a href=\\\\\\"http://election.kingsfund.org.uk/\\\\\\">The King&#x27;s Fund</a></li></ul><p>I was the original project manager for the development of the <a href=\\\\\\"https://wagtail.io\\\\\\">Wagtail CMS</a> open source project, and I still occasionally contribute to the project. I&#x27;m also a skateboarder of 18+ years.</p><p></p>\\", \\"id\\": \\"df9dc178-a982-44ef-9de0-59e5c69f5201\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "chris-rogers", "search_description": "", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/chris-rogers/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2019-05-21T02:15:00.241Z", "live": true}	\N	21	1
216	f	2019-09-24 17:11:21.500185+00	{"date": "2017-05-20", "bookmark_placements": [], "depth": 3, "expire_at": null, "draft_title": "About me", "listing_intro": "<p></p>", "pk": 21, "expired": false, "live_revision": 212, "go_live_at": null, "path": "00010001000E", "last_published_at": "2019-05-21T02:16:02.011Z", "first_published_at": "2016-04-01T11:52:47.214Z", "subtitle": "Chris Rogers", "tagged_items": [], "seo_title": "", "show_in_menus": false, "title": "About me", "body": "[{\\"value\\": \\"<p>Since March 2017 I have been working as the Director of Engineering in the Computing department of the <a href=\\\\\\"https://seas.harvard.edu\\\\\\">Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS)</a>. I manage a multi-disciplinary team of applications developers, dev-ops engineers and computational scientists. We create applications and manage infrastructure to support SEAS staff, faculty and students in both administrative and educational settings.</p><p>Previously I&#x27;ve worked at the Bodleian Library, part of Oxford University, as the Digital Projects Manager. Before that I worked as a project manager for the digital agency <a href=\\\\\\"http://torchbox.com\\\\\\">Torchbox</a>, as the ebook technical lead at Penguin, and for Oxford University Press in a variety of digital roles.</p><p>Previous website projects as a PM include:</p><ul><li><a href=\\\\\\"http://rca.ac.uk\\\\\\">The Royal College of Art</a></li><li><a href=\\\\\\"http://royaldrawingschool.org\\\\\\">The Royal Drawing School</a></li><li><a href=\\\\\\"http://globalwitness.org.uk\\\\\\">Global Witness</a></li><li><a href=\\\\\\"http://plan-international.org.uk\\\\\\">Plan International</a></li><li><a href=\\\\\\"http://events.burton.com/\\\\\\">Burton Snowboards</a></li><li><a href=\\\\\\"http://election.kingsfund.org.uk/\\\\\\">The King&#x27;s Fund</a></li></ul><p>I was the original project manager for the development of the <a href=\\\\\\"https://wagtail.io\\\\\\">Wagtail CMS</a> open source project, and I still occasionally contribute to the project. I&#x27;m also a skateboarder of 18+ years.</p><p></p>\\", \\"id\\": \\"df9dc178-a982-44ef-9de0-59e5c69f5201\\", \\"type\\": \\"paragraph\\"}]", "main_image": null, "listing_image": null, "slug": "chris-rogers", "search_description": "", "content_type": 27, "has_unpublished_changes": false, "owner": 1, "url_path": "/home/chris-rogers/", "numchild": 0, "locked": false, "intro": null, "latest_revision_created_at": "2019-05-21T02:16:01.992Z", "live": true}	\N	21	1
\.


--
-- Name: wagtailcore_pagerevision_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_pagerevision_id_seq', 216, true);


--
-- Data for Name: wagtailcore_pageviewrestriction; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_pageviewrestriction (id, password, page_id, restriction_type) FROM stdin;
\.


--
-- Data for Name: wagtailcore_pageviewrestriction_groups; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_pageviewrestriction_groups (id, pageviewrestriction_id, group_id) FROM stdin;
\.


--
-- Name: wagtailcore_pageviewrestriction_groups_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_pageviewrestriction_groups_id_seq', 1, false);


--
-- Name: wagtailcore_pageviewrestriction_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_pageviewrestriction_id_seq', 1, false);


--
-- Data for Name: wagtailcore_site; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailcore_site (id, hostname, port, is_default_site, root_page_id, site_name) FROM stdin;
2	chrxr.com	80	t	3	\N
\.


--
-- Name: wagtailcore_site_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailcore_site_id_seq', 2, true);


--
-- Data for Name: wagtaildocs_document; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtaildocs_document (id, title, file, created_at, uploaded_by_user_id, collection_id, file_size, file_hash) FROM stdin;
1	manifest.json	documents/manifest.json	2016-05-10 14:31:49.482654+00	1	1	\N	
\.


--
-- Name: wagtaildocs_document_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtaildocs_document_id_seq', 1, true);


--
-- Data for Name: wagtailembeds_embed; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailembeds_embed (id, url, max_width, type, html, title, author_name, provider_name, thumbnail_url, width, height, last_updated) FROM stdin;
1	https://youtu.be/a57UIrhDkrI?autoplay=1	\N	video	<iframe width="459" height="344" src="https://www.youtube.com/embed/a57UIrhDkrI?feature=oembed" frameborder="0" allowfullscreen></iframe>	Trans-Tasman Cup 08	Rontek9	YouTube	https://i.ytimg.com/vi/a57UIrhDkrI/hqdefault.jpg	459	344	2015-06-22 14:12:17.402482+00
2	https://www.youtube.com/watch?v=oq0JR0t-zeo&feature=youtu.be&autoplay=1	\N	video	<iframe width="459" height="344" src="https://www.youtube.com/embed/oq0JR0t-zeo?feature=oembed" frameborder="0" allowfullscreen></iframe>	Gloucester Cheese Rolling 2009	Rontek9	YouTube	https://i.ytimg.com/vi/oq0JR0t-zeo/hqdefault.jpg	459	344	2015-06-22 14:23:52.984338+00
3	https://www.youtube.com/watch?v=oq0JR0t-zeo&amp;feature=youtu.be&amp;autoplay=1	\N	video	<iframe width="459" height="344" src="https://www.youtube.com/embed/oq0JR0t-zeo?feature=oembed" frameborder="0" allowfullscreen></iframe>	Gloucester Cheese Rolling 2009	Rontek9	YouTube	https://i.ytimg.com/vi/oq0JR0t-zeo/hqdefault.jpg	459	344	2015-06-22 14:23:55.134338+00
4	https://www.youtube.com/watch?v=oq0JR0t-zeo&amp;amp;feature=youtu.be&amp;amp;autoplay=1	\N	video	<iframe width="459" height="344" src="https://www.youtube.com/embed/oq0JR0t-zeo?feature=oembed" frameborder="0" allowfullscreen></iframe>	Gloucester Cheese Rolling 2009	Rontek9	YouTube	https://i.ytimg.com/vi/oq0JR0t-zeo/hqdefault.jpg	459	344	2015-06-26 19:31:18.522256+00
5	https://www.youtube.com/watch?v=oq0JR0t-zeo&amp;amp;amp;feature=youtu.be&amp;amp;amp;autoplay=1	\N	video	<iframe width="459" height="344" src="https://www.youtube.com/embed/oq0JR0t-zeo?feature=oembed" frameborder="0" allowfullscreen></iframe>	Gloucester Cheese Rolling 2009	Rontek9	YouTube	https://i.ytimg.com/vi/oq0JR0t-zeo/hqdefault.jpg	459	344	2015-06-26 19:31:36.876096+00
6	https://www.youtube.com/watch?v=oq0JR0t-zeo&amp;amp;amp;amp;feature=youtu.be&amp;amp;amp;amp;autoplay=1	\N	video	<iframe width="459" height="344" src="https://www.youtube.com/embed/oq0JR0t-zeo?feature=oembed" frameborder="0" allowfullscreen></iframe>	Gloucester Cheese Rolling 2009	Rontek9	YouTube	https://i.ytimg.com/vi/oq0JR0t-zeo/hqdefault.jpg	459	344	2015-06-30 18:01:00.257591+00
7	https://www.youtube.com/watch?v=oq0JR0t-zeo&amp;amp;amp;amp;amp;feature=youtu.be&amp;amp;amp;amp;amp;autoplay=1	\N	video	<iframe width="459" height="344" src="https://www.youtube.com/embed/oq0JR0t-zeo?feature=oembed" frameborder="0" allowfullscreen></iframe>	Gloucester Cheese Rolling 2009	Rontek9	YouTube	https://i.ytimg.com/vi/oq0JR0t-zeo/hqdefault.jpg	459	344	2015-06-30 18:01:30.468281+00
8	https://soundcloud.com/chris-rogers-105244959/blue	\N	rich	<iframe width="100%" height="400" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?visual=true&url=https%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F219370923&show_artwork=true"></iframe>	Blue by Chris Rogers	Chris Rogers	SoundCloud	http://a1.sndcdn.com/images/fb_placeholder.png?1439544637	\N	400	2015-08-15 15:20:51.992874+00
9	https://soundcloud.com/chris-rogers-105244959/blue&amp;color=ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false	\N	rich	<iframe width="100%" height="400" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?visual=true&url=https%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F219370923&show_artwork=true&=&amp%3Bcolor=ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false"></iframe>	Blue by Chris Rogers	Chris Rogers	SoundCloud	http://a1.sndcdn.com/images/fb_placeholder.png?1439544637	\N	400	2015-08-15 15:24:32.539476+00
10	https://soundcloud.com/chris-rogers-105244959/blue&amp;amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false	\N	rich	<iframe width="100%" height="400" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?visual=true&url=https%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F219370923&show_artwork=true&=&amp%3Bamp%3Bcolor=ff5500&amp%3Bauto_play=false&amp%3Bhide_related=false&amp%3Bshow_comments=true&amp%3Bshow_user=true&amp%3Bshow_reposts=false"></iframe>	Blue by Chris Rogers	Chris Rogers	SoundCloud	http://a1.sndcdn.com/images/fb_placeholder.png?1439544637	\N	400	2015-08-15 15:24:37.719231+00
\.


--
-- Name: wagtailembeds_embed_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailembeds_embed_id_seq', 10, true);


--
-- Data for Name: wagtailforms_formsubmission; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailforms_formsubmission (id, form_data, submit_time, page_id) FROM stdin;
\.


--
-- Name: wagtailforms_formsubmission_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailforms_formsubmission_id_seq', 1, false);


--
-- Data for Name: wagtailimages_image; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailimages_image (id, title, file, width, height, created_at, focal_point_x, focal_point_y, focal_point_width, focal_point_height, uploaded_by_user_id, file_size, collection_id, file_hash) FROM stdin;
\.


--
-- Name: wagtailimages_image_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailimages_image_id_seq', 7, true);


--
-- Data for Name: wagtailimages_rendition; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailimages_rendition (id, file, width, height, focal_point_key, image_id, filter_spec) FROM stdin;
\.


--
-- Name: wagtailimages_rendition_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailimages_rendition_id_seq', 19, true);


--
-- Data for Name: wagtailredirects_redirect; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailredirects_redirect (id, old_path, is_permanent, redirect_link, redirect_page_id, site_id) FROM stdin;
\.


--
-- Name: wagtailredirects_redirect_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailredirects_redirect_id_seq', 1, false);


--
-- Name: wagtailsearch_editorspick_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailsearch_editorspick_id_seq', 1, false);


--
-- Data for Name: wagtailsearch_query; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailsearch_query (id, query_string) FROM stdin;
1	blog
2	heroku
3	herok
4	bue
5	blue
6	django
7	erm
8	blurgh
9	boo
10	wagtail
11	html
12	evernote
13	dog
14	heroky
15	djano
16	bookmarks
17	home
18	content
19	rss
20	project
21	manage
22	test
23	python
24	tinfoilxssinelementattribute8effee409c625e1a2d8f5033631840e6ce1dcb64
25	tinfoilcby51
26	javascripttinfoilxssinelementattribute8effee409c625e1a2d8f5033631840e6ce1dcb64
27	vbscripttinfoilxssinelementattribute8effee409c625e1a2d8f5033631840e6ce1dcb64
28	tinfoiltrf45tinfoilxssinelementevent8effee409c625e1a2d8f5033631840e6ce1dcb64
29	tinfoilkbx54tinfoilxssinelementevent8effee409c625e1a2d8f5033631840e6ce1dcb64
30	tinfoilvbd98tinfoilxssinelementevent8effee409c625e1a2d8f5033631840e6ce1dcb64
31	tinfoiljzr05tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
32	tinfoilohp46tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
33	tinfoilpft85tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
34	tinfoilalr73tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
35	tinfoilsam80tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
36	tinfoilupf57tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
37	tinfoilgmf42tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
38	tinfoilfya90tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
39	tinfoilqww32tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
40	tinfoilwxl13tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
41	tinfoilwak17tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
42	tinfoilysc42atinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
43	tinfoilshc31tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
44	tinfoilrru74atinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
45	tinfoilels87 tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
46	tinfoilquv27atinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
47	tinfoilqrw07tinfoilxssinscripttag8effee409c625e1a2d8f5033631840e6ce1dcb64
48	tinfoilcsu13 tinfoilxssintag8effee409c625e1a2d8f5033631840e6ce1dcb64
49	tinfoillks23 tinfoilxssintag8effee409c625e1a2d8f5033631840e6ce1dcb64
50	tinfoilszo17 tinfoilxssintag8effee409c625e1a2d8f5033631840e6ce1dcb64
51	tinfoilqva98tinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
52	tinfoilmrf38tinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
53	tinfoiltvk031tinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
54	tinfoildnz64 tinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
55	tinfoilamo56titletinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
56	tinfoileah31scripttinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
57	tinfoilcsu31textareatinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
58	tinfoilwqe27tinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
59	tinfoilkyu83titletinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
60	tinfoilvlw61scripttinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
61	tinfoiltim82textareatinfoilxss8effee409c625e1a2d8f5033631840e6ce1dcb64
62	xcrlfsafe8effee409c625e1a2d8f5033631840e6ce1dcb64 no
63	tinfoiluwq47 xcrlfsafe8effee409c625e1a2d8f5033631840e6ce1dcb64 no
65	čċxcrlfsafe8effee409c625e1a2d8f5033631840e6ce1dcb64 no
66	tinfoilqsb50čċxcrlfsafe8effee409c625e1a2d8f5033631840e6ce1dcb64 no
67	wwwtinfoilfakesitecom
68	tinfoiluvi28wwwtinfoilfakesitecom
70	httpswwwtinfoilfakesitecom
71	tinfoileuc94httpswwwtinfoilfakesitecom
72	tinfoilpgv01httpwwwtinfoilfakesitecom
73	httpwwwtinfoilfakesitecom
74	tinfoildnz29httpwwwtinfoilfakesitecom
75	tinfoilouj90httpwwwtinfoilfakesitecom
76	tinfoilepq15 tinfoilxssintag8effee409c625e1a2d8f5033631840e6ce1dcb64
77	tinfoilvhg95čċxcrlfsafe8effee409c625e1a2d8f5033631840e6ce1dcb64 no
78	tinfoilule98httpswwwtinfoilfakesitecom
79	tinfoilwpp90 tinfoilxssintag8effee409c625e1a2d8f5033631840e6ce1dcb64
80	tinfoilzoq80httpwwwtinfoilfakesitecom
81	tinfoilotm50httpwwwtinfoilfakesitecom
82	tinfoilaxy49httpwwwtinfoilfakesitecom
83	map
84	search
85	uhhj
86	seasr
87	code
88	twitter
89	js
90	a
91	y2hyehiuy29tlwprint23894789938947892334567343546345
92	print23894789938947892334567343546345
93	y2hyehiuy29tlwphp print23894789938947892334567343546345
94	y2hyehiuy29tlw
95	cloud
96	sitemap wagtail
97	sitemap
\.


--
-- Name: wagtailsearch_query_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailsearch_query_id_seq', 97, true);


--
-- Data for Name: wagtailsearch_querydailyhits; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailsearch_querydailyhits (id, date, hits, query_id) FROM stdin;
1	2015-09-27	1	1
2	2015-09-27	1	2
3	2015-09-27	1	3
4	2015-09-27	1	4
5	2015-09-27	1	5
6	2015-12-28	1	6
7	2015-12-30	6	6
8	2015-12-31	1	6
9	2016-01-21	3	7
10	2016-01-21	1	8
11	2016-01-21	1	9
12	2016-01-21	1	10
13	2016-01-21	1	11
14	2016-01-21	1	2
15	2016-01-21	1	12
16	2016-01-22	1	13
18	2016-01-25	1	14
17	2016-01-25	3	2
19	2016-01-26	7	2
20	2016-02-22	1	15
21	2016-02-22	1	6
22	2016-02-26	4	12
23	2016-02-28	1	16
24	2016-02-28	1	17
26	2016-02-28	1	14
25	2016-02-28	3	2
27	2016-03-02	1	18
28	2016-03-06	1	19
29	2016-03-15	1	20
30	2016-03-29	1	7
31	2016-03-29	1	21
32	2016-08-16	1	22
33	2016-08-16	1	23
35	2016-12-05	1	25
87	2016-12-05	1	76
75	2016-12-05	4	65
36	2016-12-05	1	26
88	2016-12-05	2	77
80	2016-12-05	4	70
34	2016-12-05	12	24
37	2016-12-05	1	27
38	2016-12-05	1	28
39	2016-12-05	1	29
40	2016-12-05	1	30
41	2016-12-05	1	31
42	2016-12-05	1	32
43	2016-12-05	1	33
44	2016-12-05	1	34
45	2016-12-05	1	35
46	2016-12-05	1	36
47	2016-12-05	1	37
49	2016-12-05	1	39
48	2016-12-05	1	38
50	2016-12-05	1	40
51	2016-12-05	1	41
52	2016-12-05	1	42
53	2016-12-05	1	43
54	2016-12-05	1	44
55	2016-12-05	1	45
56	2016-12-05	1	46
57	2016-12-05	1	47
58	2016-12-05	1	48
59	2016-12-05	1	49
60	2016-12-05	1	50
61	2016-12-05	1	51
62	2016-12-05	1	52
63	2016-12-05	1	53
64	2016-12-05	1	54
65	2016-12-05	1	55
66	2016-12-05	1	56
67	2016-12-05	1	57
68	2016-12-05	1	58
69	2016-12-05	1	59
70	2016-12-05	1	60
71	2016-12-05	1	61
89	2016-12-05	2	78
72	2016-12-05	2	62
90	2016-12-05	1	79
73	2016-12-05	2	63
76	2016-12-05	2	66
77	2016-12-05	3	67
91	2016-12-05	2	80
78	2016-12-05	2	68
92	2016-12-05	2	81
81	2016-12-05	2	71
93	2016-12-05	2	82
83	2016-12-05	12	73
84	2016-12-05	2	72
85	2016-12-05	2	74
94	2017-04-17	1	83
86	2016-12-05	2	75
95	2017-09-27	1	10
96	2018-03-29	1	84
97	2018-08-07	1	85
98	2019-03-23	1	86
99	2019-03-23	1	87
100	2019-03-23	1	88
101	2019-03-23	1	89
102	2019-03-23	1	90
104	2019-04-11	1	92
105	2019-04-11	2	93
106	2019-04-11	1	94
103	2019-04-11	8	91
107	2019-06-27	1	95
108	2019-12-13	1	96
109	2019-12-13	1	97
\.


--
-- Name: wagtailsearch_querydailyhits_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailsearch_querydailyhits_id_seq', 109, true);


--
-- Data for Name: wagtailsearchpromotions_searchpromotion; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailsearchpromotions_searchpromotion (id, sort_order, description, page_id, query_id) FROM stdin;
\.


--
-- Data for Name: wagtailusers_userprofile; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.wagtailusers_userprofile (id, submitted_notifications, approved_notifications, rejected_notifications, user_id, preferred_language, current_time_zone, avatar) FROM stdin;
1	t	t	t	3			
\.


--
-- Name: wagtailusers_userprofile_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.wagtailusers_userprofile_id_seq', 1, true);


--
-- Name: auth_group_name_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group
    ADD CONSTRAINT auth_group_name_key UNIQUE (name);


--
-- Name: auth_group_permissions_group_id_permission_id_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions
    ADD CONSTRAINT auth_group_permissions_group_id_permission_id_key UNIQUE (group_id, permission_id);


--
-- Name: auth_group_permissions_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions
    ADD CONSTRAINT auth_group_permissions_pkey PRIMARY KEY (id);


--
-- Name: auth_group_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group
    ADD CONSTRAINT auth_group_pkey PRIMARY KEY (id);


--
-- Name: auth_permission_content_type_id_codename_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_permission
    ADD CONSTRAINT auth_permission_content_type_id_codename_key UNIQUE (content_type_id, codename);


--
-- Name: auth_permission_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_permission
    ADD CONSTRAINT auth_permission_pkey PRIMARY KEY (id);


--
-- Name: auth_user_groups_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups
    ADD CONSTRAINT auth_user_groups_pkey PRIMARY KEY (id);


--
-- Name: auth_user_groups_user_id_group_id_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups
    ADD CONSTRAINT auth_user_groups_user_id_group_id_key UNIQUE (user_id, group_id);


--
-- Name: auth_user_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user
    ADD CONSTRAINT auth_user_pkey PRIMARY KEY (id);


--
-- Name: auth_user_user_permissions_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions
    ADD CONSTRAINT auth_user_user_permissions_pkey PRIMARY KEY (id);


--
-- Name: auth_user_user_permissions_user_id_permission_id_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions
    ADD CONSTRAINT auth_user_user_permissions_user_id_permission_id_key UNIQUE (user_id, permission_id);


--
-- Name: auth_user_username_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user
    ADD CONSTRAINT auth_user_username_key UNIQUE (username);


--
-- Name: blog_blogindexpage_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogindexpage
    ADD CONSTRAINT blog_blogindexpage_pkey PRIMARY KEY (page_ptr_id);


--
-- Name: blog_blogindexrelatedlink_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogindexrelatedlink
    ADD CONSTRAINT blog_blogindexrelatedlink_pkey PRIMARY KEY (id);


--
-- Name: blog_blogpage_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogpage
    ADD CONSTRAINT blog_blogpage_pkey PRIMARY KEY (page_ptr_id);


--
-- Name: blog_blogpagetag_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogpagetag
    ADD CONSTRAINT blog_blogpagetag_pkey PRIMARY KEY (id);


--
-- Name: blog_bookmark_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmark
    ADD CONSTRAINT blog_bookmark_pkey PRIMARY KEY (id);


--
-- Name: blog_bookmarkpage_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarkpage
    ADD CONSTRAINT blog_bookmarkpage_pkey PRIMARY KEY (page_ptr_id);


--
-- Name: blog_bookmarkplacement_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarkplacement
    ADD CONSTRAINT blog_bookmarkplacement_pkey PRIMARY KEY (id);


--
-- Name: blog_bookmarktag_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarktag
    ADD CONSTRAINT blog_bookmarktag_pkey PRIMARY KEY (id);


--
-- Name: django_admin_log_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_admin_log
    ADD CONSTRAINT django_admin_log_pkey PRIMARY KEY (id);


--
-- Name: django_content_type_app_label_45f3b1d93ec8c61c_uniq; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_content_type
    ADD CONSTRAINT django_content_type_app_label_45f3b1d93ec8c61c_uniq UNIQUE (app_label, model);


--
-- Name: django_content_type_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_content_type
    ADD CONSTRAINT django_content_type_pkey PRIMARY KEY (id);


--
-- Name: django_migrations_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_migrations
    ADD CONSTRAINT django_migrations_pkey PRIMARY KEY (id);


--
-- Name: django_session_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_session
    ADD CONSTRAINT django_session_pkey PRIMARY KEY (session_key);


--
-- Name: home_homepage_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.home_homepage
    ADD CONSTRAINT home_homepage_pkey PRIMARY KEY (page_ptr_id);


--
-- Name: taggit_tag_name_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.taggit_tag
    ADD CONSTRAINT taggit_tag_name_key UNIQUE (name);


--
-- Name: taggit_tag_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.taggit_tag
    ADD CONSTRAINT taggit_tag_pkey PRIMARY KEY (id);


--
-- Name: taggit_tag_slug_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.taggit_tag
    ADD CONSTRAINT taggit_tag_slug_key UNIQUE (slug);


--
-- Name: taggit_taggeditem_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.taggit_taggeditem
    ADD CONSTRAINT taggit_taggeditem_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_collection_path_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collection
    ADD CONSTRAINT wagtailcore_collection_path_key UNIQUE (path);


--
-- Name: wagtailcore_collection_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collection
    ADD CONSTRAINT wagtailcore_collection_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_collectionviewres_collectionviewrestriction_id__key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collectionviewrestriction_groups
    ADD CONSTRAINT wagtailcore_collectionviewres_collectionviewrestriction_id__key UNIQUE (collectionviewrestriction_id, group_id);


--
-- Name: wagtailcore_collectionviewrestriction_groups_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collectionviewrestriction_groups
    ADD CONSTRAINT wagtailcore_collectionviewrestriction_groups_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_collectionviewrestriction_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collectionviewrestriction
    ADD CONSTRAINT wagtailcore_collectionviewrestriction_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_groupcollectionpermi_group_id_7bdcf8bfbcce5581_uniq; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_groupcollectionpermission
    ADD CONSTRAINT wagtailcore_groupcollectionpermi_group_id_7bdcf8bfbcce5581_uniq UNIQUE (group_id, collection_id, permission_id);


--
-- Name: wagtailcore_groupcollectionpermission_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_groupcollectionpermission
    ADD CONSTRAINT wagtailcore_groupcollectionpermission_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_grouppagepermission_group_id_16e761a1726500_uniq; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_grouppagepermission
    ADD CONSTRAINT wagtailcore_grouppagepermission_group_id_16e761a1726500_uniq UNIQUE (group_id, page_id, permission_type);


--
-- Name: wagtailcore_grouppagepermission_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_grouppagepermission
    ADD CONSTRAINT wagtailcore_grouppagepermission_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_page_path_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_page
    ADD CONSTRAINT wagtailcore_page_path_key UNIQUE (path);


--
-- Name: wagtailcore_page_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_page
    ADD CONSTRAINT wagtailcore_page_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_pagerevision_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pagerevision
    ADD CONSTRAINT wagtailcore_pagerevision_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_pageviewrestricti_pageviewrestriction_id_group__key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pageviewrestriction_groups
    ADD CONSTRAINT wagtailcore_pageviewrestricti_pageviewrestriction_id_group__key UNIQUE (pageviewrestriction_id, group_id);


--
-- Name: wagtailcore_pageviewrestriction_groups_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pageviewrestriction_groups
    ADD CONSTRAINT wagtailcore_pageviewrestriction_groups_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_pageviewrestriction_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pageviewrestriction
    ADD CONSTRAINT wagtailcore_pageviewrestriction_pkey PRIMARY KEY (id);


--
-- Name: wagtailcore_site_hostname_29d2c7f94ac026_uniq; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_site
    ADD CONSTRAINT wagtailcore_site_hostname_29d2c7f94ac026_uniq UNIQUE (hostname, port);


--
-- Name: wagtailcore_site_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_site
    ADD CONSTRAINT wagtailcore_site_pkey PRIMARY KEY (id);


--
-- Name: wagtaildocs_document_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtaildocs_document
    ADD CONSTRAINT wagtaildocs_document_pkey PRIMARY KEY (id);


--
-- Name: wagtailembeds_embed_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailembeds_embed
    ADD CONSTRAINT wagtailembeds_embed_pkey PRIMARY KEY (id);


--
-- Name: wagtailembeds_embed_url_37a13a49926a4846_uniq; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailembeds_embed
    ADD CONSTRAINT wagtailembeds_embed_url_37a13a49926a4846_uniq UNIQUE (url, max_width);


--
-- Name: wagtailforms_formsubmission_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailforms_formsubmission
    ADD CONSTRAINT wagtailforms_formsubmission_pkey PRIMARY KEY (id);


--
-- Name: wagtailimages_image_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailimages_image
    ADD CONSTRAINT wagtailimages_image_pkey PRIMARY KEY (id);


--
-- Name: wagtailimages_rendition_image_id_1f1c287fb71ed0e9_uniq; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailimages_rendition
    ADD CONSTRAINT wagtailimages_rendition_image_id_1f1c287fb71ed0e9_uniq UNIQUE (image_id, filter_spec, focal_point_key);


--
-- Name: wagtailimages_rendition_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailimages_rendition
    ADD CONSTRAINT wagtailimages_rendition_pkey PRIMARY KEY (id);


--
-- Name: wagtailredirects_redirect_old_path_5e354102fcbf9c8b_uniq; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailredirects_redirect
    ADD CONSTRAINT wagtailredirects_redirect_old_path_5e354102fcbf9c8b_uniq UNIQUE (old_path, site_id);


--
-- Name: wagtailredirects_redirect_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailredirects_redirect
    ADD CONSTRAINT wagtailredirects_redirect_pkey PRIMARY KEY (id);


--
-- Name: wagtailsearch_editorspick_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearchpromotions_searchpromotion
    ADD CONSTRAINT wagtailsearch_editorspick_pkey PRIMARY KEY (id);


--
-- Name: wagtailsearch_query_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearch_query
    ADD CONSTRAINT wagtailsearch_query_pkey PRIMARY KEY (id);


--
-- Name: wagtailsearch_query_query_string_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearch_query
    ADD CONSTRAINT wagtailsearch_query_query_string_key UNIQUE (query_string);


--
-- Name: wagtailsearch_querydailyhits_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearch_querydailyhits
    ADD CONSTRAINT wagtailsearch_querydailyhits_pkey PRIMARY KEY (id);


--
-- Name: wagtailsearch_querydailyhits_query_id_4e12c633921cb0c9_uniq; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearch_querydailyhits
    ADD CONSTRAINT wagtailsearch_querydailyhits_query_id_4e12c633921cb0c9_uniq UNIQUE (query_id, date);


--
-- Name: wagtailusers_userprofile_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailusers_userprofile
    ADD CONSTRAINT wagtailusers_userprofile_pkey PRIMARY KEY (id);


--
-- Name: wagtailusers_userprofile_user_id_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailusers_userprofile
    ADD CONSTRAINT wagtailusers_userprofile_user_id_key UNIQUE (user_id);


--
-- Name: auth_group_name_253ae2a6331666e8_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_group_name_253ae2a6331666e8_like ON public.auth_group USING btree (name varchar_pattern_ops);


--
-- Name: auth_group_permissions_0e939a4f; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_group_permissions_0e939a4f ON public.auth_group_permissions USING btree (group_id);


--
-- Name: auth_group_permissions_8373b171; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_group_permissions_8373b171 ON public.auth_group_permissions USING btree (permission_id);


--
-- Name: auth_permission_417f1b1c; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_permission_417f1b1c ON public.auth_permission USING btree (content_type_id);


--
-- Name: auth_user_groups_0e939a4f; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_user_groups_0e939a4f ON public.auth_user_groups USING btree (group_id);


--
-- Name: auth_user_groups_e8701ad4; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_user_groups_e8701ad4 ON public.auth_user_groups USING btree (user_id);


--
-- Name: auth_user_user_permissions_8373b171; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_user_user_permissions_8373b171 ON public.auth_user_user_permissions USING btree (permission_id);


--
-- Name: auth_user_user_permissions_e8701ad4; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_user_user_permissions_e8701ad4 ON public.auth_user_user_permissions USING btree (user_id);


--
-- Name: auth_user_username_51b3b110094b8aae_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX auth_user_username_51b3b110094b8aae_like ON public.auth_user USING btree (username varchar_pattern_ops);


--
-- Name: blog_blogindexrelatedlink_1a63c800; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_blogindexrelatedlink_1a63c800 ON public.blog_blogindexrelatedlink USING btree (page_id);


--
-- Name: blog_blogindexrelatedlink_5b76e141; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_blogindexrelatedlink_5b76e141 ON public.blog_blogindexrelatedlink USING btree (link_page_id);


--
-- Name: blog_blogpage_36b62cbe; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_blogpage_36b62cbe ON public.blog_blogpage USING btree (main_image_id);


--
-- Name: blog_blogpage_993b3523; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_blogpage_993b3523 ON public.blog_blogpage USING btree (listing_image_id);


--
-- Name: blog_blogpagetag_09a80f33; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_blogpagetag_09a80f33 ON public.blog_blogpagetag USING btree (content_object_id);


--
-- Name: blog_blogpagetag_76f094bc; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_blogpagetag_76f094bc ON public.blog_blogpagetag USING btree (tag_id);


--
-- Name: blog_bookmarkplacement_1a63c800; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_bookmarkplacement_1a63c800 ON public.blog_bookmarkplacement USING btree (page_id);


--
-- Name: blog_bookmarkplacement_9c7b8123; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_bookmarkplacement_9c7b8123 ON public.blog_bookmarkplacement USING btree (quote_id);


--
-- Name: blog_bookmarktag_09a80f33; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_bookmarktag_09a80f33 ON public.blog_bookmarktag USING btree (content_object_id);


--
-- Name: blog_bookmarktag_76f094bc; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX blog_bookmarktag_76f094bc ON public.blog_bookmarktag USING btree (tag_id);


--
-- Name: django_admin_log_417f1b1c; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX django_admin_log_417f1b1c ON public.django_admin_log USING btree (content_type_id);


--
-- Name: django_admin_log_e8701ad4; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX django_admin_log_e8701ad4 ON public.django_admin_log USING btree (user_id);


--
-- Name: django_session_de54fa62; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX django_session_de54fa62 ON public.django_session USING btree (expire_date);


--
-- Name: django_session_session_key_461cfeaa630ca218_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX django_session_session_key_461cfeaa630ca218_like ON public.django_session USING btree (session_key varchar_pattern_ops);


--
-- Name: taggit_tag_name_4ed9aad194b72af1_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX taggit_tag_name_4ed9aad194b72af1_like ON public.taggit_tag USING btree (name varchar_pattern_ops);


--
-- Name: taggit_tag_slug_703438030cd922a7_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX taggit_tag_slug_703438030cd922a7_like ON public.taggit_tag USING btree (slug varchar_pattern_ops);


--
-- Name: taggit_taggeditem_417f1b1c; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX taggit_taggeditem_417f1b1c ON public.taggit_taggeditem USING btree (content_type_id);


--
-- Name: taggit_taggeditem_76f094bc; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX taggit_taggeditem_76f094bc ON public.taggit_taggeditem USING btree (tag_id);


--
-- Name: taggit_taggeditem_af31437c; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX taggit_taggeditem_af31437c ON public.taggit_taggeditem USING btree (object_id);


--
-- Name: taggit_taggeditem_content_type_id_3c99b32018cc9d40_idx; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX taggit_taggeditem_content_type_id_3c99b32018cc9d40_idx ON public.taggit_taggeditem USING btree (content_type_id, object_id);


--
-- Name: wagtailcore_collection_path_fb7af1cc8ed8c35_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_collection_path_fb7af1cc8ed8c35_like ON public.wagtailcore_collection USING btree (path varchar_pattern_ops);


--
-- Name: wagtailcore_collectionviewrestriction_0a1a4dd8; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_collectionviewrestriction_0a1a4dd8 ON public.wagtailcore_collectionviewrestriction USING btree (collection_id);


--
-- Name: wagtailcore_collectionviewrestriction_groups_0e939a4f; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_collectionviewrestriction_groups_0e939a4f ON public.wagtailcore_collectionviewrestriction_groups USING btree (group_id);


--
-- Name: wagtailcore_collectionviewrestriction_groups_19f356fa; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_collectionviewrestriction_groups_19f356fa ON public.wagtailcore_collectionviewrestriction_groups USING btree (collectionviewrestriction_id);


--
-- Name: wagtailcore_groupcollectionpermission_0a1a4dd8; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_groupcollectionpermission_0a1a4dd8 ON public.wagtailcore_groupcollectionpermission USING btree (collection_id);


--
-- Name: wagtailcore_groupcollectionpermission_0e939a4f; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_groupcollectionpermission_0e939a4f ON public.wagtailcore_groupcollectionpermission USING btree (group_id);


--
-- Name: wagtailcore_groupcollectionpermission_8373b171; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_groupcollectionpermission_8373b171 ON public.wagtailcore_groupcollectionpermission USING btree (permission_id);


--
-- Name: wagtailcore_grouppagepermission_0e939a4f; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_grouppagepermission_0e939a4f ON public.wagtailcore_grouppagepermission USING btree (group_id);


--
-- Name: wagtailcore_grouppagepermission_1a63c800; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_grouppagepermission_1a63c800 ON public.wagtailcore_grouppagepermission USING btree (page_id);


--
-- Name: wagtailcore_page_2dbcba41; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_page_2dbcba41 ON public.wagtailcore_page USING btree (slug);


--
-- Name: wagtailcore_page_417f1b1c; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_page_417f1b1c ON public.wagtailcore_page USING btree (content_type_id);


--
-- Name: wagtailcore_page_47e527a3; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_page_47e527a3 ON public.wagtailcore_page USING btree (live_revision_id);


--
-- Name: wagtailcore_page_5e7b1936; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_page_5e7b1936 ON public.wagtailcore_page USING btree (owner_id);


--
-- Name: wagtailcore_page_first_published_at_785096aa58388042_uniq; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_page_first_published_at_785096aa58388042_uniq ON public.wagtailcore_page USING btree (first_published_at);


--
-- Name: wagtailcore_page_path_adbf7302a1ab75e_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_page_path_adbf7302a1ab75e_like ON public.wagtailcore_page USING btree (path varchar_pattern_ops);


--
-- Name: wagtailcore_page_slug_de66a236c47d916_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_page_slug_de66a236c47d916_like ON public.wagtailcore_page USING btree (slug varchar_pattern_ops);


--
-- Name: wagtailcore_page_submitted_for_moderation_10bec949f0821f20_uniq; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_page_submitted_for_moderation_10bec949f0821f20_uniq ON public.wagtailcore_pagerevision USING btree (submitted_for_moderation);


--
-- Name: wagtailcore_pagerevision_1a63c800; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_pagerevision_1a63c800 ON public.wagtailcore_pagerevision USING btree (page_id);


--
-- Name: wagtailcore_pagerevision_created_at_55418615c0b98528_uniq; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_pagerevision_created_at_55418615c0b98528_uniq ON public.wagtailcore_pagerevision USING btree (created_at);


--
-- Name: wagtailcore_pagerevision_e8701ad4; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_pagerevision_e8701ad4 ON public.wagtailcore_pagerevision USING btree (user_id);


--
-- Name: wagtailcore_pageviewrestriction_1a63c800; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_pageviewrestriction_1a63c800 ON public.wagtailcore_pageviewrestriction USING btree (page_id);


--
-- Name: wagtailcore_pageviewrestriction_groups_0e939a4f; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_pageviewrestriction_groups_0e939a4f ON public.wagtailcore_pageviewrestriction_groups USING btree (group_id);


--
-- Name: wagtailcore_pageviewrestriction_groups_9bdbac54; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_pageviewrestriction_groups_9bdbac54 ON public.wagtailcore_pageviewrestriction_groups USING btree (pageviewrestriction_id);


--
-- Name: wagtailcore_site_0897acf4; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_site_0897acf4 ON public.wagtailcore_site USING btree (hostname);


--
-- Name: wagtailcore_site_8372b497; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_site_8372b497 ON public.wagtailcore_site USING btree (root_page_id);


--
-- Name: wagtailcore_site_hostname_3649a8ca5c8e8730_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailcore_site_hostname_3649a8ca5c8e8730_like ON public.wagtailcore_site USING btree (hostname varchar_pattern_ops);


--
-- Name: wagtaildocs_document_0a1a4dd8; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtaildocs_document_0a1a4dd8 ON public.wagtaildocs_document USING btree (collection_id);


--
-- Name: wagtaildocs_document_ef01e2b6; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtaildocs_document_ef01e2b6 ON public.wagtaildocs_document USING btree (uploaded_by_user_id);


--
-- Name: wagtailforms_formsubmission_1a63c800; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailforms_formsubmission_1a63c800 ON public.wagtailforms_formsubmission USING btree (page_id);


--
-- Name: wagtailimages_image_0a1a4dd8; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailimages_image_0a1a4dd8 ON public.wagtailimages_image USING btree (collection_id);


--
-- Name: wagtailimages_image_created_at_1e91a237c16eaa71_uniq; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailimages_image_created_at_1e91a237c16eaa71_uniq ON public.wagtailimages_image USING btree (created_at);


--
-- Name: wagtailimages_image_ef01e2b6; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailimages_image_ef01e2b6 ON public.wagtailimages_image USING btree (uploaded_by_user_id);


--
-- Name: wagtailimages_rendition_58c64917; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailimages_rendition_58c64917 ON public.wagtailimages_rendition USING btree (filter_spec);


--
-- Name: wagtailimages_rendition_f33175e6; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailimages_rendition_f33175e6 ON public.wagtailimages_rendition USING btree (image_id);


--
-- Name: wagtailredirects_redirect_2fd79f37; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailredirects_redirect_2fd79f37 ON public.wagtailredirects_redirect USING btree (redirect_page_id);


--
-- Name: wagtailredirects_redirect_9365d6e7; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailredirects_redirect_9365d6e7 ON public.wagtailredirects_redirect USING btree (site_id);


--
-- Name: wagtailredirects_redirect_old_path_579ecadc1434daf4_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailredirects_redirect_old_path_579ecadc1434daf4_like ON public.wagtailredirects_redirect USING btree (old_path varchar_pattern_ops);


--
-- Name: wagtailsearch_editorspick_0bbeda9c; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailsearch_editorspick_0bbeda9c ON public.wagtailsearchpromotions_searchpromotion USING btree (query_id);


--
-- Name: wagtailsearch_editorspick_1a63c800; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailsearch_editorspick_1a63c800 ON public.wagtailsearchpromotions_searchpromotion USING btree (page_id);


--
-- Name: wagtailsearch_query_query_string_a78010a1796bb04_like; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailsearch_query_query_string_a78010a1796bb04_like ON public.wagtailsearch_query USING btree (query_string varchar_pattern_ops);


--
-- Name: wagtailsearch_querydailyhits_0bbeda9c; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX wagtailsearch_querydailyhits_0bbeda9c ON public.wagtailsearch_querydailyhits USING btree (query_id);


--
-- Name: D1aef0323217caf70d20b42ecbb2d0f2; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collectionviewrestriction_groups
    ADD CONSTRAINT "D1aef0323217caf70d20b42ecbb2d0f2" FOREIGN KEY (collectionviewrestriction_id) REFERENCES public.wagtailcore_collectionviewrestriction(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: D59e4f8e70dc4c74113492e5e759a9c3; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogpagetag
    ADD CONSTRAINT "D59e4f8e70dc4c74113492e5e759a9c3" FOREIGN KEY (content_object_id) REFERENCES public.blog_blogpage(page_ptr_id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: D8dede6c85e6e9f9031de33138010bff; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pageviewrestriction_groups
    ADD CONSTRAINT "D8dede6c85e6e9f9031de33138010bff" FOREIGN KEY (pageviewrestriction_id) REFERENCES public.wagtailcore_pageviewrestriction(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: auth_content_type_id_508cf46651277a81_fk_django_content_type_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_permission
    ADD CONSTRAINT auth_content_type_id_508cf46651277a81_fk_django_content_type_id FOREIGN KEY (content_type_id) REFERENCES public.django_content_type(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: auth_group_permissio_group_id_689710a9a73b7457_fk_auth_group_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions
    ADD CONSTRAINT auth_group_permissio_group_id_689710a9a73b7457_fk_auth_group_id FOREIGN KEY (group_id) REFERENCES public.auth_group(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: auth_group_permission_id_1f49ccbbdc69d2fc_fk_auth_permission_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions
    ADD CONSTRAINT auth_group_permission_id_1f49ccbbdc69d2fc_fk_auth_permission_id FOREIGN KEY (permission_id) REFERENCES public.auth_permission(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: auth_user__permission_id_384b62483d7071f0_fk_auth_permission_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions
    ADD CONSTRAINT auth_user__permission_id_384b62483d7071f0_fk_auth_permission_id FOREIGN KEY (permission_id) REFERENCES public.auth_permission(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: auth_user_groups_group_id_33ac548dcf5f8e37_fk_auth_group_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups
    ADD CONSTRAINT auth_user_groups_group_id_33ac548dcf5f8e37_fk_auth_group_id FOREIGN KEY (group_id) REFERENCES public.auth_group(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: auth_user_groups_user_id_4b5ed4ffdb8fd9b0_fk_auth_user_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups
    ADD CONSTRAINT auth_user_groups_user_id_4b5ed4ffdb8fd9b0_fk_auth_user_id FOREIGN KEY (user_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: auth_user_user_permiss_user_id_7f0938558328534a_fk_auth_user_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions
    ADD CONSTRAINT auth_user_user_permiss_user_id_7f0938558328534a_fk_auth_user_id FOREIGN KEY (user_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_b_main_image_id_49d1c76b352ea95a_fk_wagtailimages_image_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogpage
    ADD CONSTRAINT blog_b_main_image_id_49d1c76b352ea95a_fk_wagtailimages_image_id FOREIGN KEY (main_image_id) REFERENCES public.wagtailimages_image(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_blogin_link_page_id_99d9f0c05b080ae_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogindexrelatedlink
    ADD CONSTRAINT blog_blogin_link_page_id_99d9f0c05b080ae_fk_wagtailcore_page_id FOREIGN KEY (link_page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_blogin_page_ptr_id_27bdacf7c3f57dce_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogindexpage
    ADD CONSTRAINT blog_blogin_page_ptr_id_27bdacf7c3f57dce_fk_wagtailcore_page_id FOREIGN KEY (page_ptr_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_blogpa_page_ptr_id_38b88b267130b8cf_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogpage
    ADD CONSTRAINT blog_blogpa_page_ptr_id_38b88b267130b8cf_fk_wagtailcore_page_id FOREIGN KEY (page_ptr_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_blogpagetag_tag_id_42cca3ea895c8183_fk_taggit_tag_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogpagetag
    ADD CONSTRAINT blog_blogpagetag_tag_id_42cca3ea895c8183_fk_taggit_tag_id FOREIGN KEY (tag_id) REFERENCES public.taggit_tag(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_boo_content_object_id_4a1162807d5666c2_fk_blog_bookmark_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarktag
    ADD CONSTRAINT blog_boo_content_object_id_4a1162807d5666c2_fk_blog_bookmark_id FOREIGN KEY (content_object_id) REFERENCES public.blog_bookmark(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_bookma_page_ptr_id_19a8f9da771c86b3_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarkpage
    ADD CONSTRAINT blog_bookma_page_ptr_id_19a8f9da771c86b3_fk_wagtailcore_page_id FOREIGN KEY (page_ptr_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_bookmarkpl_page_id_2126773b4fc1ae87_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarkplacement
    ADD CONSTRAINT blog_bookmarkpl_page_id_2126773b4fc1ae87_fk_wagtailcore_page_id FOREIGN KEY (page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_bookmarkplace_quote_id_5bd9f3da1dc2a91_fk_blog_bookmark_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarkplacement
    ADD CONSTRAINT blog_bookmarkplace_quote_id_5bd9f3da1dc2a91_fk_blog_bookmark_id FOREIGN KEY (quote_id) REFERENCES public.blog_bookmark(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_bookmarktag_tag_id_a95469015041494_fk_taggit_tag_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_bookmarktag
    ADD CONSTRAINT blog_bookmarktag_tag_id_a95469015041494_fk_taggit_tag_id FOREIGN KEY (tag_id) REFERENCES public.taggit_tag(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_listing_image_id_c980dbd59b8ba1e_fk_wagtailimages_image_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogpage
    ADD CONSTRAINT blog_listing_image_id_c980dbd59b8ba1e_fk_wagtailimages_image_id FOREIGN KEY (listing_image_id) REFERENCES public.wagtailimages_image(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: blog_page_id_1cf5bf9fa96f6eac_fk_blog_blogindexpage_page_ptr_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.blog_blogindexrelatedlink
    ADD CONSTRAINT blog_page_id_1cf5bf9fa96f6eac_fk_blog_blogindexpage_page_ptr_id FOREIGN KEY (page_id) REFERENCES public.blog_blogindexpage(page_ptr_id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: djan_content_type_id_697914295151027a_fk_django_content_type_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_admin_log
    ADD CONSTRAINT djan_content_type_id_697914295151027a_fk_django_content_type_id FOREIGN KEY (content_type_id) REFERENCES public.django_content_type(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: django_admin_log_user_id_52fdd58701c5f563_fk_auth_user_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_admin_log
    ADD CONSTRAINT django_admin_log_user_id_52fdd58701c5f563_fk_auth_user_id FOREIGN KEY (user_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: home_homepa_page_ptr_id_1bb67742a6b3ac95_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.home_homepage
    ADD CONSTRAINT home_homepa_page_ptr_id_1bb67742a6b3ac95_fk_wagtailcore_page_id FOREIGN KEY (page_ptr_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: live_revision_id_75537d6eff0a03_fk_wagtailcore_pagerevision_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_page
    ADD CONSTRAINT live_revision_id_75537d6eff0a03_fk_wagtailcore_pagerevision_id FOREIGN KEY (live_revision_id) REFERENCES public.wagtailcore_pagerevision(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: tagg_content_type_id_62e0524705c3ec8f_fk_django_content_type_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.taggit_taggeditem
    ADD CONSTRAINT tagg_content_type_id_62e0524705c3ec8f_fk_django_content_type_id FOREIGN KEY (content_type_id) REFERENCES public.django_content_type(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: taggit_taggeditem_tag_id_6318217c0d95e0d2_fk_taggit_tag_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.taggit_taggeditem
    ADD CONSTRAINT taggit_taggeditem_tag_id_6318217c0d95e0d2_fk_taggit_tag_id FOREIGN KEY (tag_id) REFERENCES public.taggit_tag(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wag_collection_id_1b29a2a37e7d436c_fk_wagtailcore_collection_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_groupcollectionpermission
    ADD CONSTRAINT wag_collection_id_1b29a2a37e7d436c_fk_wagtailcore_collection_id FOREIGN KEY (collection_id) REFERENCES public.wagtailcore_collection(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wag_collection_id_285e87239b035e6a_fk_wagtailcore_collection_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailimages_image
    ADD CONSTRAINT wag_collection_id_285e87239b035e6a_fk_wagtailcore_collection_id FOREIGN KEY (collection_id) REFERENCES public.wagtailcore_collection(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wag_collection_id_2b616eaa03011e90_fk_wagtailcore_collection_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtaildocs_document
    ADD CONSTRAINT wag_collection_id_2b616eaa03011e90_fk_wagtailcore_collection_id FOREIGN KEY (collection_id) REFERENCES public.wagtailcore_collection(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wag_collection_id_560c2f005981aa81_fk_wagtailcore_collection_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collectionviewrestriction
    ADD CONSTRAINT wag_collection_id_560c2f005981aa81_fk_wagtailcore_collection_id FOREIGN KEY (collection_id) REFERENCES public.wagtailcore_collection(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagt_content_type_id_7ae0ebb2acb1454e_fk_django_content_type_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_page
    ADD CONSTRAINT wagt_content_type_id_7ae0ebb2acb1454e_fk_django_content_type_id FOREIGN KEY (content_type_id) REFERENCES public.django_content_type(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtai_redirect_page_id_4fb5deae195b3223_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailredirects_redirect
    ADD CONSTRAINT wagtai_redirect_page_id_4fb5deae195b3223_fk_wagtailcore_page_id FOREIGN KEY (redirect_page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcor_permission_id_48aff6f1dd268787_fk_auth_permission_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_groupcollectionpermission
    ADD CONSTRAINT wagtailcor_permission_id_48aff6f1dd268787_fk_auth_permission_id FOREIGN KEY (permission_id) REFERENCES public.auth_permission(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcor_root_page_id_5c8b4b84e03f7f29_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_site
    ADD CONSTRAINT wagtailcor_root_page_id_5c8b4b84e03f7f29_fk_wagtailcore_page_id FOREIGN KEY (root_page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_collecti_group_id_11892603113c505f_fk_auth_group_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_collectionviewrestriction_groups
    ADD CONSTRAINT wagtailcore_collecti_group_id_11892603113c505f_fk_auth_group_id FOREIGN KEY (group_id) REFERENCES public.auth_group(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_gro_page_id_70d2788c0579bb7c_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_grouppagepermission
    ADD CONSTRAINT wagtailcore_gro_page_id_70d2788c0579bb7c_fk_wagtailcore_page_id FOREIGN KEY (page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_groupcol_group_id_6d27de27630f0e7a_fk_auth_group_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_groupcollectionpermission
    ADD CONSTRAINT wagtailcore_groupcol_group_id_6d27de27630f0e7a_fk_auth_group_id FOREIGN KEY (group_id) REFERENCES public.auth_group(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_grouppage_group_id_2df9571b92fb26d_fk_auth_group_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_grouppagepermission
    ADD CONSTRAINT wagtailcore_grouppage_group_id_2df9571b92fb26d_fk_auth_group_id FOREIGN KEY (group_id) REFERENCES public.auth_group(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_pag_page_id_1d5ab1303676feba_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pagerevision
    ADD CONSTRAINT wagtailcore_pag_page_id_1d5ab1303676feba_fk_wagtailcore_page_id FOREIGN KEY (page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_pag_page_id_318895e696da7fed_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pageviewrestriction
    ADD CONSTRAINT wagtailcore_pag_page_id_318895e696da7fed_fk_wagtailcore_page_id FOREIGN KEY (page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_page_owner_id_7a2f24f1767b30bc_fk_auth_user_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_page
    ADD CONSTRAINT wagtailcore_page_owner_id_7a2f24f1767b30bc_fk_auth_user_id FOREIGN KEY (owner_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_pagerevisi_user_id_3a9a8cf31a218402_fk_auth_user_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pagerevision
    ADD CONSTRAINT wagtailcore_pagerevisi_user_id_3a9a8cf31a218402_fk_auth_user_id FOREIGN KEY (user_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailcore_pageview_group_id_26828dafa94f0d28_fk_auth_group_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailcore_pageviewrestriction_groups
    ADD CONSTRAINT wagtailcore_pageview_group_id_26828dafa94f0d28_fk_auth_group_id FOREIGN KEY (group_id) REFERENCES public.auth_group(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtaildoc_uploaded_by_user_id_62c5d96169f4ec20_fk_auth_user_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtaildocs_document
    ADD CONSTRAINT wagtaildoc_uploaded_by_user_id_62c5d96169f4ec20_fk_auth_user_id FOREIGN KEY (uploaded_by_user_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailforms_fo_page_id_72bcec1db96e6d21_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailforms_formsubmission
    ADD CONSTRAINT wagtailforms_fo_page_id_72bcec1db96e6d21_fk_wagtailcore_page_id FOREIGN KEY (page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailima_uploaded_by_user_id_4941ddafe7e6985a_fk_auth_user_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailimages_image
    ADD CONSTRAINT wagtailima_uploaded_by_user_id_4941ddafe7e6985a_fk_auth_user_id FOREIGN KEY (uploaded_by_user_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailimag_image_id_4b83f0a74ebd24db_fk_wagtailimages_image_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailimages_rendition
    ADD CONSTRAINT wagtailimag_image_id_4b83f0a74ebd24db_fk_wagtailimages_image_id FOREIGN KEY (image_id) REFERENCES public.wagtailimages_image(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailredirect_site_id_72075f3bbfcf92e7_fk_wagtailcore_site_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailredirects_redirect
    ADD CONSTRAINT wagtailredirect_site_id_72075f3bbfcf92e7_fk_wagtailcore_site_id FOREIGN KEY (site_id) REFERENCES public.wagtailcore_site(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailsear_query_id_355494074ca8351a_fk_wagtailsearch_query_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearch_querydailyhits
    ADD CONSTRAINT wagtailsear_query_id_355494074ca8351a_fk_wagtailsearch_query_id FOREIGN KEY (query_id) REFERENCES public.wagtailsearch_query(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailsear_query_id_74051b390c9e69bd_fk_wagtailsearch_query_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearchpromotions_searchpromotion
    ADD CONSTRAINT wagtailsear_query_id_74051b390c9e69bd_fk_wagtailsearch_query_id FOREIGN KEY (query_id) REFERENCES public.wagtailsearch_query(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailsearchpr_page_id_3462cbff9e5ac96d_fk_wagtailcore_page_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailsearchpromotions_searchpromotion
    ADD CONSTRAINT wagtailsearchpr_page_id_3462cbff9e5ac96d_fk_wagtailcore_page_id FOREIGN KEY (page_id) REFERENCES public.wagtailcore_page(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: wagtailusers_userprofi_user_id_755efda9998dba71_fk_auth_user_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.wagtailusers_userprofile
    ADD CONSTRAINT wagtailusers_userprofi_user_id_755efda9998dba71_fk_auth_user_id FOREIGN KEY (user_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED;


--
-- Name: SCHEMA public; Type: ACL; Schema: -; Owner: postgres
--

REVOKE ALL ON SCHEMA public FROM PUBLIC;
REVOKE ALL ON SCHEMA public FROM postgres;
GRANT ALL ON SCHEMA public TO postgres;
GRANT ALL ON SCHEMA public TO PUBLIC;


--
-- PostgreSQL database dump complete
--

